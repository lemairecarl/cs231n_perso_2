{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.convnet import *\n",
    "from cs231n.solver import Solver\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.sequential import *\n",
    "from cs231n import test\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'jet'\n",
    "plt.rcParams['axes.formatter.useoffset'] = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val (1000, 3, 32, 32)\n",
      "X_train (49000, 3, 32, 32)\n",
      "X_test (1000, 3, 32, 32)\n",
      "y_val (1000,)\n",
      "y_train (49000,)\n",
      "y_test (1000,)\n"
     ]
    }
   ],
   "source": [
    "data = get_CIFAR10_data()\n",
    "\n",
    "for n, d in data.items():\n",
    "    if n.startswith('X'):\n",
    "        d /= 127.0\n",
    "    print n, d.shape\n",
    "\n",
    "num_val = 2000\n",
    "val_data = {\n",
    "    'X_train': data['X_train'][:num_val],\n",
    "    'y_train': data['y_train'][:num_val],\n",
    "    'X_val': data['X_val'],\n",
    "    'y_val': data['y_val']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradcheck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "\n",
    "* Fix Dense backprop\n",
    "* Relu between Dense\n",
    "* Gradcheck on f(x) = u.T \\* g(v \\* x) où g : mon réseau; u,v : random vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 8, 8)\n",
      "0.196183743773 0.655725145515\n"
     ]
    }
   ],
   "source": [
    "# Prepare a small dataset\n",
    "total_examples = 3\n",
    "X = data['X_train'][:total_examples, :, :8, :8]\n",
    "y = data['y_train'][:total_examples]\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\n",
      "ConvBnRelu1_W        (8, 3, 3, 3)\n",
      "ConvBnRelu1_b        (8,)\n",
      "ConvBnRelu1_beta     (8,)\n",
      "ConvBnRelu1_gamma    (8,)\n",
      "ConvBnRelu2_W        (8, 8, 3, 3)\n",
      "ConvBnRelu2_b        (8,)\n",
      "ConvBnRelu2_beta     (8,)\n",
      "ConvBnRelu2_gamma    (8,)\n",
      "ConvBnRelu3_W        (16, 8, 3, 3)\n",
      "ConvBnRelu3_b        (16,)\n",
      "ConvBnRelu3_beta     (16,)\n",
      "ConvBnRelu3_gamma    (16,)\n",
      "ConvBnRelu4_W        (16, 16, 3, 3)\n",
      "ConvBnRelu4_b        (16,)\n",
      "ConvBnRelu4_beta     (16,)\n",
      "ConvBnRelu4_gamma    (16,)\n",
      "ConvBnRelu5_W        (32, 16, 3, 3)\n",
      "ConvBnRelu5_b        (32,)\n",
      "ConvBnRelu5_beta     (32,)\n",
      "ConvBnRelu5_gamma    (32,)\n",
      "ConvBnRelu6_W        (32, 32, 3, 3)\n",
      "ConvBnRelu6_b        (32,)\n",
      "ConvBnRelu6_beta     (32,)\n",
      "ConvBnRelu6_gamma    (32,)\n",
      "Dense1_W             (512, 256)\n",
      "Dense1_b             (256,)\n",
      "Dense2_W             (256, 128)\n",
      "Dense2_b             (128,)\n",
      "Dense3_W             (128, 10)\n",
      "Dense3_b             (10,)\n",
      "Total 183922\n",
      "\n",
      "Layer outputs:\n",
      "InputLayer1      out: (50, 3, 32, 32)\n",
      "ConvBnRelu1      out: (50, 8, 32, 32)\n",
      "ConvBnRelu2      out: (50, 8, 32, 32)\n",
      "Pool1            out: (50, 8, 16, 16)\n",
      "ConvBnRelu3      out: (50, 16, 16, 16)\n",
      "ConvBnRelu4      out: (50, 16, 16, 16)\n",
      "Pool2            out: (50, 16, 8, 8)\n",
      "ConvBnRelu5      out: (50, 32, 8, 8)\n",
      "ConvBnRelu6      out: (50, 32, 8, 8)\n",
      "Pool3            out: (50, 32, 4, 4)\n",
      "Dense1           out: (50, 256)\n",
      "Relu1            out: (50, 256)\n",
      "Dense2           out: (50, 128)\n",
      "Relu2            out: (50, 128)\n",
      "Dense3           out: (50, 10)\n",
      "Softmax1         out: None\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "lr = 3.464142e-02\n",
    "reg = 1.060586e-04\n",
    "batch_shape = (50,) + data['X_train'].shape[1:]\n",
    "model = Sequential(batch_shape=batch_shape, weight_scale=1.0, reg=reg, dtype=np.float64)\n",
    "for num_filters in [8, 16, 32]:\n",
    "    model.add(ConvBnRelu(num_filters))\n",
    "    model.add(ConvBnRelu(num_filters))\n",
    "    model.add(Pool())\n",
    "for num_neurons in [256, 128]:\n",
    "    model.add(Dense(num_neurons, weight_init='sqrt_2_over_n'))\n",
    "    model.add(Relu())\n",
    "model.add(Dense(num_neurons=10, weight_init='sqrt_2_over_n'))\n",
    "model.build(loss=Softmax())\n",
    "model.print_params()\n",
    "print ''\n",
    "model.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc 1.0   Loss 0.417043459145\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1AAAAK9CAYAAAAqto3ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117780550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model a bit\n",
    "solver = Solver(model, {'X_train': X, 'y_train': y, 'X_val': X, 'y_val': y},\n",
    "                num_epochs=10, batch_size=X.shape[0],\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': lr,\n",
    "                },\n",
    "                verbose=False, print_every=1)\n",
    "solver.train()\n",
    "print 'Train acc', solver.train_acc_history[-1], '  Loss', solver.loss_history[-1]\n",
    "plt.plot(solver.loss_history)\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Gradient check ---\n",
      "Max relative error:   (h = 1e-06)\n",
      "Param                Error                                   Ana           Num          \n",
      "Dense1_W             2.101274e-04  uncomfortable     avgval: 2.363497e-03  2.363498e-03 \n",
      "Dense1_b             2.891305e-06                    avgval: 7.765718e-03  7.765715e-03 \n",
      "Dense2_W             2.217252e-05                    avgval: 1.824976e-02  1.824976e-02 \n",
      "Dense2_b             2.308361e-06                    avgval: 1.840074e-02  1.840074e-02 \n",
      "Dense3_W             1.327402e-05                    avgval: 2.328581e-02  2.328581e-02 \n",
      "Dense3_b             3.354189e-07                    avgval: 9.750922e-03  9.750919e-03 \n",
      "Dense4_W             7.194919e-06                    avgval: 4.764273e-02  4.764233e-02 \n",
      "Dense4_b             2.716196e-07                    avgval: 5.025158e-02  5.025155e-02 \n"
     ]
    }
   ],
   "source": [
    "# Perform gradient check\n",
    "test.gradient_check(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "best_solver = None\n",
    "best_model = None\n",
    "best_acc = -1\n",
    "best_loss = 10000\n",
    "best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 14700) loss: 4.351709\n",
      "(Epoch 0 / 15) train acc: 0.113000; val_acc: 0.106000\n",
      "(Iteration 2 / 14700) loss: 3.603637\n",
      "(Iteration 3 / 14700) loss: 4.000894\n",
      "(Iteration 4 / 14700) loss: 3.824935\n",
      "(Iteration 5 / 14700) loss: 4.013979\n",
      "(Iteration 6 / 14700) loss: 3.574642\n",
      "(Iteration 7 / 14700) loss: 3.819104\n",
      "(Iteration 8 / 14700) loss: 3.267116\n",
      "(Iteration 9 / 14700) loss: 3.318181\n",
      "(Iteration 10 / 14700) loss: 3.553186\n",
      "(Iteration 11 / 14700) loss: 3.490904\n",
      "(Iteration 12 / 14700) loss: 3.267551\n",
      "(Iteration 13 / 14700) loss: 3.354269\n",
      "(Iteration 14 / 14700) loss: 3.234851\n",
      "(Iteration 15 / 14700) loss: 3.409393\n",
      "(Iteration 16 / 14700) loss: 3.349814\n",
      "(Iteration 17 / 14700) loss: 3.189426\n",
      "(Iteration 18 / 14700) loss: 3.358762\n",
      "(Iteration 19 / 14700) loss: 3.260574\n",
      "(Iteration 20 / 14700) loss: 3.373480\n",
      "(Iteration 21 / 14700) loss: 2.917864\n",
      "(Iteration 22 / 14700) loss: 3.228835\n",
      "(Iteration 23 / 14700) loss: 3.257188\n",
      "(Iteration 24 / 14700) loss: 3.231630\n",
      "(Iteration 25 / 14700) loss: 3.297267\n",
      "(Iteration 26 / 14700) loss: 3.276417\n",
      "(Iteration 27 / 14700) loss: 3.055236\n",
      "(Iteration 28 / 14700) loss: 3.062798\n",
      "(Iteration 29 / 14700) loss: 3.060819\n",
      "(Iteration 30 / 14700) loss: 2.878289\n",
      "(Iteration 31 / 14700) loss: 2.979589\n",
      "(Iteration 32 / 14700) loss: 3.013409\n",
      "(Iteration 33 / 14700) loss: 2.973438\n",
      "(Iteration 34 / 14700) loss: 3.072798\n",
      "(Iteration 35 / 14700) loss: 2.882197\n",
      "(Iteration 36 / 14700) loss: 2.991855\n",
      "(Iteration 37 / 14700) loss: 2.872047\n",
      "(Iteration 38 / 14700) loss: 3.007028\n",
      "(Iteration 39 / 14700) loss: 2.754571\n",
      "(Iteration 40 / 14700) loss: 2.924292\n",
      "(Iteration 41 / 14700) loss: 3.143166\n",
      "(Iteration 42 / 14700) loss: 2.872685\n",
      "(Iteration 43 / 14700) loss: 3.117640\n",
      "(Iteration 44 / 14700) loss: 2.885684\n",
      "(Iteration 45 / 14700) loss: 2.860123\n",
      "(Iteration 46 / 14700) loss: 2.973184\n",
      "(Iteration 47 / 14700) loss: 2.719636\n",
      "(Iteration 48 / 14700) loss: 2.968973\n",
      "(Iteration 49 / 14700) loss: 2.886690\n",
      "(Iteration 50 / 14700) loss: 3.143064\n",
      "(Iteration 51 / 14700) loss: 2.931893\n",
      "(Iteration 52 / 14700) loss: 2.757486\n",
      "(Iteration 53 / 14700) loss: 2.808818\n",
      "(Iteration 54 / 14700) loss: 2.888106\n",
      "(Iteration 55 / 14700) loss: 2.757314\n",
      "(Iteration 56 / 14700) loss: 2.994437\n",
      "(Iteration 57 / 14700) loss: 2.834939\n",
      "(Iteration 58 / 14700) loss: 2.982798\n",
      "(Iteration 59 / 14700) loss: 2.840545\n",
      "(Iteration 60 / 14700) loss: 2.772991\n",
      "(Iteration 61 / 14700) loss: 2.815563\n",
      "(Iteration 62 / 14700) loss: 2.631548\n",
      "(Iteration 63 / 14700) loss: 2.822948\n",
      "(Iteration 64 / 14700) loss: 2.733455\n",
      "(Iteration 65 / 14700) loss: 2.738221\n",
      "(Iteration 66 / 14700) loss: 3.008315\n",
      "(Iteration 67 / 14700) loss: 2.980090\n",
      "(Iteration 68 / 14700) loss: 2.762515\n",
      "(Iteration 69 / 14700) loss: 2.627420\n",
      "(Iteration 70 / 14700) loss: 2.718674\n",
      "(Iteration 71 / 14700) loss: 3.041028\n",
      "(Iteration 72 / 14700) loss: 2.895872\n",
      "(Iteration 73 / 14700) loss: 2.678110\n",
      "(Iteration 74 / 14700) loss: 2.611017\n",
      "(Iteration 75 / 14700) loss: 2.856376\n",
      "(Iteration 76 / 14700) loss: 2.450337\n",
      "(Iteration 77 / 14700) loss: 2.826132\n",
      "(Iteration 78 / 14700) loss: 2.670104\n",
      "(Iteration 79 / 14700) loss: 2.835994\n",
      "(Iteration 80 / 14700) loss: 2.662015\n",
      "(Iteration 81 / 14700) loss: 2.915897\n",
      "(Iteration 82 / 14700) loss: 2.738310\n",
      "(Iteration 83 / 14700) loss: 2.794360\n",
      "(Iteration 84 / 14700) loss: 2.591353\n",
      "(Iteration 85 / 14700) loss: 2.676547\n",
      "(Iteration 86 / 14700) loss: 2.513607\n",
      "(Iteration 87 / 14700) loss: 2.655517\n",
      "(Iteration 88 / 14700) loss: 2.564169\n",
      "(Iteration 89 / 14700) loss: 2.747695\n",
      "(Iteration 90 / 14700) loss: 2.737536\n",
      "(Iteration 91 / 14700) loss: 2.510928\n",
      "(Iteration 92 / 14700) loss: 2.789811\n",
      "(Iteration 93 / 14700) loss: 2.585074\n",
      "(Iteration 94 / 14700) loss: 2.674199\n",
      "(Iteration 95 / 14700) loss: 2.720096\n",
      "(Iteration 96 / 14700) loss: 2.987581\n",
      "(Iteration 97 / 14700) loss: 2.726392\n",
      "(Iteration 98 / 14700) loss: 2.782826\n",
      "(Iteration 99 / 14700) loss: 2.684243\n",
      "(Iteration 100 / 14700) loss: 2.775032\n",
      "(Iteration 101 / 14700) loss: 2.523545\n",
      "(Iteration 102 / 14700) loss: 2.745186\n",
      "(Iteration 103 / 14700) loss: 2.758958\n",
      "(Iteration 104 / 14700) loss: 2.839454\n",
      "(Iteration 105 / 14700) loss: 2.599713\n",
      "(Iteration 106 / 14700) loss: 2.918985\n",
      "(Iteration 107 / 14700) loss: 2.688497\n",
      "(Iteration 108 / 14700) loss: 2.468074\n",
      "(Iteration 109 / 14700) loss: 2.630526\n",
      "(Iteration 110 / 14700) loss: 2.857556\n",
      "(Iteration 111 / 14700) loss: 2.772349\n",
      "(Iteration 112 / 14700) loss: 3.036079\n",
      "(Iteration 113 / 14700) loss: 2.629880\n",
      "(Iteration 114 / 14700) loss: 2.806352\n",
      "(Iteration 115 / 14700) loss: 2.527787\n",
      "(Iteration 116 / 14700) loss: 2.611258\n",
      "(Iteration 117 / 14700) loss: 2.637615\n",
      "(Iteration 118 / 14700) loss: 2.555756\n",
      "(Iteration 119 / 14700) loss: 2.778524\n",
      "(Iteration 120 / 14700) loss: 2.686264\n",
      "(Iteration 121 / 14700) loss: 2.597945\n",
      "(Iteration 122 / 14700) loss: 2.814360\n",
      "(Iteration 123 / 14700) loss: 2.551958\n",
      "(Iteration 124 / 14700) loss: 2.763685\n",
      "(Iteration 125 / 14700) loss: 2.588808\n",
      "(Iteration 126 / 14700) loss: 2.890669\n",
      "(Iteration 127 / 14700) loss: 2.515410\n",
      "(Iteration 128 / 14700) loss: 2.490590\n",
      "(Iteration 129 / 14700) loss: 2.481167\n",
      "(Iteration 130 / 14700) loss: 2.567026\n",
      "(Iteration 131 / 14700) loss: 2.584743\n",
      "(Iteration 132 / 14700) loss: 2.512272\n",
      "(Iteration 133 / 14700) loss: 2.596884\n",
      "(Iteration 134 / 14700) loss: 2.776834\n",
      "(Iteration 135 / 14700) loss: 2.713979\n",
      "(Iteration 136 / 14700) loss: 2.660579\n",
      "(Iteration 137 / 14700) loss: 2.537998\n",
      "(Iteration 138 / 14700) loss: 2.339343\n",
      "(Iteration 139 / 14700) loss: 2.483150\n",
      "(Iteration 140 / 14700) loss: 2.555645\n",
      "(Iteration 141 / 14700) loss: 2.341076\n",
      "(Iteration 142 / 14700) loss: 2.762848\n",
      "(Iteration 143 / 14700) loss: 2.688633\n",
      "(Iteration 144 / 14700) loss: 2.416599\n",
      "(Iteration 145 / 14700) loss: 2.507930\n",
      "(Iteration 146 / 14700) loss: 2.623404\n",
      "(Iteration 147 / 14700) loss: 2.379065\n",
      "(Iteration 148 / 14700) loss: 2.381340\n",
      "(Iteration 149 / 14700) loss: 2.515115\n",
      "(Iteration 150 / 14700) loss: 2.514356\n",
      "(Iteration 151 / 14700) loss: 2.404416\n",
      "(Iteration 152 / 14700) loss: 2.446806\n",
      "(Iteration 153 / 14700) loss: 2.405760\n",
      "(Iteration 154 / 14700) loss: 2.568889\n",
      "(Iteration 155 / 14700) loss: 2.596393\n",
      "(Iteration 156 / 14700) loss: 2.518457\n",
      "(Iteration 157 / 14700) loss: 2.506314\n",
      "(Iteration 158 / 14700) loss: 2.581127\n",
      "(Iteration 159 / 14700) loss: 2.310433\n",
      "(Iteration 160 / 14700) loss: 2.158052\n",
      "(Iteration 161 / 14700) loss: 2.219121\n",
      "(Iteration 162 / 14700) loss: 2.387743\n",
      "(Iteration 163 / 14700) loss: 2.278743\n",
      "(Iteration 164 / 14700) loss: 2.656238\n",
      "(Iteration 165 / 14700) loss: 2.360025\n",
      "(Iteration 166 / 14700) loss: 2.215919\n",
      "(Iteration 167 / 14700) loss: 2.267656\n",
      "(Iteration 168 / 14700) loss: 2.192801\n",
      "(Iteration 169 / 14700) loss: 2.395836\n",
      "(Iteration 170 / 14700) loss: 2.255539\n",
      "(Iteration 171 / 14700) loss: 2.583287\n",
      "(Iteration 172 / 14700) loss: 2.775133\n",
      "(Iteration 173 / 14700) loss: 2.219826\n",
      "(Iteration 174 / 14700) loss: 2.334238\n",
      "(Iteration 175 / 14700) loss: 2.317930\n",
      "(Iteration 176 / 14700) loss: 2.548646\n",
      "(Iteration 177 / 14700) loss: 2.213330\n",
      "(Iteration 178 / 14700) loss: 2.491719\n",
      "(Iteration 179 / 14700) loss: 2.319375\n",
      "(Iteration 180 / 14700) loss: 2.538005\n",
      "(Iteration 181 / 14700) loss: 2.375671\n",
      "(Iteration 182 / 14700) loss: 2.381525\n",
      "(Iteration 183 / 14700) loss: 2.425680\n",
      "(Iteration 184 / 14700) loss: 2.364257\n",
      "(Iteration 185 / 14700) loss: 2.532904\n",
      "(Iteration 186 / 14700) loss: 2.299422\n",
      "(Iteration 187 / 14700) loss: 2.257503\n",
      "(Iteration 188 / 14700) loss: 2.353611\n",
      "(Iteration 189 / 14700) loss: 2.309127\n",
      "(Iteration 190 / 14700) loss: 2.256948\n",
      "(Iteration 191 / 14700) loss: 2.504021\n",
      "(Iteration 192 / 14700) loss: 2.244857\n",
      "(Iteration 193 / 14700) loss: 2.315081\n",
      "(Iteration 194 / 14700) loss: 2.262307\n",
      "(Iteration 195 / 14700) loss: 2.311794\n",
      "(Iteration 196 / 14700) loss: 2.432992\n",
      "(Iteration 197 / 14700) loss: 2.263716\n",
      "(Iteration 198 / 14700) loss: 2.352984\n",
      "(Iteration 199 / 14700) loss: 2.145468\n",
      "(Iteration 200 / 14700) loss: 2.239134\n",
      "(Iteration 201 / 14700) loss: 2.164326\n",
      "(Iteration 202 / 14700) loss: 2.347560\n",
      "(Iteration 203 / 14700) loss: 2.352332\n",
      "(Iteration 204 / 14700) loss: 2.519828\n",
      "(Iteration 205 / 14700) loss: 2.480790\n",
      "(Iteration 206 / 14700) loss: 2.438301\n",
      "(Iteration 207 / 14700) loss: 2.316696\n",
      "(Iteration 208 / 14700) loss: 2.502246\n",
      "(Iteration 209 / 14700) loss: 2.293637\n",
      "(Iteration 210 / 14700) loss: 2.340862\n",
      "(Iteration 211 / 14700) loss: 2.321670\n",
      "(Iteration 212 / 14700) loss: 2.319701\n",
      "(Iteration 213 / 14700) loss: 2.319790\n",
      "(Iteration 214 / 14700) loss: 2.461362\n",
      "(Iteration 215 / 14700) loss: 2.250609\n",
      "(Iteration 216 / 14700) loss: 2.686464\n",
      "(Iteration 217 / 14700) loss: 2.269175\n",
      "(Iteration 218 / 14700) loss: 2.523058\n",
      "(Iteration 219 / 14700) loss: 2.483258\n",
      "(Iteration 220 / 14700) loss: 2.357978\n",
      "(Iteration 221 / 14700) loss: 2.403431\n",
      "(Iteration 222 / 14700) loss: 2.253989\n",
      "(Iteration 223 / 14700) loss: 2.149753\n",
      "(Iteration 224 / 14700) loss: 2.416095\n",
      "(Iteration 225 / 14700) loss: 2.206975\n",
      "(Iteration 226 / 14700) loss: 2.363696\n",
      "(Iteration 227 / 14700) loss: 2.335677\n",
      "(Iteration 228 / 14700) loss: 2.183871\n",
      "(Iteration 229 / 14700) loss: 2.046354\n",
      "(Iteration 230 / 14700) loss: 2.274739\n",
      "(Iteration 231 / 14700) loss: 2.443749\n",
      "(Iteration 232 / 14700) loss: 2.207643\n",
      "(Iteration 233 / 14700) loss: 2.071290\n",
      "(Iteration 234 / 14700) loss: 2.221174\n",
      "(Iteration 235 / 14700) loss: 2.079758\n",
      "(Iteration 236 / 14700) loss: 2.182159\n",
      "(Iteration 237 / 14700) loss: 2.180993\n",
      "(Iteration 238 / 14700) loss: 2.146248\n",
      "(Iteration 239 / 14700) loss: 2.142285\n",
      "(Iteration 240 / 14700) loss: 1.921028\n",
      "(Iteration 241 / 14700) loss: 2.233037\n",
      "(Iteration 242 / 14700) loss: 2.213744\n",
      "(Iteration 243 / 14700) loss: 2.385599\n",
      "(Iteration 244 / 14700) loss: 2.200759\n",
      "(Iteration 245 / 14700) loss: 2.137213\n",
      "(Iteration 246 / 14700) loss: 2.251638\n",
      "(Iteration 247 / 14700) loss: 2.319550\n",
      "(Iteration 248 / 14700) loss: 2.230772\n",
      "(Iteration 249 / 14700) loss: 2.256685\n",
      "(Iteration 250 / 14700) loss: 2.036181\n",
      "(Iteration 251 / 14700) loss: 2.426710\n",
      "(Iteration 252 / 14700) loss: 2.557382\n",
      "(Iteration 253 / 14700) loss: 2.183860\n",
      "(Iteration 254 / 14700) loss: 2.235000\n",
      "(Iteration 255 / 14700) loss: 2.414488\n",
      "(Iteration 256 / 14700) loss: 2.432412\n",
      "(Iteration 257 / 14700) loss: 1.946287\n",
      "(Iteration 258 / 14700) loss: 2.595029\n",
      "(Iteration 259 / 14700) loss: 2.181025\n",
      "(Iteration 260 / 14700) loss: 2.411427\n",
      "(Iteration 261 / 14700) loss: 2.119345\n",
      "(Iteration 262 / 14700) loss: 2.402797\n",
      "(Iteration 263 / 14700) loss: 2.217823\n",
      "(Iteration 264 / 14700) loss: 2.458946\n",
      "(Iteration 265 / 14700) loss: 2.734084\n",
      "(Iteration 266 / 14700) loss: 2.306437\n",
      "(Iteration 267 / 14700) loss: 2.501917\n",
      "(Iteration 268 / 14700) loss: 2.345167\n",
      "(Iteration 269 / 14700) loss: 2.260469\n",
      "(Iteration 270 / 14700) loss: 2.056073\n",
      "(Iteration 271 / 14700) loss: 2.158241\n",
      "(Iteration 272 / 14700) loss: 2.148498\n",
      "(Iteration 273 / 14700) loss: 2.129379\n",
      "(Iteration 274 / 14700) loss: 2.349549\n",
      "(Iteration 275 / 14700) loss: 2.166684\n",
      "(Iteration 276 / 14700) loss: 1.967196\n",
      "(Iteration 277 / 14700) loss: 2.171235\n",
      "(Iteration 278 / 14700) loss: 2.253906\n",
      "(Iteration 279 / 14700) loss: 2.061820\n",
      "(Iteration 280 / 14700) loss: 2.112088\n",
      "(Iteration 281 / 14700) loss: 2.102677\n",
      "(Iteration 282 / 14700) loss: 2.238419\n",
      "(Iteration 283 / 14700) loss: 2.148583\n",
      "(Iteration 284 / 14700) loss: 2.178301\n",
      "(Iteration 285 / 14700) loss: 2.158003\n",
      "(Iteration 286 / 14700) loss: 2.315060\n",
      "(Iteration 287 / 14700) loss: 2.371831\n",
      "(Iteration 288 / 14700) loss: 2.613943\n",
      "(Iteration 289 / 14700) loss: 2.246209\n",
      "(Iteration 290 / 14700) loss: 2.218018\n",
      "(Iteration 291 / 14700) loss: 2.223545\n",
      "(Iteration 292 / 14700) loss: 2.237187\n",
      "(Iteration 293 / 14700) loss: 2.095126\n",
      "(Iteration 294 / 14700) loss: 2.253004\n",
      "(Iteration 295 / 14700) loss: 2.414871\n",
      "(Iteration 296 / 14700) loss: 2.192117\n",
      "(Iteration 297 / 14700) loss: 2.101982\n",
      "(Iteration 298 / 14700) loss: 2.344178\n",
      "(Iteration 299 / 14700) loss: 2.347256\n",
      "(Iteration 300 / 14700) loss: 2.481854\n",
      "(Iteration 301 / 14700) loss: 2.203015\n",
      "(Iteration 302 / 14700) loss: 1.967067\n",
      "(Iteration 303 / 14700) loss: 2.170038\n",
      "(Iteration 304 / 14700) loss: 1.922865\n",
      "(Iteration 305 / 14700) loss: 2.363656\n",
      "(Iteration 306 / 14700) loss: 2.297591\n",
      "(Iteration 307 / 14700) loss: 2.165572\n",
      "(Iteration 308 / 14700) loss: 2.163572\n",
      "(Iteration 309 / 14700) loss: 2.399698\n",
      "(Iteration 310 / 14700) loss: 2.333881\n",
      "(Iteration 311 / 14700) loss: 2.370809\n",
      "(Iteration 312 / 14700) loss: 2.195922\n",
      "(Iteration 313 / 14700) loss: 2.205894\n",
      "(Iteration 314 / 14700) loss: 1.872963\n",
      "(Iteration 315 / 14700) loss: 2.138741\n",
      "(Iteration 316 / 14700) loss: 1.910553\n",
      "(Iteration 317 / 14700) loss: 2.206887\n",
      "(Iteration 318 / 14700) loss: 1.902960\n",
      "(Iteration 319 / 14700) loss: 2.129632\n",
      "(Iteration 320 / 14700) loss: 2.328688\n",
      "(Iteration 321 / 14700) loss: 2.116758\n",
      "(Iteration 322 / 14700) loss: 2.165213\n",
      "(Iteration 323 / 14700) loss: 2.257005\n",
      "(Iteration 324 / 14700) loss: 2.235016\n",
      "(Iteration 325 / 14700) loss: 2.147102\n",
      "(Iteration 326 / 14700) loss: 2.121523\n",
      "(Iteration 327 / 14700) loss: 2.011405\n",
      "(Iteration 328 / 14700) loss: 2.034287\n",
      "(Iteration 329 / 14700) loss: 1.958044\n",
      "(Iteration 330 / 14700) loss: 1.776110\n",
      "(Iteration 331 / 14700) loss: 2.122026\n",
      "(Iteration 332 / 14700) loss: 2.070638\n",
      "(Iteration 333 / 14700) loss: 2.085235\n",
      "(Iteration 334 / 14700) loss: 2.121982\n",
      "(Iteration 335 / 14700) loss: 2.061670\n",
      "(Iteration 336 / 14700) loss: 1.971306\n",
      "(Iteration 337 / 14700) loss: 1.916776\n",
      "(Iteration 338 / 14700) loss: 1.999849\n",
      "(Iteration 339 / 14700) loss: 2.088622\n",
      "(Iteration 340 / 14700) loss: 1.895555\n",
      "(Iteration 341 / 14700) loss: 2.302147\n",
      "(Iteration 342 / 14700) loss: 2.373864\n",
      "(Iteration 343 / 14700) loss: 2.060673\n",
      "(Iteration 344 / 14700) loss: 2.041641\n",
      "(Iteration 345 / 14700) loss: 2.147068\n",
      "(Iteration 346 / 14700) loss: 1.819316\n",
      "(Iteration 347 / 14700) loss: 2.234259\n",
      "(Iteration 348 / 14700) loss: 1.807536\n",
      "(Iteration 349 / 14700) loss: 1.927556\n",
      "(Iteration 350 / 14700) loss: 2.059276\n",
      "(Iteration 351 / 14700) loss: 2.036802\n",
      "(Iteration 352 / 14700) loss: 2.068804\n",
      "(Iteration 353 / 14700) loss: 1.978366\n",
      "(Iteration 354 / 14700) loss: 1.947443\n",
      "(Iteration 355 / 14700) loss: 1.935661\n",
      "(Iteration 356 / 14700) loss: 2.063370\n",
      "(Iteration 357 / 14700) loss: 2.117021\n",
      "(Iteration 358 / 14700) loss: 2.008220\n",
      "(Iteration 359 / 14700) loss: 1.892260\n",
      "(Iteration 360 / 14700) loss: 2.043220\n",
      "(Iteration 361 / 14700) loss: 2.054270\n",
      "(Iteration 362 / 14700) loss: 2.140074\n",
      "(Iteration 363 / 14700) loss: 2.094268\n",
      "(Iteration 364 / 14700) loss: 2.039825\n",
      "(Iteration 365 / 14700) loss: 1.936940\n",
      "(Iteration 366 / 14700) loss: 2.117037\n",
      "(Iteration 367 / 14700) loss: 2.114670\n",
      "(Iteration 368 / 14700) loss: 1.947518\n",
      "(Iteration 369 / 14700) loss: 2.072592\n",
      "(Iteration 370 / 14700) loss: 2.184832\n",
      "(Iteration 371 / 14700) loss: 2.199623\n",
      "(Iteration 372 / 14700) loss: 2.229009\n",
      "(Iteration 373 / 14700) loss: 1.868691\n",
      "(Iteration 374 / 14700) loss: 2.062679\n",
      "(Iteration 375 / 14700) loss: 1.805462\n",
      "(Iteration 376 / 14700) loss: 1.976473\n",
      "(Iteration 377 / 14700) loss: 2.015693\n",
      "(Iteration 378 / 14700) loss: 2.061535\n",
      "(Iteration 379 / 14700) loss: 2.077235\n",
      "(Iteration 380 / 14700) loss: 1.847830\n",
      "(Iteration 381 / 14700) loss: 1.840796\n",
      "(Iteration 382 / 14700) loss: 2.121797\n",
      "(Iteration 383 / 14700) loss: 1.950721\n",
      "(Iteration 384 / 14700) loss: 2.152866\n",
      "(Iteration 385 / 14700) loss: 2.022391\n",
      "(Iteration 386 / 14700) loss: 1.821064\n",
      "(Iteration 387 / 14700) loss: 2.062355\n",
      "(Iteration 388 / 14700) loss: 2.085547\n",
      "(Iteration 389 / 14700) loss: 1.966600\n",
      "(Iteration 390 / 14700) loss: 1.834349\n",
      "(Iteration 391 / 14700) loss: 2.362641\n",
      "(Iteration 392 / 14700) loss: 1.949656\n",
      "(Iteration 393 / 14700) loss: 2.077642\n",
      "(Iteration 394 / 14700) loss: 1.895265\n",
      "(Iteration 395 / 14700) loss: 1.956458\n",
      "(Iteration 396 / 14700) loss: 1.712665\n",
      "(Iteration 397 / 14700) loss: 2.154670\n",
      "(Iteration 398 / 14700) loss: 1.816631\n",
      "(Iteration 399 / 14700) loss: 1.908005\n",
      "(Iteration 400 / 14700) loss: 2.232814\n",
      "(Iteration 401 / 14700) loss: 2.134895\n",
      "(Iteration 402 / 14700) loss: 2.104230\n",
      "(Iteration 403 / 14700) loss: 1.576438\n",
      "(Iteration 404 / 14700) loss: 2.277547\n",
      "(Iteration 405 / 14700) loss: 2.011178\n",
      "(Iteration 406 / 14700) loss: 2.060522\n",
      "(Iteration 407 / 14700) loss: 2.203170\n",
      "(Iteration 408 / 14700) loss: 2.026203\n",
      "(Iteration 409 / 14700) loss: 1.893261\n",
      "(Iteration 410 / 14700) loss: 2.046756\n",
      "(Iteration 411 / 14700) loss: 2.236894\n",
      "(Iteration 412 / 14700) loss: 2.028527\n",
      "(Iteration 413 / 14700) loss: 1.884147\n",
      "(Iteration 414 / 14700) loss: 1.672880\n",
      "(Iteration 415 / 14700) loss: 2.054280\n",
      "(Iteration 416 / 14700) loss: 1.978031\n",
      "(Iteration 417 / 14700) loss: 2.097042\n",
      "(Iteration 418 / 14700) loss: 2.095017\n",
      "(Iteration 419 / 14700) loss: 1.971681\n",
      "(Iteration 420 / 14700) loss: 1.855249\n",
      "(Iteration 421 / 14700) loss: 1.768421\n",
      "(Iteration 422 / 14700) loss: 1.911841\n",
      "(Iteration 423 / 14700) loss: 1.803483\n",
      "(Iteration 424 / 14700) loss: 1.887244\n",
      "(Iteration 425 / 14700) loss: 1.878877\n",
      "(Iteration 426 / 14700) loss: 2.128018\n",
      "(Iteration 427 / 14700) loss: 1.992483\n",
      "(Iteration 428 / 14700) loss: 1.901985\n",
      "(Iteration 429 / 14700) loss: 1.861719\n",
      "(Iteration 430 / 14700) loss: 1.997111\n",
      "(Iteration 431 / 14700) loss: 2.180049\n",
      "(Iteration 432 / 14700) loss: 1.853457\n",
      "(Iteration 433 / 14700) loss: 2.351396\n",
      "(Iteration 434 / 14700) loss: 2.099729\n",
      "(Iteration 435 / 14700) loss: 2.033557\n",
      "(Iteration 436 / 14700) loss: 1.849993\n",
      "(Iteration 437 / 14700) loss: 1.845945\n",
      "(Iteration 438 / 14700) loss: 1.741967\n",
      "(Iteration 439 / 14700) loss: 1.972719\n",
      "(Iteration 440 / 14700) loss: 1.744517\n",
      "(Iteration 441 / 14700) loss: 1.765518\n",
      "(Iteration 442 / 14700) loss: 2.122262\n",
      "(Iteration 443 / 14700) loss: 1.873034\n",
      "(Iteration 444 / 14700) loss: 2.066342\n",
      "(Iteration 445 / 14700) loss: 1.599731\n",
      "(Iteration 446 / 14700) loss: 1.648950\n",
      "(Iteration 447 / 14700) loss: 1.886274\n",
      "(Iteration 448 / 14700) loss: 2.070026\n",
      "(Iteration 449 / 14700) loss: 1.785474\n",
      "(Iteration 450 / 14700) loss: 1.618884\n",
      "(Iteration 451 / 14700) loss: 2.020539\n",
      "(Iteration 452 / 14700) loss: 1.892961\n",
      "(Iteration 453 / 14700) loss: 1.751433\n",
      "(Iteration 454 / 14700) loss: 1.640732\n",
      "(Iteration 455 / 14700) loss: 1.824756\n",
      "(Iteration 456 / 14700) loss: 2.064561\n",
      "(Iteration 457 / 14700) loss: 1.962588\n",
      "(Iteration 458 / 14700) loss: 2.058677\n",
      "(Iteration 459 / 14700) loss: 1.932777\n",
      "(Iteration 460 / 14700) loss: 1.669473\n",
      "(Iteration 461 / 14700) loss: 2.274442\n",
      "(Iteration 462 / 14700) loss: 1.998666\n",
      "(Iteration 463 / 14700) loss: 2.066361\n",
      "(Iteration 464 / 14700) loss: 2.187104\n",
      "(Iteration 465 / 14700) loss: 1.921898\n",
      "(Iteration 466 / 14700) loss: 1.751805\n",
      "(Iteration 467 / 14700) loss: 1.803447\n",
      "(Iteration 468 / 14700) loss: 1.735665\n",
      "(Iteration 469 / 14700) loss: 1.990483\n",
      "(Iteration 470 / 14700) loss: 1.792586\n",
      "(Iteration 471 / 14700) loss: 1.975923\n",
      "(Iteration 472 / 14700) loss: 1.992024\n",
      "(Iteration 473 / 14700) loss: 1.920410\n",
      "(Iteration 474 / 14700) loss: 1.895000\n",
      "(Iteration 475 / 14700) loss: 2.095530\n",
      "(Iteration 476 / 14700) loss: 1.948209\n",
      "(Iteration 477 / 14700) loss: 2.000809\n",
      "(Iteration 478 / 14700) loss: 1.754706\n",
      "(Iteration 479 / 14700) loss: 2.175861\n",
      "(Iteration 480 / 14700) loss: 2.199661\n",
      "(Iteration 481 / 14700) loss: 1.828586\n",
      "(Iteration 482 / 14700) loss: 1.902753\n",
      "(Iteration 483 / 14700) loss: 1.785490\n",
      "(Iteration 484 / 14700) loss: 1.659786\n",
      "(Iteration 485 / 14700) loss: 1.823641\n",
      "(Iteration 486 / 14700) loss: 2.101317\n",
      "(Iteration 487 / 14700) loss: 1.682183\n",
      "(Iteration 488 / 14700) loss: 1.864469\n",
      "(Iteration 489 / 14700) loss: 1.763787\n",
      "(Iteration 490 / 14700) loss: 1.863112\n",
      "(Iteration 491 / 14700) loss: 1.782526\n",
      "(Iteration 492 / 14700) loss: 1.737797\n",
      "(Iteration 493 / 14700) loss: 1.719494\n",
      "(Iteration 494 / 14700) loss: 1.842042\n",
      "(Iteration 495 / 14700) loss: 1.892334\n",
      "(Iteration 496 / 14700) loss: 1.933938\n",
      "(Iteration 497 / 14700) loss: 1.796985\n",
      "(Iteration 498 / 14700) loss: 1.742836\n",
      "(Iteration 499 / 14700) loss: 1.815215\n",
      "(Iteration 500 / 14700) loss: 1.507747\n",
      "(Iteration 501 / 14700) loss: 1.843286\n",
      "(Iteration 502 / 14700) loss: 1.896803\n",
      "(Iteration 503 / 14700) loss: 1.956364\n",
      "(Iteration 504 / 14700) loss: 2.059628\n",
      "(Iteration 505 / 14700) loss: 2.067212\n",
      "(Iteration 506 / 14700) loss: 1.869972\n",
      "(Iteration 507 / 14700) loss: 1.853570\n",
      "(Iteration 508 / 14700) loss: 1.880238\n",
      "(Iteration 509 / 14700) loss: 1.746968\n",
      "(Iteration 510 / 14700) loss: 1.714306\n",
      "(Iteration 511 / 14700) loss: 1.870918\n",
      "(Iteration 512 / 14700) loss: 1.807165\n",
      "(Iteration 513 / 14700) loss: 1.866450\n",
      "(Iteration 514 / 14700) loss: 1.699700\n",
      "(Iteration 515 / 14700) loss: 2.117975\n",
      "(Iteration 516 / 14700) loss: 1.778694\n",
      "(Iteration 517 / 14700) loss: 1.878545\n",
      "(Iteration 518 / 14700) loss: 1.565724\n",
      "(Iteration 519 / 14700) loss: 1.712490\n",
      "(Iteration 520 / 14700) loss: 1.549099\n",
      "(Iteration 521 / 14700) loss: 1.556783\n",
      "(Iteration 522 / 14700) loss: 2.050323\n",
      "(Iteration 523 / 14700) loss: 1.949828\n",
      "(Iteration 524 / 14700) loss: 1.560648\n",
      "(Iteration 525 / 14700) loss: 2.052716\n",
      "(Iteration 526 / 14700) loss: 1.614330\n",
      "(Iteration 527 / 14700) loss: 1.541375\n",
      "(Iteration 528 / 14700) loss: 2.013209\n",
      "(Iteration 529 / 14700) loss: 1.879914\n",
      "(Iteration 530 / 14700) loss: 1.877773\n",
      "(Iteration 531 / 14700) loss: 1.870175\n",
      "(Iteration 532 / 14700) loss: 2.115548\n",
      "(Iteration 533 / 14700) loss: 1.841320\n",
      "(Iteration 534 / 14700) loss: 1.806809\n",
      "(Iteration 535 / 14700) loss: 1.858617\n",
      "(Iteration 536 / 14700) loss: 1.671777\n",
      "(Iteration 537 / 14700) loss: 1.610033\n",
      "(Iteration 538 / 14700) loss: 1.436471\n",
      "(Iteration 539 / 14700) loss: 1.871005\n",
      "(Iteration 540 / 14700) loss: 1.876552\n",
      "(Iteration 541 / 14700) loss: 1.692454\n",
      "(Iteration 542 / 14700) loss: 1.647551\n",
      "(Iteration 543 / 14700) loss: 1.938610\n",
      "(Iteration 544 / 14700) loss: 1.468345\n",
      "(Iteration 545 / 14700) loss: 1.551379\n",
      "(Iteration 546 / 14700) loss: 2.038194\n",
      "(Iteration 547 / 14700) loss: 1.935906\n",
      "(Iteration 548 / 14700) loss: 1.826206\n",
      "(Iteration 549 / 14700) loss: 1.876113\n",
      "(Iteration 550 / 14700) loss: 1.873628\n",
      "(Iteration 551 / 14700) loss: 1.630260\n",
      "(Iteration 552 / 14700) loss: 1.931145\n",
      "(Iteration 553 / 14700) loss: 1.440054\n",
      "(Iteration 554 / 14700) loss: 1.548291\n",
      "(Iteration 555 / 14700) loss: 1.800551\n",
      "(Iteration 556 / 14700) loss: 1.684980\n",
      "(Iteration 557 / 14700) loss: 2.113054\n",
      "(Iteration 558 / 14700) loss: 1.774298\n",
      "(Iteration 559 / 14700) loss: 1.741561\n",
      "(Iteration 560 / 14700) loss: 1.506571\n",
      "(Iteration 561 / 14700) loss: 1.992871\n",
      "(Iteration 562 / 14700) loss: 1.663925\n",
      "(Iteration 563 / 14700) loss: 1.800407\n",
      "(Iteration 564 / 14700) loss: 1.839287\n",
      "(Iteration 565 / 14700) loss: 2.240539\n",
      "(Iteration 566 / 14700) loss: 1.806744\n",
      "(Iteration 567 / 14700) loss: 2.097424\n",
      "(Iteration 568 / 14700) loss: 1.904036\n",
      "(Iteration 569 / 14700) loss: 1.760915\n",
      "(Iteration 570 / 14700) loss: 1.806998\n",
      "(Iteration 571 / 14700) loss: 1.934576\n",
      "(Iteration 572 / 14700) loss: 1.639570\n",
      "(Iteration 573 / 14700) loss: 1.911813\n",
      "(Iteration 574 / 14700) loss: 2.034822\n",
      "(Iteration 575 / 14700) loss: 1.763367\n",
      "(Iteration 576 / 14700) loss: 1.633061\n",
      "(Iteration 577 / 14700) loss: 1.828821\n",
      "(Iteration 578 / 14700) loss: 1.578782\n",
      "(Iteration 579 / 14700) loss: 1.818236\n",
      "(Iteration 580 / 14700) loss: 1.551601\n",
      "(Iteration 581 / 14700) loss: 1.725331\n",
      "(Iteration 582 / 14700) loss: 1.608352\n",
      "(Iteration 583 / 14700) loss: 1.643113\n",
      "(Iteration 584 / 14700) loss: 1.766043\n",
      "(Iteration 585 / 14700) loss: 1.856574\n",
      "(Iteration 586 / 14700) loss: 1.661965\n",
      "(Iteration 587 / 14700) loss: 1.606066\n",
      "(Iteration 588 / 14700) loss: 1.573262\n",
      "(Iteration 589 / 14700) loss: 1.726323\n",
      "(Iteration 590 / 14700) loss: 1.658067\n",
      "(Iteration 591 / 14700) loss: 1.712636\n",
      "(Iteration 592 / 14700) loss: 1.456237\n",
      "(Iteration 593 / 14700) loss: 1.951865\n",
      "(Iteration 594 / 14700) loss: 1.963946\n",
      "(Iteration 595 / 14700) loss: 1.988943\n",
      "(Iteration 596 / 14700) loss: 1.804572\n",
      "(Iteration 597 / 14700) loss: 1.719431\n",
      "(Iteration 598 / 14700) loss: 1.718861\n",
      "(Iteration 599 / 14700) loss: 1.878264\n",
      "(Iteration 600 / 14700) loss: 1.787427\n",
      "(Iteration 601 / 14700) loss: 1.675512\n",
      "(Iteration 602 / 14700) loss: 1.834890\n",
      "(Iteration 603 / 14700) loss: 1.743266\n",
      "(Iteration 604 / 14700) loss: 1.606337\n",
      "(Iteration 605 / 14700) loss: 2.046203\n",
      "(Iteration 606 / 14700) loss: 2.191992\n",
      "(Iteration 607 / 14700) loss: 1.824631\n",
      "(Iteration 608 / 14700) loss: 1.748009\n",
      "(Iteration 609 / 14700) loss: 1.589926\n",
      "(Iteration 610 / 14700) loss: 1.463259\n",
      "(Iteration 611 / 14700) loss: 1.636127\n",
      "(Iteration 612 / 14700) loss: 1.596310\n",
      "(Iteration 613 / 14700) loss: 1.647404\n",
      "(Iteration 614 / 14700) loss: 1.767339\n",
      "(Iteration 615 / 14700) loss: 1.673408\n",
      "(Iteration 616 / 14700) loss: 1.618658\n",
      "(Iteration 617 / 14700) loss: 1.664892\n",
      "(Iteration 618 / 14700) loss: 1.786350\n",
      "(Iteration 619 / 14700) loss: 1.743321\n",
      "(Iteration 620 / 14700) loss: 1.542800\n",
      "(Iteration 621 / 14700) loss: 1.638937\n",
      "(Iteration 622 / 14700) loss: 1.530034\n",
      "(Iteration 623 / 14700) loss: 1.941662\n",
      "(Iteration 624 / 14700) loss: 1.913878\n",
      "(Iteration 625 / 14700) loss: 1.903214\n",
      "(Iteration 626 / 14700) loss: 1.725100\n",
      "(Iteration 627 / 14700) loss: 1.418752\n",
      "(Iteration 628 / 14700) loss: 1.764340\n",
      "(Iteration 629 / 14700) loss: 2.042730\n",
      "(Iteration 630 / 14700) loss: 1.532383\n",
      "(Iteration 631 / 14700) loss: 1.583897\n",
      "(Iteration 632 / 14700) loss: 1.627485\n",
      "(Iteration 633 / 14700) loss: 1.599156\n",
      "(Iteration 634 / 14700) loss: 1.696790\n",
      "(Iteration 635 / 14700) loss: 1.649613\n",
      "(Iteration 636 / 14700) loss: 1.551672\n",
      "(Iteration 637 / 14700) loss: 1.716509\n",
      "(Iteration 638 / 14700) loss: 1.783513\n",
      "(Iteration 639 / 14700) loss: 1.590898\n",
      "(Iteration 640 / 14700) loss: 1.892750\n",
      "(Iteration 641 / 14700) loss: 1.428390\n",
      "(Iteration 642 / 14700) loss: 1.488520\n",
      "(Iteration 643 / 14700) loss: 1.469821\n",
      "(Iteration 644 / 14700) loss: 1.791987\n",
      "(Iteration 645 / 14700) loss: 1.760327\n",
      "(Iteration 646 / 14700) loss: 1.592879\n",
      "(Iteration 647 / 14700) loss: 1.411781\n",
      "(Iteration 648 / 14700) loss: 1.777505\n",
      "(Iteration 649 / 14700) loss: 1.712566\n",
      "(Iteration 650 / 14700) loss: 1.501984\n",
      "(Iteration 651 / 14700) loss: 1.385208\n",
      "(Iteration 652 / 14700) loss: 1.450181\n",
      "(Iteration 653 / 14700) loss: 1.526476\n",
      "(Iteration 654 / 14700) loss: 1.647483\n",
      "(Iteration 655 / 14700) loss: 1.324605\n",
      "(Iteration 656 / 14700) loss: 1.533073\n",
      "(Iteration 657 / 14700) loss: 1.458878\n",
      "(Iteration 658 / 14700) loss: 1.812167\n",
      "(Iteration 659 / 14700) loss: 2.008610\n",
      "(Iteration 660 / 14700) loss: 1.659256\n",
      "(Iteration 661 / 14700) loss: 1.470146\n",
      "(Iteration 662 / 14700) loss: 1.312283\n",
      "(Iteration 663 / 14700) loss: 1.733932\n",
      "(Iteration 664 / 14700) loss: 1.545089\n",
      "(Iteration 665 / 14700) loss: 1.367906\n",
      "(Iteration 666 / 14700) loss: 1.409050\n",
      "(Iteration 667 / 14700) loss: 1.659874\n",
      "(Iteration 668 / 14700) loss: 1.541354\n",
      "(Iteration 669 / 14700) loss: 1.648638\n",
      "(Iteration 670 / 14700) loss: 1.680657\n",
      "(Iteration 671 / 14700) loss: 1.991263\n",
      "(Iteration 672 / 14700) loss: 1.641958\n",
      "(Iteration 673 / 14700) loss: 1.686736\n",
      "(Iteration 674 / 14700) loss: 1.626812\n",
      "(Iteration 675 / 14700) loss: 1.486675\n",
      "(Iteration 676 / 14700) loss: 1.843711\n",
      "(Iteration 677 / 14700) loss: 1.715980\n",
      "(Iteration 678 / 14700) loss: 1.976093\n",
      "(Iteration 679 / 14700) loss: 1.455310\n",
      "(Iteration 680 / 14700) loss: 1.872571\n",
      "(Iteration 681 / 14700) loss: 1.661805\n",
      "(Iteration 682 / 14700) loss: 1.493802\n",
      "(Iteration 683 / 14700) loss: 1.522193\n",
      "(Iteration 684 / 14700) loss: 1.454176\n",
      "(Iteration 685 / 14700) loss: 1.402281\n",
      "(Iteration 686 / 14700) loss: 1.603244\n",
      "(Iteration 687 / 14700) loss: 1.616666\n",
      "(Iteration 688 / 14700) loss: 1.530753\n",
      "(Iteration 689 / 14700) loss: 1.539578\n",
      "(Iteration 690 / 14700) loss: 1.414006\n",
      "(Iteration 691 / 14700) loss: 1.441539\n",
      "(Iteration 692 / 14700) loss: 1.957066\n",
      "(Iteration 693 / 14700) loss: 1.499673\n",
      "(Iteration 694 / 14700) loss: 1.502451\n",
      "(Iteration 695 / 14700) loss: 1.310939\n",
      "(Iteration 696 / 14700) loss: 1.546800\n",
      "(Iteration 697 / 14700) loss: 1.583224\n",
      "(Iteration 698 / 14700) loss: 1.819294\n",
      "(Iteration 699 / 14700) loss: 1.697769\n",
      "(Iteration 700 / 14700) loss: 1.522893\n",
      "(Iteration 701 / 14700) loss: 1.504474\n",
      "(Iteration 702 / 14700) loss: 1.859330\n",
      "(Iteration 703 / 14700) loss: 1.783594\n",
      "(Iteration 704 / 14700) loss: 1.768804\n",
      "(Iteration 705 / 14700) loss: 1.607181\n",
      "(Iteration 706 / 14700) loss: 1.691933\n",
      "(Iteration 707 / 14700) loss: 1.570426\n",
      "(Iteration 708 / 14700) loss: 1.671560\n",
      "(Iteration 709 / 14700) loss: 1.617405\n",
      "(Iteration 710 / 14700) loss: 1.887475\n",
      "(Iteration 711 / 14700) loss: 1.537212\n",
      "(Iteration 712 / 14700) loss: 1.713206\n",
      "(Iteration 713 / 14700) loss: 1.489054\n",
      "(Iteration 714 / 14700) loss: 1.637284\n",
      "(Iteration 715 / 14700) loss: 1.903387\n",
      "(Iteration 716 / 14700) loss: 1.627561\n",
      "(Iteration 717 / 14700) loss: 1.531158\n",
      "(Iteration 718 / 14700) loss: 1.548234\n",
      "(Iteration 719 / 14700) loss: 1.859471\n",
      "(Iteration 720 / 14700) loss: 1.644310\n",
      "(Iteration 721 / 14700) loss: 1.644147\n",
      "(Iteration 722 / 14700) loss: 1.920021\n",
      "(Iteration 723 / 14700) loss: 1.403914\n",
      "(Iteration 724 / 14700) loss: 1.617221\n",
      "(Iteration 725 / 14700) loss: 1.691258\n",
      "(Iteration 726 / 14700) loss: 1.729109\n",
      "(Iteration 727 / 14700) loss: 1.671787\n",
      "(Iteration 728 / 14700) loss: 1.350476\n",
      "(Iteration 729 / 14700) loss: 1.680625\n",
      "(Iteration 730 / 14700) loss: 1.494922\n",
      "(Iteration 731 / 14700) loss: 1.819319\n",
      "(Iteration 732 / 14700) loss: 1.767398\n",
      "(Iteration 733 / 14700) loss: 1.951736\n",
      "(Iteration 734 / 14700) loss: 1.717462\n",
      "(Iteration 735 / 14700) loss: 1.391337\n",
      "(Iteration 736 / 14700) loss: 1.663973\n",
      "(Iteration 737 / 14700) loss: 1.341340\n",
      "(Iteration 738 / 14700) loss: 1.538305\n",
      "(Iteration 739 / 14700) loss: 1.828816\n",
      "(Iteration 740 / 14700) loss: 1.712150\n",
      "(Iteration 741 / 14700) loss: 1.711230\n",
      "(Iteration 742 / 14700) loss: 1.378050\n",
      "(Iteration 743 / 14700) loss: 1.548520\n",
      "(Iteration 744 / 14700) loss: 1.329968\n",
      "(Iteration 745 / 14700) loss: 1.406889\n",
      "(Iteration 746 / 14700) loss: 1.382995\n",
      "(Iteration 747 / 14700) loss: 1.494608\n",
      "(Iteration 748 / 14700) loss: 1.440063\n",
      "(Iteration 749 / 14700) loss: 1.693203\n",
      "(Iteration 750 / 14700) loss: 1.653124\n",
      "(Iteration 751 / 14700) loss: 1.466409\n",
      "(Iteration 752 / 14700) loss: 1.370665\n",
      "(Iteration 753 / 14700) loss: 1.857845\n",
      "(Iteration 754 / 14700) loss: 1.801350\n",
      "(Iteration 755 / 14700) loss: 1.255228\n",
      "(Iteration 756 / 14700) loss: 1.532859\n",
      "(Iteration 757 / 14700) loss: 1.624974\n",
      "(Iteration 758 / 14700) loss: 1.623956\n",
      "(Iteration 759 / 14700) loss: 1.938334\n",
      "(Iteration 760 / 14700) loss: 1.616777\n",
      "(Iteration 761 / 14700) loss: 1.813934\n",
      "(Iteration 762 / 14700) loss: 1.479490\n",
      "(Iteration 763 / 14700) loss: 1.847126\n",
      "(Iteration 764 / 14700) loss: 1.625474\n",
      "(Iteration 765 / 14700) loss: 1.458727\n",
      "(Iteration 766 / 14700) loss: 1.873292\n",
      "(Iteration 767 / 14700) loss: 1.745068\n",
      "(Iteration 768 / 14700) loss: 1.559228\n",
      "(Iteration 769 / 14700) loss: 1.596198\n",
      "(Iteration 770 / 14700) loss: 1.598099\n",
      "(Iteration 771 / 14700) loss: 1.924650\n",
      "(Iteration 772 / 14700) loss: 1.454772\n",
      "(Iteration 773 / 14700) loss: 1.770350\n",
      "(Iteration 774 / 14700) loss: 1.656616\n",
      "(Iteration 775 / 14700) loss: 1.568597\n",
      "(Iteration 776 / 14700) loss: 1.805860\n",
      "(Iteration 777 / 14700) loss: 1.463145\n",
      "(Iteration 778 / 14700) loss: 1.768987\n",
      "(Iteration 779 / 14700) loss: 1.385921\n",
      "(Iteration 780 / 14700) loss: 1.626407\n",
      "(Iteration 781 / 14700) loss: 1.565943\n",
      "(Iteration 782 / 14700) loss: 1.506999\n",
      "(Iteration 783 / 14700) loss: 1.400707\n",
      "(Iteration 784 / 14700) loss: 1.660541\n",
      "(Iteration 785 / 14700) loss: 1.698197\n",
      "(Iteration 786 / 14700) loss: 1.788936\n",
      "(Iteration 787 / 14700) loss: 1.423486\n",
      "(Iteration 788 / 14700) loss: 1.712360\n",
      "(Iteration 789 / 14700) loss: 1.459357\n",
      "(Iteration 790 / 14700) loss: 1.728635\n",
      "(Iteration 791 / 14700) loss: 1.587638\n",
      "(Iteration 792 / 14700) loss: 1.596058\n",
      "(Iteration 793 / 14700) loss: 1.611957\n",
      "(Iteration 794 / 14700) loss: 1.400831\n",
      "(Iteration 795 / 14700) loss: 1.429791\n",
      "(Iteration 796 / 14700) loss: 1.643397\n",
      "(Iteration 797 / 14700) loss: 1.567611\n",
      "(Iteration 798 / 14700) loss: 1.650922\n",
      "(Iteration 799 / 14700) loss: 1.414135\n",
      "(Iteration 800 / 14700) loss: 1.492120\n",
      "(Iteration 801 / 14700) loss: 1.809556\n",
      "(Iteration 802 / 14700) loss: 1.831424\n",
      "(Iteration 803 / 14700) loss: 1.545203\n",
      "(Iteration 804 / 14700) loss: 1.682756\n",
      "(Iteration 805 / 14700) loss: 1.736417\n",
      "(Iteration 806 / 14700) loss: 1.350882\n",
      "(Iteration 807 / 14700) loss: 1.438067\n",
      "(Iteration 808 / 14700) loss: 1.727605\n",
      "(Iteration 809 / 14700) loss: 1.537922\n",
      "(Iteration 810 / 14700) loss: 1.846425\n",
      "(Iteration 811 / 14700) loss: 1.767873\n",
      "(Iteration 812 / 14700) loss: 1.286458\n",
      "(Iteration 813 / 14700) loss: 1.331905\n",
      "(Iteration 814 / 14700) loss: 1.619929\n",
      "(Iteration 815 / 14700) loss: 1.226586\n",
      "(Iteration 816 / 14700) loss: 1.657162\n",
      "(Iteration 817 / 14700) loss: 1.745881\n",
      "(Iteration 818 / 14700) loss: 1.549626\n",
      "(Iteration 819 / 14700) loss: 1.468756\n",
      "(Iteration 820 / 14700) loss: 1.470459\n",
      "(Iteration 821 / 14700) loss: 1.413618\n",
      "(Iteration 822 / 14700) loss: 1.646973\n",
      "(Iteration 823 / 14700) loss: 1.778460\n",
      "(Iteration 824 / 14700) loss: 1.382500\n",
      "(Iteration 825 / 14700) loss: 1.266396\n",
      "(Iteration 826 / 14700) loss: 1.370956\n",
      "(Iteration 827 / 14700) loss: 1.393034\n",
      "(Iteration 828 / 14700) loss: 1.822964\n",
      "(Iteration 829 / 14700) loss: 1.748935\n",
      "(Iteration 830 / 14700) loss: 1.372081\n",
      "(Iteration 831 / 14700) loss: 1.417566\n",
      "(Iteration 832 / 14700) loss: 1.490267\n",
      "(Iteration 833 / 14700) loss: 1.385335\n",
      "(Iteration 834 / 14700) loss: 1.508785\n",
      "(Iteration 835 / 14700) loss: 1.408486\n",
      "(Iteration 836 / 14700) loss: 1.506875\n",
      "(Iteration 837 / 14700) loss: 1.354680\n",
      "(Iteration 838 / 14700) loss: 1.332849\n",
      "(Iteration 839 / 14700) loss: 1.475080\n",
      "(Iteration 840 / 14700) loss: 1.444225\n",
      "(Iteration 841 / 14700) loss: 1.629826\n",
      "(Iteration 842 / 14700) loss: 1.411776\n",
      "(Iteration 843 / 14700) loss: 1.495400\n",
      "(Iteration 844 / 14700) loss: 1.384809\n",
      "(Iteration 845 / 14700) loss: 1.569821\n",
      "(Iteration 846 / 14700) loss: 1.590247\n",
      "(Iteration 847 / 14700) loss: 1.418123\n",
      "(Iteration 848 / 14700) loss: 1.465729\n",
      "(Iteration 849 / 14700) loss: 1.413483\n",
      "(Iteration 850 / 14700) loss: 1.486567\n",
      "(Iteration 851 / 14700) loss: 1.458610\n",
      "(Iteration 852 / 14700) loss: 1.433877\n",
      "(Iteration 853 / 14700) loss: 1.602623\n",
      "(Iteration 854 / 14700) loss: 1.341596\n",
      "(Iteration 855 / 14700) loss: 1.630403\n",
      "(Iteration 856 / 14700) loss: 1.725033\n",
      "(Iteration 857 / 14700) loss: 1.608010\n",
      "(Iteration 858 / 14700) loss: 1.444322\n",
      "(Iteration 859 / 14700) loss: 1.514547\n",
      "(Iteration 860 / 14700) loss: 1.486073\n",
      "(Iteration 861 / 14700) loss: 1.491523\n",
      "(Iteration 862 / 14700) loss: 1.388509\n",
      "(Iteration 863 / 14700) loss: 1.581926\n",
      "(Iteration 864 / 14700) loss: 1.461404\n",
      "(Iteration 865 / 14700) loss: 1.462774\n",
      "(Iteration 866 / 14700) loss: 1.544648\n",
      "(Iteration 867 / 14700) loss: 1.617884\n",
      "(Iteration 868 / 14700) loss: 1.333856\n",
      "(Iteration 869 / 14700) loss: 1.360787\n",
      "(Iteration 870 / 14700) loss: 1.246772\n",
      "(Iteration 871 / 14700) loss: 1.760220\n",
      "(Iteration 872 / 14700) loss: 1.421187\n",
      "(Iteration 873 / 14700) loss: 1.502158\n",
      "(Iteration 874 / 14700) loss: 1.565163\n",
      "(Iteration 875 / 14700) loss: 1.666681\n",
      "(Iteration 876 / 14700) loss: 1.560584\n",
      "(Iteration 877 / 14700) loss: 1.689211\n",
      "(Iteration 878 / 14700) loss: 1.247229\n",
      "(Iteration 879 / 14700) loss: 1.501753\n",
      "(Iteration 880 / 14700) loss: 1.324749\n",
      "(Iteration 881 / 14700) loss: 1.554122\n",
      "(Iteration 882 / 14700) loss: 1.786698\n",
      "(Iteration 883 / 14700) loss: 1.435583\n",
      "(Iteration 884 / 14700) loss: 1.145985\n",
      "(Iteration 885 / 14700) loss: 1.809493\n",
      "(Iteration 886 / 14700) loss: 1.481297\n",
      "(Iteration 887 / 14700) loss: 1.417067\n",
      "(Iteration 888 / 14700) loss: 1.391157\n",
      "(Iteration 889 / 14700) loss: 1.385960\n",
      "(Iteration 890 / 14700) loss: 1.561005\n",
      "(Iteration 891 / 14700) loss: 1.520154\n",
      "(Iteration 892 / 14700) loss: 1.431132\n",
      "(Iteration 893 / 14700) loss: 1.622546\n",
      "(Iteration 894 / 14700) loss: 1.705867\n",
      "(Iteration 895 / 14700) loss: 1.181159\n",
      "(Iteration 896 / 14700) loss: 1.234530\n",
      "(Iteration 897 / 14700) loss: 1.237366\n",
      "(Iteration 898 / 14700) loss: 1.279137\n",
      "(Iteration 899 / 14700) loss: 1.349465\n",
      "(Iteration 900 / 14700) loss: 1.643056\n",
      "(Iteration 901 / 14700) loss: 1.463207\n",
      "(Iteration 902 / 14700) loss: 1.676115\n",
      "(Iteration 903 / 14700) loss: 1.437391\n",
      "(Iteration 904 / 14700) loss: 1.436545\n",
      "(Iteration 905 / 14700) loss: 1.354880\n",
      "(Iteration 906 / 14700) loss: 1.519920\n",
      "(Iteration 907 / 14700) loss: 1.445668\n",
      "(Iteration 908 / 14700) loss: 1.531245\n",
      "(Iteration 909 / 14700) loss: 1.375075\n",
      "(Iteration 910 / 14700) loss: 1.342920\n",
      "(Iteration 911 / 14700) loss: 1.495092\n",
      "(Iteration 912 / 14700) loss: 1.501353\n",
      "(Iteration 913 / 14700) loss: 1.462753\n",
      "(Iteration 914 / 14700) loss: 1.427938\n",
      "(Iteration 915 / 14700) loss: 1.330069\n",
      "(Iteration 916 / 14700) loss: 1.828766\n",
      "(Iteration 917 / 14700) loss: 1.313864\n",
      "(Iteration 918 / 14700) loss: 1.560012\n",
      "(Iteration 919 / 14700) loss: 1.197253\n",
      "(Iteration 920 / 14700) loss: 1.379614\n",
      "(Iteration 921 / 14700) loss: 1.570496\n",
      "(Iteration 922 / 14700) loss: 1.293891\n",
      "(Iteration 923 / 14700) loss: 1.413976\n",
      "(Iteration 924 / 14700) loss: 1.626612\n",
      "(Iteration 925 / 14700) loss: 1.393205\n",
      "(Iteration 926 / 14700) loss: 1.292679\n",
      "(Iteration 927 / 14700) loss: 1.385737\n",
      "(Iteration 928 / 14700) loss: 1.605105\n",
      "(Iteration 929 / 14700) loss: 1.357476\n",
      "(Iteration 930 / 14700) loss: 1.331614\n",
      "(Iteration 931 / 14700) loss: 1.474409\n",
      "(Iteration 932 / 14700) loss: 1.571235\n",
      "(Iteration 933 / 14700) loss: 1.693590\n",
      "(Iteration 934 / 14700) loss: 1.334192\n",
      "(Iteration 935 / 14700) loss: 1.431393\n",
      "(Iteration 936 / 14700) loss: 1.569625\n",
      "(Iteration 937 / 14700) loss: 1.591030\n",
      "(Iteration 938 / 14700) loss: 1.294064\n",
      "(Iteration 939 / 14700) loss: 1.587822\n",
      "(Iteration 940 / 14700) loss: 1.276739\n",
      "(Iteration 941 / 14700) loss: 1.411766\n",
      "(Iteration 942 / 14700) loss: 1.157056\n",
      "(Iteration 943 / 14700) loss: 1.519559\n",
      "(Iteration 944 / 14700) loss: 1.523629\n",
      "(Iteration 945 / 14700) loss: 1.140317\n",
      "(Iteration 946 / 14700) loss: 1.781171\n",
      "(Iteration 947 / 14700) loss: 1.590937\n",
      "(Iteration 948 / 14700) loss: 1.084558\n",
      "(Iteration 949 / 14700) loss: 1.592934\n",
      "(Iteration 950 / 14700) loss: 1.544616\n",
      "(Iteration 951 / 14700) loss: 1.365159\n",
      "(Iteration 952 / 14700) loss: 1.472743\n",
      "(Iteration 953 / 14700) loss: 1.500625\n",
      "(Iteration 954 / 14700) loss: 1.445852\n",
      "(Iteration 955 / 14700) loss: 1.755624\n",
      "(Iteration 956 / 14700) loss: 1.497777\n",
      "(Iteration 957 / 14700) loss: 1.439185\n",
      "(Iteration 958 / 14700) loss: 1.560107\n",
      "(Iteration 959 / 14700) loss: 1.314489\n",
      "(Iteration 960 / 14700) loss: 1.482956\n",
      "(Iteration 961 / 14700) loss: 1.620731\n",
      "(Iteration 962 / 14700) loss: 1.491743\n",
      "(Iteration 963 / 14700) loss: 1.675507\n",
      "(Iteration 964 / 14700) loss: 1.423950\n",
      "(Iteration 965 / 14700) loss: 1.486855\n",
      "(Iteration 966 / 14700) loss: 1.493144\n",
      "(Iteration 967 / 14700) loss: 1.326809\n",
      "(Iteration 968 / 14700) loss: 1.437475\n",
      "(Iteration 969 / 14700) loss: 1.276363\n",
      "(Iteration 970 / 14700) loss: 1.446948\n",
      "(Iteration 971 / 14700) loss: 1.592272\n",
      "(Iteration 972 / 14700) loss: 1.512140\n",
      "(Iteration 973 / 14700) loss: 1.233073\n",
      "(Iteration 974 / 14700) loss: 1.431971\n",
      "(Iteration 975 / 14700) loss: 1.530985\n",
      "(Iteration 976 / 14700) loss: 1.506234\n",
      "(Iteration 977 / 14700) loss: 1.484331\n",
      "(Iteration 978 / 14700) loss: 1.263644\n",
      "(Iteration 979 / 14700) loss: 1.426161\n",
      "(Iteration 980 / 14700) loss: 1.543762\n",
      "(Epoch 1 / 15) train acc: 0.616000; val_acc: 0.595000\n",
      "(Iteration 981 / 14700) loss: 1.232309\n",
      "(Iteration 982 / 14700) loss: 1.330059\n",
      "(Iteration 983 / 14700) loss: 1.768823\n",
      "(Iteration 984 / 14700) loss: 1.457319\n",
      "(Iteration 985 / 14700) loss: 1.392172\n",
      "(Iteration 986 / 14700) loss: 1.343621\n",
      "(Iteration 987 / 14700) loss: 1.409749\n",
      "(Iteration 988 / 14700) loss: 1.522182\n",
      "(Iteration 989 / 14700) loss: 1.471823\n",
      "(Iteration 990 / 14700) loss: 1.265432\n",
      "(Iteration 991 / 14700) loss: 1.459492\n",
      "(Iteration 992 / 14700) loss: 1.456968\n",
      "(Iteration 993 / 14700) loss: 1.479862\n",
      "(Iteration 994 / 14700) loss: 1.360719\n",
      "(Iteration 995 / 14700) loss: 1.242631\n",
      "(Iteration 996 / 14700) loss: 1.473942\n",
      "(Iteration 997 / 14700) loss: 1.453689\n",
      "(Iteration 998 / 14700) loss: 1.438915\n",
      "(Iteration 999 / 14700) loss: 1.550019\n",
      "(Iteration 1000 / 14700) loss: 1.183708\n",
      "(Iteration 1001 / 14700) loss: 1.355588\n",
      "(Iteration 1002 / 14700) loss: 1.409585\n",
      "(Iteration 1003 / 14700) loss: 1.425396\n",
      "(Iteration 1004 / 14700) loss: 1.415470\n",
      "(Iteration 1005 / 14700) loss: 1.451035\n",
      "(Iteration 1006 / 14700) loss: 1.410477\n",
      "(Iteration 1007 / 14700) loss: 1.207207\n",
      "(Iteration 1008 / 14700) loss: 1.416178\n",
      "(Iteration 1009 / 14700) loss: 1.559489\n",
      "(Iteration 1010 / 14700) loss: 1.596976\n",
      "(Iteration 1011 / 14700) loss: 1.328425\n",
      "(Iteration 1012 / 14700) loss: 1.217393\n",
      "(Iteration 1013 / 14700) loss: 1.378262\n",
      "(Iteration 1014 / 14700) loss: 1.486232\n",
      "(Iteration 1015 / 14700) loss: 1.752111\n",
      "(Iteration 1016 / 14700) loss: 1.461409\n",
      "(Iteration 1017 / 14700) loss: 1.371059\n",
      "(Iteration 1018 / 14700) loss: 1.539174\n",
      "(Iteration 1019 / 14700) loss: 1.630713\n",
      "(Iteration 1020 / 14700) loss: 1.291614\n",
      "(Iteration 1021 / 14700) loss: 1.555287\n",
      "(Iteration 1022 / 14700) loss: 1.563668\n",
      "(Iteration 1023 / 14700) loss: 1.538362\n",
      "(Iteration 1024 / 14700) loss: 1.319234\n",
      "(Iteration 1025 / 14700) loss: 1.285653\n",
      "(Iteration 1026 / 14700) loss: 1.206754\n",
      "(Iteration 1027 / 14700) loss: 1.314572\n",
      "(Iteration 1028 / 14700) loss: 1.506088\n",
      "(Iteration 1029 / 14700) loss: 1.420162\n",
      "(Iteration 1030 / 14700) loss: 1.525471\n",
      "(Iteration 1031 / 14700) loss: 1.399809\n",
      "(Iteration 1032 / 14700) loss: 1.453774\n",
      "(Iteration 1033 / 14700) loss: 1.461660\n",
      "(Iteration 1034 / 14700) loss: 1.459284\n",
      "(Iteration 1035 / 14700) loss: 1.286665\n",
      "(Iteration 1036 / 14700) loss: 1.438583\n",
      "(Iteration 1037 / 14700) loss: 1.505751\n",
      "(Iteration 1038 / 14700) loss: 1.535499\n",
      "(Iteration 1039 / 14700) loss: 1.646143\n",
      "(Iteration 1040 / 14700) loss: 1.224395\n",
      "(Iteration 1041 / 14700) loss: 1.240312\n",
      "(Iteration 1042 / 14700) loss: 1.448116\n",
      "(Iteration 1043 / 14700) loss: 1.489348\n",
      "(Iteration 1044 / 14700) loss: 1.486783\n",
      "(Iteration 1045 / 14700) loss: 1.391588\n",
      "(Iteration 1046 / 14700) loss: 1.644866\n",
      "(Iteration 1047 / 14700) loss: 1.435059\n",
      "(Iteration 1048 / 14700) loss: 1.275147\n",
      "(Iteration 1049 / 14700) loss: 1.306220\n",
      "(Iteration 1050 / 14700) loss: 1.295725\n",
      "(Iteration 1051 / 14700) loss: 1.428600\n",
      "(Iteration 1052 / 14700) loss: 1.267066\n",
      "(Iteration 1053 / 14700) loss: 1.332204\n",
      "(Iteration 1054 / 14700) loss: 1.393963\n",
      "(Iteration 1055 / 14700) loss: 1.569938\n",
      "(Iteration 1056 / 14700) loss: 1.421151\n",
      "(Iteration 1057 / 14700) loss: 1.201977\n",
      "(Iteration 1058 / 14700) loss: 1.617443\n",
      "(Iteration 1059 / 14700) loss: 1.416804\n",
      "(Iteration 1060 / 14700) loss: 1.416893\n",
      "(Iteration 1061 / 14700) loss: 1.421195\n",
      "(Iteration 1062 / 14700) loss: 1.826750\n",
      "(Iteration 1063 / 14700) loss: 1.175604\n",
      "(Iteration 1064 / 14700) loss: 1.426175\n",
      "(Iteration 1065 / 14700) loss: 1.542802\n",
      "(Iteration 1066 / 14700) loss: 1.793607\n",
      "(Iteration 1067 / 14700) loss: 1.678803\n",
      "(Iteration 1068 / 14700) loss: 1.361982\n",
      "(Iteration 1069 / 14700) loss: 1.450290\n",
      "(Iteration 1070 / 14700) loss: 1.688072\n",
      "(Iteration 1071 / 14700) loss: 1.172172\n",
      "(Iteration 1072 / 14700) loss: 1.323442\n",
      "(Iteration 1073 / 14700) loss: 1.226426\n",
      "(Iteration 1074 / 14700) loss: 1.250296\n",
      "(Iteration 1075 / 14700) loss: 1.362151\n",
      "(Iteration 1076 / 14700) loss: 1.359845\n",
      "(Iteration 1077 / 14700) loss: 1.279378\n",
      "(Iteration 1078 / 14700) loss: 1.513675\n",
      "(Iteration 1079 / 14700) loss: 1.710778\n",
      "(Iteration 1080 / 14700) loss: 1.184762\n",
      "(Iteration 1081 / 14700) loss: 1.462116\n",
      "(Iteration 1082 / 14700) loss: 1.279953\n",
      "(Iteration 1083 / 14700) loss: 1.065955\n",
      "(Iteration 1084 / 14700) loss: 1.272180\n",
      "(Iteration 1085 / 14700) loss: 1.697298\n",
      "(Iteration 1086 / 14700) loss: 1.364771\n",
      "(Iteration 1087 / 14700) loss: 1.544346\n",
      "(Iteration 1088 / 14700) loss: 1.507824\n",
      "(Iteration 1089 / 14700) loss: 1.324923\n",
      "(Iteration 1090 / 14700) loss: 1.500014\n",
      "(Iteration 1091 / 14700) loss: 1.413412\n",
      "(Iteration 1092 / 14700) loss: 1.613718\n",
      "(Iteration 1093 / 14700) loss: 1.253500\n",
      "(Iteration 1094 / 14700) loss: 1.589246\n",
      "(Iteration 1095 / 14700) loss: 1.492493\n",
      "(Iteration 1096 / 14700) loss: 1.538919\n",
      "(Iteration 1097 / 14700) loss: 1.230132\n",
      "(Iteration 1098 / 14700) loss: 1.291449\n",
      "(Iteration 1099 / 14700) loss: 1.315481\n",
      "(Iteration 1100 / 14700) loss: 1.396616\n",
      "(Iteration 1101 / 14700) loss: 1.354423\n",
      "(Iteration 1102 / 14700) loss: 1.647868\n",
      "(Iteration 1103 / 14700) loss: 1.090849\n",
      "(Iteration 1104 / 14700) loss: 1.197692\n",
      "(Iteration 1105 / 14700) loss: 1.448895\n",
      "(Iteration 1106 / 14700) loss: 1.424125\n",
      "(Iteration 1107 / 14700) loss: 1.471052\n",
      "(Iteration 1108 / 14700) loss: 1.490061\n",
      "(Iteration 1109 / 14700) loss: 1.318771\n",
      "(Iteration 1110 / 14700) loss: 1.269101\n",
      "(Iteration 1111 / 14700) loss: 1.285511\n",
      "(Iteration 1112 / 14700) loss: 1.651596\n",
      "(Iteration 1113 / 14700) loss: 1.030799\n",
      "(Iteration 1114 / 14700) loss: 1.249604\n",
      "(Iteration 1115 / 14700) loss: 1.587134\n",
      "(Iteration 1116 / 14700) loss: 1.638582\n",
      "(Iteration 1117 / 14700) loss: 1.299110\n",
      "(Iteration 1118 / 14700) loss: 1.321567\n",
      "(Iteration 1119 / 14700) loss: 1.125599\n",
      "(Iteration 1120 / 14700) loss: 1.159835\n",
      "(Iteration 1121 / 14700) loss: 1.214813\n",
      "(Iteration 1122 / 14700) loss: 1.231810\n",
      "(Iteration 1123 / 14700) loss: 1.434778\n",
      "(Iteration 1124 / 14700) loss: 1.413085\n",
      "(Iteration 1125 / 14700) loss: 1.112601\n",
      "(Iteration 1126 / 14700) loss: 1.319654\n",
      "(Iteration 1127 / 14700) loss: 1.471493\n",
      "(Iteration 1128 / 14700) loss: 1.260102\n",
      "(Iteration 1129 / 14700) loss: 1.358018\n",
      "(Iteration 1130 / 14700) loss: 1.254833\n",
      "(Iteration 1131 / 14700) loss: 1.135760\n",
      "(Iteration 1132 / 14700) loss: 1.256587\n",
      "(Iteration 1133 / 14700) loss: 1.464810\n",
      "(Iteration 1134 / 14700) loss: 1.787365\n",
      "(Iteration 1135 / 14700) loss: 1.469730\n",
      "(Iteration 1136 / 14700) loss: 1.201533\n",
      "(Iteration 1137 / 14700) loss: 1.492442\n",
      "(Iteration 1138 / 14700) loss: 1.477184\n",
      "(Iteration 1139 / 14700) loss: 1.383428\n",
      "(Iteration 1140 / 14700) loss: 1.191608\n",
      "(Iteration 1141 / 14700) loss: 1.418143\n",
      "(Iteration 1142 / 14700) loss: 1.460798\n",
      "(Iteration 1143 / 14700) loss: 1.432222\n",
      "(Iteration 1144 / 14700) loss: 1.285124\n",
      "(Iteration 1145 / 14700) loss: 1.355610\n",
      "(Iteration 1146 / 14700) loss: 1.236061\n",
      "(Iteration 1147 / 14700) loss: 1.467573\n",
      "(Iteration 1148 / 14700) loss: 1.170155\n",
      "(Iteration 1149 / 14700) loss: 1.216648\n",
      "(Iteration 1150 / 14700) loss: 1.223854\n",
      "(Iteration 1151 / 14700) loss: 1.454017\n",
      "(Iteration 1152 / 14700) loss: 1.322786\n",
      "(Iteration 1153 / 14700) loss: 1.275129\n",
      "(Iteration 1154 / 14700) loss: 1.261183\n",
      "(Iteration 1155 / 14700) loss: 1.253970\n",
      "(Iteration 1156 / 14700) loss: 1.496508\n",
      "(Iteration 1157 / 14700) loss: 1.144169\n",
      "(Iteration 1158 / 14700) loss: 1.224179\n",
      "(Iteration 1159 / 14700) loss: 0.982262\n",
      "(Iteration 1160 / 14700) loss: 1.221232\n",
      "(Iteration 1161 / 14700) loss: 1.367368\n",
      "(Iteration 1162 / 14700) loss: 1.383496\n",
      "(Iteration 1163 / 14700) loss: 1.402126\n",
      "(Iteration 1164 / 14700) loss: 1.411589\n",
      "(Iteration 1165 / 14700) loss: 1.241241\n",
      "(Iteration 1166 / 14700) loss: 1.302201\n",
      "(Iteration 1167 / 14700) loss: 1.250600\n",
      "(Iteration 1168 / 14700) loss: 1.461941\n",
      "(Iteration 1169 / 14700) loss: 1.196171\n",
      "(Iteration 1170 / 14700) loss: 1.402226\n",
      "(Iteration 1171 / 14700) loss: 1.284853\n",
      "(Iteration 1172 / 14700) loss: 1.148626\n",
      "(Iteration 1173 / 14700) loss: 1.378718\n",
      "(Iteration 1174 / 14700) loss: 1.266523\n",
      "(Iteration 1175 / 14700) loss: 1.336587\n",
      "(Iteration 1176 / 14700) loss: 1.567130\n",
      "(Iteration 1177 / 14700) loss: 1.404468\n",
      "(Iteration 1178 / 14700) loss: 1.346114\n",
      "(Iteration 1179 / 14700) loss: 1.088258\n",
      "(Iteration 1180 / 14700) loss: 1.141397\n",
      "(Iteration 1181 / 14700) loss: 1.608625\n",
      "(Iteration 1182 / 14700) loss: 1.588730\n",
      "(Iteration 1183 / 14700) loss: 1.457458\n",
      "(Iteration 1184 / 14700) loss: 1.415023\n",
      "(Iteration 1185 / 14700) loss: 1.021738\n",
      "(Iteration 1186 / 14700) loss: 1.139982\n",
      "(Iteration 1187 / 14700) loss: 1.250729\n",
      "(Iteration 1188 / 14700) loss: 1.440453\n",
      "(Iteration 1189 / 14700) loss: 1.397456\n",
      "(Iteration 1190 / 14700) loss: 1.022221\n",
      "(Iteration 1191 / 14700) loss: 1.153361\n",
      "(Iteration 1192 / 14700) loss: 1.413734\n",
      "(Iteration 1193 / 14700) loss: 1.612893\n",
      "(Iteration 1194 / 14700) loss: 1.188178\n",
      "(Iteration 1195 / 14700) loss: 1.265349\n",
      "(Iteration 1196 / 14700) loss: 1.182548\n",
      "(Iteration 1197 / 14700) loss: 1.261979\n",
      "(Iteration 1198 / 14700) loss: 1.276262\n",
      "(Iteration 1199 / 14700) loss: 0.966670\n",
      "(Iteration 1200 / 14700) loss: 1.204014\n",
      "(Iteration 1201 / 14700) loss: 1.247477\n",
      "(Iteration 1202 / 14700) loss: 1.348670\n",
      "(Iteration 1203 / 14700) loss: 0.975644\n",
      "(Iteration 1204 / 14700) loss: 1.176622\n",
      "(Iteration 1205 / 14700) loss: 1.276489\n",
      "(Iteration 1206 / 14700) loss: 1.349809\n",
      "(Iteration 1207 / 14700) loss: 1.346531\n",
      "(Iteration 1208 / 14700) loss: 1.324966\n",
      "(Iteration 1209 / 14700) loss: 1.407598\n",
      "(Iteration 1210 / 14700) loss: 1.316479\n",
      "(Iteration 1211 / 14700) loss: 1.040435\n",
      "(Iteration 1212 / 14700) loss: 1.109515\n",
      "(Iteration 1213 / 14700) loss: 1.287249\n",
      "(Iteration 1214 / 14700) loss: 1.272114\n",
      "(Iteration 1215 / 14700) loss: 1.198304\n",
      "(Iteration 1216 / 14700) loss: 1.246407\n",
      "(Iteration 1217 / 14700) loss: 1.409852\n",
      "(Iteration 1218 / 14700) loss: 1.241007\n",
      "(Iteration 1219 / 14700) loss: 1.419693\n",
      "(Iteration 1220 / 14700) loss: 1.217210\n",
      "(Iteration 1221 / 14700) loss: 1.232866\n",
      "(Iteration 1222 / 14700) loss: 1.274800\n",
      "(Iteration 1223 / 14700) loss: 1.510335\n",
      "(Iteration 1224 / 14700) loss: 1.317363\n",
      "(Iteration 1225 / 14700) loss: 1.167486\n",
      "(Iteration 1226 / 14700) loss: 1.212243\n",
      "(Iteration 1227 / 14700) loss: 1.404432\n",
      "(Iteration 1228 / 14700) loss: 1.146076\n",
      "(Iteration 1229 / 14700) loss: 1.633725\n",
      "(Iteration 1230 / 14700) loss: 1.433280\n",
      "(Iteration 1231 / 14700) loss: 1.332448\n",
      "(Iteration 1232 / 14700) loss: 1.189600\n",
      "(Iteration 1233 / 14700) loss: 1.388774\n",
      "(Iteration 1234 / 14700) loss: 1.324927\n",
      "(Iteration 1235 / 14700) loss: 1.117908\n",
      "(Iteration 1236 / 14700) loss: 1.402678\n",
      "(Iteration 1237 / 14700) loss: 1.455201\n",
      "(Iteration 1238 / 14700) loss: 1.591204\n",
      "(Iteration 1239 / 14700) loss: 1.319572\n",
      "(Iteration 1240 / 14700) loss: 1.130494\n",
      "(Iteration 1241 / 14700) loss: 1.568765\n",
      "(Iteration 1242 / 14700) loss: 1.227894\n",
      "(Iteration 1243 / 14700) loss: 1.317718\n",
      "(Iteration 1244 / 14700) loss: 1.176533\n",
      "(Iteration 1245 / 14700) loss: 1.002068\n",
      "(Iteration 1246 / 14700) loss: 1.370838\n",
      "(Iteration 1247 / 14700) loss: 1.702214\n",
      "(Iteration 1248 / 14700) loss: 1.391170\n",
      "(Iteration 1249 / 14700) loss: 1.448989\n",
      "(Iteration 1250 / 14700) loss: 1.534543\n",
      "(Iteration 1251 / 14700) loss: 1.178352\n",
      "(Iteration 1252 / 14700) loss: 1.421027\n",
      "(Iteration 1253 / 14700) loss: 1.239076\n",
      "(Iteration 1254 / 14700) loss: 1.212654\n",
      "(Iteration 1255 / 14700) loss: 1.438978\n",
      "(Iteration 1256 / 14700) loss: 1.430586\n",
      "(Iteration 1257 / 14700) loss: 1.143111\n",
      "(Iteration 1258 / 14700) loss: 1.194466\n",
      "(Iteration 1259 / 14700) loss: 1.164268\n",
      "(Iteration 1260 / 14700) loss: 1.170443\n",
      "(Iteration 1261 / 14700) loss: 1.495257\n",
      "(Iteration 1262 / 14700) loss: 1.221136\n",
      "(Iteration 1263 / 14700) loss: 1.178719\n",
      "(Iteration 1264 / 14700) loss: 1.196986\n",
      "(Iteration 1265 / 14700) loss: 1.099418\n",
      "(Iteration 1266 / 14700) loss: 1.192198\n",
      "(Iteration 1267 / 14700) loss: 1.359042\n",
      "(Iteration 1268 / 14700) loss: 1.316393\n",
      "(Iteration 1269 / 14700) loss: 1.304964\n",
      "(Iteration 1270 / 14700) loss: 0.956213\n",
      "(Iteration 1271 / 14700) loss: 1.137075\n",
      "(Iteration 1272 / 14700) loss: 1.140989\n",
      "(Iteration 1273 / 14700) loss: 1.154579\n",
      "(Iteration 1274 / 14700) loss: 1.172322\n",
      "(Iteration 1275 / 14700) loss: 1.571345\n",
      "(Iteration 1276 / 14700) loss: 1.200751\n",
      "(Iteration 1277 / 14700) loss: 1.539938\n",
      "(Iteration 1278 / 14700) loss: 1.181223\n",
      "(Iteration 1279 / 14700) loss: 1.066612\n",
      "(Iteration 1280 / 14700) loss: 1.314644\n",
      "(Iteration 1281 / 14700) loss: 1.312726\n",
      "(Iteration 1282 / 14700) loss: 1.159568\n",
      "(Iteration 1283 / 14700) loss: 1.070698\n",
      "(Iteration 1284 / 14700) loss: 1.517607\n",
      "(Iteration 1285 / 14700) loss: 1.131168\n",
      "(Iteration 1286 / 14700) loss: 1.251948\n",
      "(Iteration 1287 / 14700) loss: 1.250583\n",
      "(Iteration 1288 / 14700) loss: 1.309066\n",
      "(Iteration 1289 / 14700) loss: 1.516852\n",
      "(Iteration 1290 / 14700) loss: 1.067740\n",
      "(Iteration 1291 / 14700) loss: 1.411917\n",
      "(Iteration 1292 / 14700) loss: 1.201066\n",
      "(Iteration 1293 / 14700) loss: 1.128738\n",
      "(Iteration 1294 / 14700) loss: 1.216900\n",
      "(Iteration 1295 / 14700) loss: 1.546110\n",
      "(Iteration 1296 / 14700) loss: 1.377810\n",
      "(Iteration 1297 / 14700) loss: 1.246781\n",
      "(Iteration 1298 / 14700) loss: 1.193640\n",
      "(Iteration 1299 / 14700) loss: 1.571812\n",
      "(Iteration 1300 / 14700) loss: 1.305054\n",
      "(Iteration 1301 / 14700) loss: 1.288651\n",
      "(Iteration 1302 / 14700) loss: 1.305211\n",
      "(Iteration 1303 / 14700) loss: 1.310006\n",
      "(Iteration 1304 / 14700) loss: 1.228325\n",
      "(Iteration 1305 / 14700) loss: 1.412056\n",
      "(Iteration 1306 / 14700) loss: 1.302989\n",
      "(Iteration 1307 / 14700) loss: 1.161297\n",
      "(Iteration 1308 / 14700) loss: 0.945675\n",
      "(Iteration 1309 / 14700) loss: 1.486922\n",
      "(Iteration 1310 / 14700) loss: 1.052891\n",
      "(Iteration 1311 / 14700) loss: 1.356675\n",
      "(Iteration 1312 / 14700) loss: 1.292054\n",
      "(Iteration 1313 / 14700) loss: 1.254307\n",
      "(Iteration 1314 / 14700) loss: 1.293587\n",
      "(Iteration 1315 / 14700) loss: 1.050241\n",
      "(Iteration 1316 / 14700) loss: 1.291870\n",
      "(Iteration 1317 / 14700) loss: 1.246220\n",
      "(Iteration 1318 / 14700) loss: 1.405921\n",
      "(Iteration 1319 / 14700) loss: 1.495449\n",
      "(Iteration 1320 / 14700) loss: 1.274498\n",
      "(Iteration 1321 / 14700) loss: 1.019720\n",
      "(Iteration 1322 / 14700) loss: 1.178964\n",
      "(Iteration 1323 / 14700) loss: 1.216912\n",
      "(Iteration 1324 / 14700) loss: 1.290967\n",
      "(Iteration 1325 / 14700) loss: 1.441710\n",
      "(Iteration 1326 / 14700) loss: 1.216697\n",
      "(Iteration 1327 / 14700) loss: 1.072953\n",
      "(Iteration 1328 / 14700) loss: 1.166656\n",
      "(Iteration 1329 / 14700) loss: 1.145959\n",
      "(Iteration 1330 / 14700) loss: 1.149248\n",
      "(Iteration 1331 / 14700) loss: 1.123551\n",
      "(Iteration 1332 / 14700) loss: 1.340248\n",
      "(Iteration 1333 / 14700) loss: 1.189599\n",
      "(Iteration 1334 / 14700) loss: 1.441577\n",
      "(Iteration 1335 / 14700) loss: 1.279028\n",
      "(Iteration 1336 / 14700) loss: 1.242828\n",
      "(Iteration 1337 / 14700) loss: 1.444228\n",
      "(Iteration 1338 / 14700) loss: 1.204808\n",
      "(Iteration 1339 / 14700) loss: 1.376717\n",
      "(Iteration 1340 / 14700) loss: 1.153932\n",
      "(Iteration 1341 / 14700) loss: 1.623444\n",
      "(Iteration 1342 / 14700) loss: 1.103534\n",
      "(Iteration 1343 / 14700) loss: 1.283575\n",
      "(Iteration 1344 / 14700) loss: 1.402375\n",
      "(Iteration 1345 / 14700) loss: 1.451137\n",
      "(Iteration 1346 / 14700) loss: 1.346717\n",
      "(Iteration 1347 / 14700) loss: 1.436262\n",
      "(Iteration 1348 / 14700) loss: 1.197203\n",
      "(Iteration 1349 / 14700) loss: 1.323399\n",
      "(Iteration 1350 / 14700) loss: 1.260624\n",
      "(Iteration 1351 / 14700) loss: 1.364814\n",
      "(Iteration 1352 / 14700) loss: 1.061837\n",
      "(Iteration 1353 / 14700) loss: 1.714215\n",
      "(Iteration 1354 / 14700) loss: 1.400752\n",
      "(Iteration 1355 / 14700) loss: 1.120609\n",
      "(Iteration 1356 / 14700) loss: 1.109068\n",
      "(Iteration 1357 / 14700) loss: 1.365319\n",
      "(Iteration 1358 / 14700) loss: 1.234924\n",
      "(Iteration 1359 / 14700) loss: 1.319889\n",
      "(Iteration 1360 / 14700) loss: 1.362264\n",
      "(Iteration 1361 / 14700) loss: 1.401121\n",
      "(Iteration 1362 / 14700) loss: 1.514212\n",
      "(Iteration 1363 / 14700) loss: 1.273314\n",
      "(Iteration 1364 / 14700) loss: 1.104450\n",
      "(Iteration 1365 / 14700) loss: 1.153963\n",
      "(Iteration 1366 / 14700) loss: 1.011025\n",
      "(Iteration 1367 / 14700) loss: 1.139504\n",
      "(Iteration 1368 / 14700) loss: 1.546713\n",
      "(Iteration 1369 / 14700) loss: 1.199659\n",
      "(Iteration 1370 / 14700) loss: 1.291181\n",
      "(Iteration 1371 / 14700) loss: 1.488484\n",
      "(Iteration 1372 / 14700) loss: 1.219325\n",
      "(Iteration 1373 / 14700) loss: 1.295608\n",
      "(Iteration 1374 / 14700) loss: 1.066192\n",
      "(Iteration 1375 / 14700) loss: 1.383887\n",
      "(Iteration 1376 / 14700) loss: 1.350485\n",
      "(Iteration 1377 / 14700) loss: 1.364552\n",
      "(Iteration 1378 / 14700) loss: 1.198636\n",
      "(Iteration 1379 / 14700) loss: 1.412486\n",
      "(Iteration 1380 / 14700) loss: 1.590809\n",
      "(Iteration 1381 / 14700) loss: 1.156287\n",
      "(Iteration 1382 / 14700) loss: 1.076178\n",
      "(Iteration 1383 / 14700) loss: 1.327606\n",
      "(Iteration 1384 / 14700) loss: 1.071163\n",
      "(Iteration 1385 / 14700) loss: 0.957853\n",
      "(Iteration 1386 / 14700) loss: 1.324086\n",
      "(Iteration 1387 / 14700) loss: 1.200553\n",
      "(Iteration 1388 / 14700) loss: 1.176277\n",
      "(Iteration 1389 / 14700) loss: 1.203300\n",
      "(Iteration 1390 / 14700) loss: 1.057601\n",
      "(Iteration 1391 / 14700) loss: 1.163325\n",
      "(Iteration 1392 / 14700) loss: 1.174745\n",
      "(Iteration 1393 / 14700) loss: 1.155897\n",
      "(Iteration 1394 / 14700) loss: 1.316822\n",
      "(Iteration 1395 / 14700) loss: 1.048114\n",
      "(Iteration 1396 / 14700) loss: 1.047816\n",
      "(Iteration 1397 / 14700) loss: 1.171380\n",
      "(Iteration 1398 / 14700) loss: 1.241648\n",
      "(Iteration 1399 / 14700) loss: 1.253162\n",
      "(Iteration 1400 / 14700) loss: 1.379634\n",
      "(Iteration 1401 / 14700) loss: 1.100767\n",
      "(Iteration 1402 / 14700) loss: 1.116653\n",
      "(Iteration 1403 / 14700) loss: 1.422019\n",
      "(Iteration 1404 / 14700) loss: 1.276141\n",
      "(Iteration 1405 / 14700) loss: 1.446764\n",
      "(Iteration 1406 / 14700) loss: 1.342998\n",
      "(Iteration 1407 / 14700) loss: 1.278824\n",
      "(Iteration 1408 / 14700) loss: 1.226438\n",
      "(Iteration 1409 / 14700) loss: 1.265777\n",
      "(Iteration 1410 / 14700) loss: 1.339880\n",
      "(Iteration 1411 / 14700) loss: 1.120398\n",
      "(Iteration 1412 / 14700) loss: 1.154197\n",
      "(Iteration 1413 / 14700) loss: 1.010586\n",
      "(Iteration 1414 / 14700) loss: 1.357222\n",
      "(Iteration 1415 / 14700) loss: 1.077172\n",
      "(Iteration 1416 / 14700) loss: 1.488173\n",
      "(Iteration 1417 / 14700) loss: 0.998896\n",
      "(Iteration 1418 / 14700) loss: 1.279113\n",
      "(Iteration 1419 / 14700) loss: 1.149919\n",
      "(Iteration 1420 / 14700) loss: 1.436419\n",
      "(Iteration 1421 / 14700) loss: 1.210746\n",
      "(Iteration 1422 / 14700) loss: 1.255581\n",
      "(Iteration 1423 / 14700) loss: 1.613611\n",
      "(Iteration 1424 / 14700) loss: 1.339660\n",
      "(Iteration 1425 / 14700) loss: 1.466243\n",
      "(Iteration 1426 / 14700) loss: 1.234075\n",
      "(Iteration 1427 / 14700) loss: 1.263386\n",
      "(Iteration 1428 / 14700) loss: 1.033146\n",
      "(Iteration 1429 / 14700) loss: 1.330773\n",
      "(Iteration 1430 / 14700) loss: 1.090241\n",
      "(Iteration 1431 / 14700) loss: 1.410719\n",
      "(Iteration 1432 / 14700) loss: 1.288998\n",
      "(Iteration 1433 / 14700) loss: 1.360540\n",
      "(Iteration 1434 / 14700) loss: 1.173204\n",
      "(Iteration 1435 / 14700) loss: 1.484818\n",
      "(Iteration 1436 / 14700) loss: 1.130839\n",
      "(Iteration 1437 / 14700) loss: 1.291132\n",
      "(Iteration 1438 / 14700) loss: 1.131404\n",
      "(Iteration 1439 / 14700) loss: 1.206206\n",
      "(Iteration 1440 / 14700) loss: 1.304071\n",
      "(Iteration 1441 / 14700) loss: 1.127121\n",
      "(Iteration 1442 / 14700) loss: 1.092671\n",
      "(Iteration 1443 / 14700) loss: 1.241611\n",
      "(Iteration 1444 / 14700) loss: 1.107539\n",
      "(Iteration 1445 / 14700) loss: 1.105562\n",
      "(Iteration 1446 / 14700) loss: 1.156820\n",
      "(Iteration 1447 / 14700) loss: 1.393322\n",
      "(Iteration 1448 / 14700) loss: 1.438150\n",
      "(Iteration 1449 / 14700) loss: 0.883243\n",
      "(Iteration 1450 / 14700) loss: 1.240059\n",
      "(Iteration 1451 / 14700) loss: 1.237708\n",
      "(Iteration 1452 / 14700) loss: 1.070116\n",
      "(Iteration 1453 / 14700) loss: 1.292832\n",
      "(Iteration 1454 / 14700) loss: 1.279117\n",
      "(Iteration 1455 / 14700) loss: 1.209530\n",
      "(Iteration 1456 / 14700) loss: 1.184011\n",
      "(Iteration 1457 / 14700) loss: 1.137889\n",
      "(Iteration 1458 / 14700) loss: 1.302785\n",
      "(Iteration 1459 / 14700) loss: 1.299942\n",
      "(Iteration 1460 / 14700) loss: 1.138002\n",
      "(Iteration 1461 / 14700) loss: 1.457222\n",
      "(Iteration 1462 / 14700) loss: 1.037306\n",
      "(Iteration 1463 / 14700) loss: 0.775872\n",
      "(Iteration 1464 / 14700) loss: 1.146072\n",
      "(Iteration 1465 / 14700) loss: 1.051631\n",
      "(Iteration 1466 / 14700) loss: 1.303326\n",
      "(Iteration 1467 / 14700) loss: 1.305891\n",
      "(Iteration 1468 / 14700) loss: 1.191757\n",
      "(Iteration 1469 / 14700) loss: 1.019793\n",
      "(Iteration 1470 / 14700) loss: 1.035169\n",
      "(Iteration 1471 / 14700) loss: 1.332300\n",
      "(Iteration 1472 / 14700) loss: 1.318747\n",
      "(Iteration 1473 / 14700) loss: 1.016889\n",
      "(Iteration 1474 / 14700) loss: 1.118324\n",
      "(Iteration 1475 / 14700) loss: 1.402406\n",
      "(Iteration 1476 / 14700) loss: 0.853635\n",
      "(Iteration 1477 / 14700) loss: 1.381963\n",
      "(Iteration 1478 / 14700) loss: 1.153572\n",
      "(Iteration 1479 / 14700) loss: 1.027608\n",
      "(Iteration 1480 / 14700) loss: 1.296066\n",
      "(Iteration 1481 / 14700) loss: 1.147554\n",
      "(Iteration 1482 / 14700) loss: 1.134144\n",
      "(Iteration 1483 / 14700) loss: 1.359829\n",
      "(Iteration 1484 / 14700) loss: 1.311393\n",
      "(Iteration 1485 / 14700) loss: 1.135256\n",
      "(Iteration 1486 / 14700) loss: 1.504177\n",
      "(Iteration 1487 / 14700) loss: 1.208608\n",
      "(Iteration 1488 / 14700) loss: 1.011099\n",
      "(Iteration 1489 / 14700) loss: 1.208729\n",
      "(Iteration 1490 / 14700) loss: 1.240751\n",
      "(Iteration 1491 / 14700) loss: 1.222052\n",
      "(Iteration 1492 / 14700) loss: 1.161943\n",
      "(Iteration 1493 / 14700) loss: 1.303665\n",
      "(Iteration 1494 / 14700) loss: 1.057333\n",
      "(Iteration 1495 / 14700) loss: 1.024256\n",
      "(Iteration 1496 / 14700) loss: 1.353473\n",
      "(Iteration 1497 / 14700) loss: 1.537681\n",
      "(Iteration 1498 / 14700) loss: 1.210589\n",
      "(Iteration 1499 / 14700) loss: 1.176705\n",
      "(Iteration 1500 / 14700) loss: 0.943584\n",
      "(Iteration 1501 / 14700) loss: 1.042881\n",
      "(Iteration 1502 / 14700) loss: 1.247800\n",
      "(Iteration 1503 / 14700) loss: 1.184449\n",
      "(Iteration 1504 / 14700) loss: 1.052744\n",
      "(Iteration 1505 / 14700) loss: 1.276482\n",
      "(Iteration 1506 / 14700) loss: 1.217983\n",
      "(Iteration 1507 / 14700) loss: 1.299444\n",
      "(Iteration 1508 / 14700) loss: 1.280172\n",
      "(Iteration 1509 / 14700) loss: 1.295036\n",
      "(Iteration 1510 / 14700) loss: 1.160892\n",
      "(Iteration 1511 / 14700) loss: 1.291776\n",
      "(Iteration 1512 / 14700) loss: 1.120271\n",
      "(Iteration 1513 / 14700) loss: 1.058967\n",
      "(Iteration 1514 / 14700) loss: 1.031579\n",
      "(Iteration 1515 / 14700) loss: 1.142760\n",
      "(Iteration 1516 / 14700) loss: 1.120053\n",
      "(Iteration 1517 / 14700) loss: 0.903092\n",
      "(Iteration 1518 / 14700) loss: 1.570712\n",
      "(Iteration 1519 / 14700) loss: 0.896164\n",
      "(Iteration 1520 / 14700) loss: 1.145748\n",
      "(Iteration 1521 / 14700) loss: 1.475574\n",
      "(Iteration 1522 / 14700) loss: 1.268274\n",
      "(Iteration 1523 / 14700) loss: 1.412285\n",
      "(Iteration 1524 / 14700) loss: 1.362218\n",
      "(Iteration 1525 / 14700) loss: 1.432045\n",
      "(Iteration 1526 / 14700) loss: 1.105412\n",
      "(Iteration 1527 / 14700) loss: 1.090191\n",
      "(Iteration 1528 / 14700) loss: 1.340474\n",
      "(Iteration 1529 / 14700) loss: 1.119122\n",
      "(Iteration 1530 / 14700) loss: 1.251126\n",
      "(Iteration 1531 / 14700) loss: 1.202787\n",
      "(Iteration 1532 / 14700) loss: 1.286667\n",
      "(Iteration 1533 / 14700) loss: 1.218105\n",
      "(Iteration 1534 / 14700) loss: 1.195277\n",
      "(Iteration 1535 / 14700) loss: 1.237410\n",
      "(Iteration 1536 / 14700) loss: 1.291718\n",
      "(Iteration 1537 / 14700) loss: 1.494098\n",
      "(Iteration 1538 / 14700) loss: 1.242663\n",
      "(Iteration 1539 / 14700) loss: 1.132938\n",
      "(Iteration 1540 / 14700) loss: 1.170904\n",
      "(Iteration 1541 / 14700) loss: 1.434524\n",
      "(Iteration 1542 / 14700) loss: 1.194727\n",
      "(Iteration 1543 / 14700) loss: 1.279841\n",
      "(Iteration 1544 / 14700) loss: 1.086754\n",
      "(Iteration 1545 / 14700) loss: 1.179461\n",
      "(Iteration 1546 / 14700) loss: 1.098179\n",
      "(Iteration 1547 / 14700) loss: 0.911071\n",
      "(Iteration 1548 / 14700) loss: 1.162735\n",
      "(Iteration 1549 / 14700) loss: 1.090742\n",
      "(Iteration 1550 / 14700) loss: 1.095811\n",
      "(Iteration 1551 / 14700) loss: 1.197914\n",
      "(Iteration 1552 / 14700) loss: 1.179159\n",
      "(Iteration 1553 / 14700) loss: 1.141432\n",
      "(Iteration 1554 / 14700) loss: 1.122909\n",
      "(Iteration 1555 / 14700) loss: 1.199871\n",
      "(Iteration 1556 / 14700) loss: 1.308484\n",
      "(Iteration 1557 / 14700) loss: 0.962376\n",
      "(Iteration 1558 / 14700) loss: 1.471258\n",
      "(Iteration 1559 / 14700) loss: 1.109408\n",
      "(Iteration 1560 / 14700) loss: 1.112051\n",
      "(Iteration 1561 / 14700) loss: 1.229242\n",
      "(Iteration 1562 / 14700) loss: 1.155895\n",
      "(Iteration 1563 / 14700) loss: 1.319310\n",
      "(Iteration 1564 / 14700) loss: 1.342943\n",
      "(Iteration 1565 / 14700) loss: 1.038921\n",
      "(Iteration 1566 / 14700) loss: 1.214371\n",
      "(Iteration 1567 / 14700) loss: 1.096381\n",
      "(Iteration 1568 / 14700) loss: 1.123935\n",
      "(Iteration 1569 / 14700) loss: 1.336360\n",
      "(Iteration 1570 / 14700) loss: 1.224571\n",
      "(Iteration 1571 / 14700) loss: 1.344009\n",
      "(Iteration 1572 / 14700) loss: 1.382491\n",
      "(Iteration 1573 / 14700) loss: 1.247011\n",
      "(Iteration 1574 / 14700) loss: 1.158692\n",
      "(Iteration 1575 / 14700) loss: 1.247597\n",
      "(Iteration 1576 / 14700) loss: 1.324676\n",
      "(Iteration 1577 / 14700) loss: 1.045733\n",
      "(Iteration 1578 / 14700) loss: 1.135897\n",
      "(Iteration 1579 / 14700) loss: 1.002979\n",
      "(Iteration 1580 / 14700) loss: 1.324840\n",
      "(Iteration 1581 / 14700) loss: 1.345569\n",
      "(Iteration 1582 / 14700) loss: 1.192378\n",
      "(Iteration 1583 / 14700) loss: 1.140562\n",
      "(Iteration 1584 / 14700) loss: 1.117974\n",
      "(Iteration 1585 / 14700) loss: 1.148006\n",
      "(Iteration 1586 / 14700) loss: 1.206269\n",
      "(Iteration 1587 / 14700) loss: 0.971713\n",
      "(Iteration 1588 / 14700) loss: 1.131261\n",
      "(Iteration 1589 / 14700) loss: 1.163359\n",
      "(Iteration 1590 / 14700) loss: 1.092063\n",
      "(Iteration 1591 / 14700) loss: 1.132570\n",
      "(Iteration 1592 / 14700) loss: 1.479826\n",
      "(Iteration 1593 / 14700) loss: 1.268164\n",
      "(Iteration 1594 / 14700) loss: 0.914846\n",
      "(Iteration 1595 / 14700) loss: 0.999880\n",
      "(Iteration 1596 / 14700) loss: 1.194907\n",
      "(Iteration 1597 / 14700) loss: 1.088249\n",
      "(Iteration 1598 / 14700) loss: 0.978653\n",
      "(Iteration 1599 / 14700) loss: 1.484527\n",
      "(Iteration 1600 / 14700) loss: 1.066490\n",
      "(Iteration 1601 / 14700) loss: 1.311854\n",
      "(Iteration 1602 / 14700) loss: 1.131905\n",
      "(Iteration 1603 / 14700) loss: 1.112014\n",
      "(Iteration 1604 / 14700) loss: 0.999716\n",
      "(Iteration 1605 / 14700) loss: 1.096917\n",
      "(Iteration 1606 / 14700) loss: 1.122609\n",
      "(Iteration 1607 / 14700) loss: 1.120001\n",
      "(Iteration 1608 / 14700) loss: 0.941209\n",
      "(Iteration 1609 / 14700) loss: 0.923849\n",
      "(Iteration 1610 / 14700) loss: 1.331250\n",
      "(Iteration 1611 / 14700) loss: 1.169578\n",
      "(Iteration 1612 / 14700) loss: 1.411816\n",
      "(Iteration 1613 / 14700) loss: 1.188321\n",
      "(Iteration 1614 / 14700) loss: 1.132097\n",
      "(Iteration 1615 / 14700) loss: 1.071063\n",
      "(Iteration 1616 / 14700) loss: 1.306353\n",
      "(Iteration 1617 / 14700) loss: 1.141306\n",
      "(Iteration 1618 / 14700) loss: 1.432713\n",
      "(Iteration 1619 / 14700) loss: 1.059343\n",
      "(Iteration 1620 / 14700) loss: 1.122592\n",
      "(Iteration 1621 / 14700) loss: 1.372152\n",
      "(Iteration 1622 / 14700) loss: 1.224580\n",
      "(Iteration 1623 / 14700) loss: 1.102163\n",
      "(Iteration 1624 / 14700) loss: 0.974824\n",
      "(Iteration 1625 / 14700) loss: 1.025083\n",
      "(Iteration 1626 / 14700) loss: 1.419289\n",
      "(Iteration 1627 / 14700) loss: 1.015277\n",
      "(Iteration 1628 / 14700) loss: 1.328106\n",
      "(Iteration 1629 / 14700) loss: 1.264677\n",
      "(Iteration 1630 / 14700) loss: 1.306493\n",
      "(Iteration 1631 / 14700) loss: 1.033148\n",
      "(Iteration 1632 / 14700) loss: 1.265516\n",
      "(Iteration 1633 / 14700) loss: 1.271589\n",
      "(Iteration 1634 / 14700) loss: 1.293565\n",
      "(Iteration 1635 / 14700) loss: 1.177875\n",
      "(Iteration 1636 / 14700) loss: 1.122102\n",
      "(Iteration 1637 / 14700) loss: 1.133153\n",
      "(Iteration 1638 / 14700) loss: 1.123348\n",
      "(Iteration 1639 / 14700) loss: 0.923864\n",
      "(Iteration 1640 / 14700) loss: 1.123606\n",
      "(Iteration 1641 / 14700) loss: 1.043492\n",
      "(Iteration 1642 / 14700) loss: 1.036652\n",
      "(Iteration 1643 / 14700) loss: 1.330064\n",
      "(Iteration 1644 / 14700) loss: 1.395825\n",
      "(Iteration 1645 / 14700) loss: 1.093769\n",
      "(Iteration 1646 / 14700) loss: 1.141127\n",
      "(Iteration 1647 / 14700) loss: 1.327648\n",
      "(Iteration 1648 / 14700) loss: 1.488160\n",
      "(Iteration 1649 / 14700) loss: 0.917899\n",
      "(Iteration 1650 / 14700) loss: 1.661620\n",
      "(Iteration 1651 / 14700) loss: 1.215017\n",
      "(Iteration 1652 / 14700) loss: 1.275499\n",
      "(Iteration 1653 / 14700) loss: 1.135124\n",
      "(Iteration 1654 / 14700) loss: 1.210859\n",
      "(Iteration 1655 / 14700) loss: 1.103917\n",
      "(Iteration 1656 / 14700) loss: 0.948402\n",
      "(Iteration 1657 / 14700) loss: 1.383124\n",
      "(Iteration 1658 / 14700) loss: 1.214068\n",
      "(Iteration 1659 / 14700) loss: 1.062501\n",
      "(Iteration 1660 / 14700) loss: 1.243582\n",
      "(Iteration 1661 / 14700) loss: 1.201220\n",
      "(Iteration 1662 / 14700) loss: 1.241954\n",
      "(Iteration 1663 / 14700) loss: 1.104966\n",
      "(Iteration 1664 / 14700) loss: 1.267643\n",
      "(Iteration 1665 / 14700) loss: 1.132360\n",
      "(Iteration 1666 / 14700) loss: 1.093006\n",
      "(Iteration 1667 / 14700) loss: 0.997350\n",
      "(Iteration 1668 / 14700) loss: 1.084483\n",
      "(Iteration 1669 / 14700) loss: 1.172153\n",
      "(Iteration 1670 / 14700) loss: 1.243706\n",
      "(Iteration 1671 / 14700) loss: 1.053071\n",
      "(Iteration 1672 / 14700) loss: 1.207940\n",
      "(Iteration 1673 / 14700) loss: 1.276504\n",
      "(Iteration 1674 / 14700) loss: 0.914328\n",
      "(Iteration 1675 / 14700) loss: 1.248508\n",
      "(Iteration 1676 / 14700) loss: 0.953602\n",
      "(Iteration 1677 / 14700) loss: 1.203941\n",
      "(Iteration 1678 / 14700) loss: 1.103677\n",
      "(Iteration 1679 / 14700) loss: 0.890374\n",
      "(Iteration 1680 / 14700) loss: 1.074405\n",
      "(Iteration 1681 / 14700) loss: 1.062009\n",
      "(Iteration 1682 / 14700) loss: 1.372049\n",
      "(Iteration 1683 / 14700) loss: 1.066405\n",
      "(Iteration 1684 / 14700) loss: 1.106336\n",
      "(Iteration 1685 / 14700) loss: 0.925494\n",
      "(Iteration 1686 / 14700) loss: 1.054809\n",
      "(Iteration 1687 / 14700) loss: 1.269880\n",
      "(Iteration 1688 / 14700) loss: 1.170827\n",
      "(Iteration 1689 / 14700) loss: 1.252823\n",
      "(Iteration 1690 / 14700) loss: 1.190130\n",
      "(Iteration 1691 / 14700) loss: 1.205761\n",
      "(Iteration 1692 / 14700) loss: 1.483558\n",
      "(Iteration 1693 / 14700) loss: 1.434877\n",
      "(Iteration 1694 / 14700) loss: 1.245624\n",
      "(Iteration 1695 / 14700) loss: 1.196768\n",
      "(Iteration 1696 / 14700) loss: 0.944159\n",
      "(Iteration 1697 / 14700) loss: 1.176161\n",
      "(Iteration 1698 / 14700) loss: 1.308241\n",
      "(Iteration 1699 / 14700) loss: 0.960837\n",
      "(Iteration 1700 / 14700) loss: 0.896853\n",
      "(Iteration 1701 / 14700) loss: 1.104448\n",
      "(Iteration 1702 / 14700) loss: 0.929351\n",
      "(Iteration 1703 / 14700) loss: 1.319980\n",
      "(Iteration 1704 / 14700) loss: 0.864964\n",
      "(Iteration 1705 / 14700) loss: 1.155250\n",
      "(Iteration 1706 / 14700) loss: 0.962608\n",
      "(Iteration 1707 / 14700) loss: 1.041474\n",
      "(Iteration 1708 / 14700) loss: 0.945764\n",
      "(Iteration 1709 / 14700) loss: 1.002057\n",
      "(Iteration 1710 / 14700) loss: 0.892605\n",
      "(Iteration 1711 / 14700) loss: 1.315004\n",
      "(Iteration 1712 / 14700) loss: 1.086307\n",
      "(Iteration 1713 / 14700) loss: 1.184687\n",
      "(Iteration 1714 / 14700) loss: 1.254981\n",
      "(Iteration 1715 / 14700) loss: 1.525318\n",
      "(Iteration 1716 / 14700) loss: 1.142803\n",
      "(Iteration 1717 / 14700) loss: 1.091581\n",
      "(Iteration 1718 / 14700) loss: 1.266571\n",
      "(Iteration 1719 / 14700) loss: 1.323753\n",
      "(Iteration 1720 / 14700) loss: 1.171771\n",
      "(Iteration 1721 / 14700) loss: 1.183221\n",
      "(Iteration 1722 / 14700) loss: 1.010478\n",
      "(Iteration 1723 / 14700) loss: 0.980026\n",
      "(Iteration 1724 / 14700) loss: 1.453642\n",
      "(Iteration 1725 / 14700) loss: 1.100486\n",
      "(Iteration 1726 / 14700) loss: 1.098399\n",
      "(Iteration 1727 / 14700) loss: 1.276447\n",
      "(Iteration 1728 / 14700) loss: 1.337463\n",
      "(Iteration 1729 / 14700) loss: 1.185055\n",
      "(Iteration 1730 / 14700) loss: 0.919912\n",
      "(Iteration 1731 / 14700) loss: 1.182615\n",
      "(Iteration 1732 / 14700) loss: 1.291131\n",
      "(Iteration 1733 / 14700) loss: 1.189214\n",
      "(Iteration 1734 / 14700) loss: 1.155621\n",
      "(Iteration 1735 / 14700) loss: 1.019041\n",
      "(Iteration 1736 / 14700) loss: 1.232651\n",
      "(Iteration 1737 / 14700) loss: 1.053353\n",
      "(Iteration 1738 / 14700) loss: 1.084711\n",
      "(Iteration 1739 / 14700) loss: 1.122909\n",
      "(Iteration 1740 / 14700) loss: 1.335992\n",
      "(Iteration 1741 / 14700) loss: 1.068014\n",
      "(Iteration 1742 / 14700) loss: 1.183804\n",
      "(Iteration 1743 / 14700) loss: 0.914137\n",
      "(Iteration 1744 / 14700) loss: 0.963528\n",
      "(Iteration 1745 / 14700) loss: 1.475640\n",
      "(Iteration 1746 / 14700) loss: 0.886455\n",
      "(Iteration 1747 / 14700) loss: 0.957252\n",
      "(Iteration 1748 / 14700) loss: 1.028621\n",
      "(Iteration 1749 / 14700) loss: 1.147554\n",
      "(Iteration 1750 / 14700) loss: 0.816169\n",
      "(Iteration 1751 / 14700) loss: 1.279914\n",
      "(Iteration 1752 / 14700) loss: 1.255781\n",
      "(Iteration 1753 / 14700) loss: 1.302849\n",
      "(Iteration 1754 / 14700) loss: 1.240910\n",
      "(Iteration 1755 / 14700) loss: 1.224894\n",
      "(Iteration 1756 / 14700) loss: 1.147246\n",
      "(Iteration 1757 / 14700) loss: 1.264791\n",
      "(Iteration 1758 / 14700) loss: 1.140076\n",
      "(Iteration 1759 / 14700) loss: 1.332683\n",
      "(Iteration 1760 / 14700) loss: 1.065108\n",
      "(Iteration 1761 / 14700) loss: 1.372108\n",
      "(Iteration 1762 / 14700) loss: 1.087509\n",
      "(Iteration 1763 / 14700) loss: 1.205147\n",
      "(Iteration 1764 / 14700) loss: 1.110340\n",
      "(Iteration 1765 / 14700) loss: 1.092847\n",
      "(Iteration 1766 / 14700) loss: 0.928114\n",
      "(Iteration 1767 / 14700) loss: 1.279945\n",
      "(Iteration 1768 / 14700) loss: 1.083878\n",
      "(Iteration 1769 / 14700) loss: 1.220344\n",
      "(Iteration 1770 / 14700) loss: 1.123136\n",
      "(Iteration 1771 / 14700) loss: 0.936293\n",
      "(Iteration 1772 / 14700) loss: 1.062176\n",
      "(Iteration 1773 / 14700) loss: 0.991190\n",
      "(Iteration 1774 / 14700) loss: 0.851794\n",
      "(Iteration 1775 / 14700) loss: 1.071261\n",
      "(Iteration 1776 / 14700) loss: 1.095485\n",
      "(Iteration 1777 / 14700) loss: 1.140204\n",
      "(Iteration 1778 / 14700) loss: 1.045898\n",
      "(Iteration 1779 / 14700) loss: 0.970832\n",
      "(Iteration 1780 / 14700) loss: 1.171961\n",
      "(Iteration 1781 / 14700) loss: 1.011552\n",
      "(Iteration 1782 / 14700) loss: 1.039320\n",
      "(Iteration 1783 / 14700) loss: 1.048604\n",
      "(Iteration 1784 / 14700) loss: 1.325144\n",
      "(Iteration 1785 / 14700) loss: 1.047874\n",
      "(Iteration 1786 / 14700) loss: 1.167468\n",
      "(Iteration 1787 / 14700) loss: 1.128156\n",
      "(Iteration 1788 / 14700) loss: 0.956767\n",
      "(Iteration 1789 / 14700) loss: 1.113190\n",
      "(Iteration 1790 / 14700) loss: 1.197459\n",
      "(Iteration 1791 / 14700) loss: 1.024296\n",
      "(Iteration 1792 / 14700) loss: 1.395902\n",
      "(Iteration 1793 / 14700) loss: 1.168879\n",
      "(Iteration 1794 / 14700) loss: 1.117807\n",
      "(Iteration 1795 / 14700) loss: 1.161610\n",
      "(Iteration 1796 / 14700) loss: 1.435315\n",
      "(Iteration 1797 / 14700) loss: 1.206594\n",
      "(Iteration 1798 / 14700) loss: 0.884515\n",
      "(Iteration 1799 / 14700) loss: 1.002818\n",
      "(Iteration 1800 / 14700) loss: 1.140698\n",
      "(Iteration 1801 / 14700) loss: 1.458235\n",
      "(Iteration 1802 / 14700) loss: 1.060983\n",
      "(Iteration 1803 / 14700) loss: 1.187507\n",
      "(Iteration 1804 / 14700) loss: 0.917339\n",
      "(Iteration 1805 / 14700) loss: 1.085095\n",
      "(Iteration 1806 / 14700) loss: 1.032422\n",
      "(Iteration 1807 / 14700) loss: 1.325963\n",
      "(Iteration 1808 / 14700) loss: 1.108543\n",
      "(Iteration 1809 / 14700) loss: 1.456479\n",
      "(Iteration 1810 / 14700) loss: 0.946103\n",
      "(Iteration 1811 / 14700) loss: 1.212286\n",
      "(Iteration 1812 / 14700) loss: 1.024103\n",
      "(Iteration 1813 / 14700) loss: 1.259272\n",
      "(Iteration 1814 / 14700) loss: 1.125972\n",
      "(Iteration 1815 / 14700) loss: 1.164726\n",
      "(Iteration 1816 / 14700) loss: 1.280717\n",
      "(Iteration 1817 / 14700) loss: 1.189807\n",
      "(Iteration 1818 / 14700) loss: 0.897080\n",
      "(Iteration 1819 / 14700) loss: 0.952480\n",
      "(Iteration 1820 / 14700) loss: 1.118637\n",
      "(Iteration 1821 / 14700) loss: 1.080161\n",
      "(Iteration 1822 / 14700) loss: 0.974910\n",
      "(Iteration 1823 / 14700) loss: 1.148637\n",
      "(Iteration 1824 / 14700) loss: 0.964810\n",
      "(Iteration 1825 / 14700) loss: 1.031382\n",
      "(Iteration 1826 / 14700) loss: 0.893931\n",
      "(Iteration 1827 / 14700) loss: 0.994171\n",
      "(Iteration 1828 / 14700) loss: 1.464194\n",
      "(Iteration 1829 / 14700) loss: 1.137588\n",
      "(Iteration 1830 / 14700) loss: 0.881026\n",
      "(Iteration 1831 / 14700) loss: 1.117419\n",
      "(Iteration 1832 / 14700) loss: 1.089857\n",
      "(Iteration 1833 / 14700) loss: 1.232410\n",
      "(Iteration 1834 / 14700) loss: 1.110195\n",
      "(Iteration 1835 / 14700) loss: 0.987933\n",
      "(Iteration 1836 / 14700) loss: 1.287739\n",
      "(Iteration 1837 / 14700) loss: 0.883409\n",
      "(Iteration 1838 / 14700) loss: 1.275029\n",
      "(Iteration 1839 / 14700) loss: 1.047758\n",
      "(Iteration 1840 / 14700) loss: 1.275511\n",
      "(Iteration 1841 / 14700) loss: 1.194873\n",
      "(Iteration 1842 / 14700) loss: 1.039095\n",
      "(Iteration 1843 / 14700) loss: 1.193828\n",
      "(Iteration 1844 / 14700) loss: 1.150109\n",
      "(Iteration 1845 / 14700) loss: 0.986871\n",
      "(Iteration 1846 / 14700) loss: 1.325583\n",
      "(Iteration 1847 / 14700) loss: 1.198327\n",
      "(Iteration 1848 / 14700) loss: 1.423115\n",
      "(Iteration 1849 / 14700) loss: 1.007446\n",
      "(Iteration 1850 / 14700) loss: 1.148869\n",
      "(Iteration 1851 / 14700) loss: 1.132637\n",
      "(Iteration 1852 / 14700) loss: 1.072349\n",
      "(Iteration 1853 / 14700) loss: 0.884289\n",
      "(Iteration 1854 / 14700) loss: 1.003151\n",
      "(Iteration 1855 / 14700) loss: 0.930048\n",
      "(Iteration 1856 / 14700) loss: 0.909016\n",
      "(Iteration 1857 / 14700) loss: 1.302612\n",
      "(Iteration 1858 / 14700) loss: 0.807911\n",
      "(Iteration 1859 / 14700) loss: 1.061053\n",
      "(Iteration 1860 / 14700) loss: 1.061760\n",
      "(Iteration 1861 / 14700) loss: 1.173859\n",
      "(Iteration 1862 / 14700) loss: 1.140094\n",
      "(Iteration 1863 / 14700) loss: 0.893699\n",
      "(Iteration 1864 / 14700) loss: 1.081231\n",
      "(Iteration 1865 / 14700) loss: 1.090963\n",
      "(Iteration 1866 / 14700) loss: 1.161406\n",
      "(Iteration 1867 / 14700) loss: 1.099150\n",
      "(Iteration 1868 / 14700) loss: 1.497257\n",
      "(Iteration 1869 / 14700) loss: 1.038806\n",
      "(Iteration 1870 / 14700) loss: 1.044687\n",
      "(Iteration 1871 / 14700) loss: 1.032056\n",
      "(Iteration 1872 / 14700) loss: 1.128199\n",
      "(Iteration 1873 / 14700) loss: 1.054929\n",
      "(Iteration 1874 / 14700) loss: 0.879506\n",
      "(Iteration 1875 / 14700) loss: 1.014834\n",
      "(Iteration 1876 / 14700) loss: 1.088517\n",
      "(Iteration 1877 / 14700) loss: 1.397690\n",
      "(Iteration 1878 / 14700) loss: 1.168575\n",
      "(Iteration 1879 / 14700) loss: 0.895178\n",
      "(Iteration 1880 / 14700) loss: 1.387640\n",
      "(Iteration 1881 / 14700) loss: 1.005049\n",
      "(Iteration 1882 / 14700) loss: 0.939497\n",
      "(Iteration 1883 / 14700) loss: 1.032712\n",
      "(Iteration 1884 / 14700) loss: 1.041478\n",
      "(Iteration 1885 / 14700) loss: 1.150772\n",
      "(Iteration 1886 / 14700) loss: 1.040505\n",
      "(Iteration 1887 / 14700) loss: 1.503758\n",
      "(Iteration 1888 / 14700) loss: 1.499896\n",
      "(Iteration 1889 / 14700) loss: 1.302696\n",
      "(Iteration 1890 / 14700) loss: 1.026883\n",
      "(Iteration 1891 / 14700) loss: 1.143048\n",
      "(Iteration 1892 / 14700) loss: 1.104543\n",
      "(Iteration 1893 / 14700) loss: 1.374362\n",
      "(Iteration 1894 / 14700) loss: 0.944822\n",
      "(Iteration 1895 / 14700) loss: 1.055054\n",
      "(Iteration 1896 / 14700) loss: 1.504685\n",
      "(Iteration 1897 / 14700) loss: 0.867535\n",
      "(Iteration 1898 / 14700) loss: 1.182518\n",
      "(Iteration 1899 / 14700) loss: 1.257081\n",
      "(Iteration 1900 / 14700) loss: 1.019125\n",
      "(Iteration 1901 / 14700) loss: 1.216845\n",
      "(Iteration 1902 / 14700) loss: 1.479817\n",
      "(Iteration 1903 / 14700) loss: 1.172484\n",
      "(Iteration 1904 / 14700) loss: 1.181702\n",
      "(Iteration 1905 / 14700) loss: 1.090597\n",
      "(Iteration 1906 / 14700) loss: 1.068209\n",
      "(Iteration 1907 / 14700) loss: 1.269984\n",
      "(Iteration 1908 / 14700) loss: 1.041693\n",
      "(Iteration 1909 / 14700) loss: 0.908907\n",
      "(Iteration 1910 / 14700) loss: 0.970365\n",
      "(Iteration 1911 / 14700) loss: 1.330526\n",
      "(Iteration 1912 / 14700) loss: 1.076190\n",
      "(Iteration 1913 / 14700) loss: 1.065693\n",
      "(Iteration 1914 / 14700) loss: 0.922812\n",
      "(Iteration 1915 / 14700) loss: 1.336162\n",
      "(Iteration 1916 / 14700) loss: 0.982126\n",
      "(Iteration 1917 / 14700) loss: 1.073941\n",
      "(Iteration 1918 / 14700) loss: 1.414135\n",
      "(Iteration 1919 / 14700) loss: 1.191897\n",
      "(Iteration 1920 / 14700) loss: 0.959864\n",
      "(Iteration 1921 / 14700) loss: 0.932788\n",
      "(Iteration 1922 / 14700) loss: 0.956983\n",
      "(Iteration 1923 / 14700) loss: 1.101855\n",
      "(Iteration 1924 / 14700) loss: 1.151713\n",
      "(Iteration 1925 / 14700) loss: 1.021119\n",
      "(Iteration 1926 / 14700) loss: 1.160205\n",
      "(Iteration 1927 / 14700) loss: 0.904431\n",
      "(Iteration 1928 / 14700) loss: 1.141668\n",
      "(Iteration 1929 / 14700) loss: 1.250392\n",
      "(Iteration 1930 / 14700) loss: 0.710315\n",
      "(Iteration 1931 / 14700) loss: 1.071456\n",
      "(Iteration 1932 / 14700) loss: 1.301509\n",
      "(Iteration 1933 / 14700) loss: 1.341501\n",
      "(Iteration 1934 / 14700) loss: 1.285998\n",
      "(Iteration 1935 / 14700) loss: 1.072686\n",
      "(Iteration 1936 / 14700) loss: 0.902681\n",
      "(Iteration 1937 / 14700) loss: 1.100352\n",
      "(Iteration 1938 / 14700) loss: 1.081807\n",
      "(Iteration 1939 / 14700) loss: 1.193264\n",
      "(Iteration 1940 / 14700) loss: 1.088759\n",
      "(Iteration 1941 / 14700) loss: 0.850439\n",
      "(Iteration 1942 / 14700) loss: 1.021221\n",
      "(Iteration 1943 / 14700) loss: 1.263038\n",
      "(Iteration 1944 / 14700) loss: 1.068582\n",
      "(Iteration 1945 / 14700) loss: 1.126683\n",
      "(Iteration 1946 / 14700) loss: 0.911918\n",
      "(Iteration 1947 / 14700) loss: 1.602863\n",
      "(Iteration 1948 / 14700) loss: 1.020927\n",
      "(Iteration 1949 / 14700) loss: 0.947371\n",
      "(Iteration 1950 / 14700) loss: 1.002431\n",
      "(Iteration 1951 / 14700) loss: 1.062752\n",
      "(Iteration 1952 / 14700) loss: 1.123111\n",
      "(Iteration 1953 / 14700) loss: 1.186712\n",
      "(Iteration 1954 / 14700) loss: 1.037506\n",
      "(Iteration 1955 / 14700) loss: 1.141861\n",
      "(Iteration 1956 / 14700) loss: 1.104462\n",
      "(Iteration 1957 / 14700) loss: 1.095390\n",
      "(Iteration 1958 / 14700) loss: 0.999882\n",
      "(Iteration 1959 / 14700) loss: 1.269447\n",
      "(Iteration 1960 / 14700) loss: 1.174665\n",
      "(Epoch 2 / 15) train acc: 0.691000; val_acc: 0.683000\n",
      "(Iteration 1961 / 14700) loss: 1.031984\n",
      "(Iteration 1962 / 14700) loss: 1.004438\n",
      "(Iteration 1963 / 14700) loss: 1.239092\n",
      "(Iteration 1964 / 14700) loss: 1.165167\n",
      "(Iteration 1965 / 14700) loss: 1.106715\n",
      "(Iteration 1966 / 14700) loss: 1.004212\n",
      "(Iteration 1967 / 14700) loss: 1.167177\n",
      "(Iteration 1968 / 14700) loss: 1.157055\n",
      "(Iteration 1969 / 14700) loss: 0.969527\n",
      "(Iteration 1970 / 14700) loss: 0.944050\n",
      "(Iteration 1971 / 14700) loss: 1.034852\n",
      "(Iteration 1972 / 14700) loss: 1.037378\n",
      "(Iteration 1973 / 14700) loss: 1.076361\n",
      "(Iteration 1974 / 14700) loss: 1.057182\n",
      "(Iteration 1975 / 14700) loss: 1.179617\n",
      "(Iteration 1976 / 14700) loss: 1.174055\n",
      "(Iteration 1977 / 14700) loss: 1.165089\n",
      "(Iteration 1978 / 14700) loss: 0.905634\n",
      "(Iteration 1979 / 14700) loss: 0.993481\n",
      "(Iteration 1980 / 14700) loss: 0.971591\n",
      "(Iteration 1981 / 14700) loss: 1.117244\n",
      "(Iteration 1982 / 14700) loss: 1.028877\n",
      "(Iteration 1983 / 14700) loss: 1.120775\n",
      "(Iteration 1984 / 14700) loss: 0.900229\n",
      "(Iteration 1985 / 14700) loss: 1.347863\n",
      "(Iteration 1986 / 14700) loss: 1.192617\n",
      "(Iteration 1987 / 14700) loss: 1.195400\n",
      "(Iteration 1988 / 14700) loss: 1.020491\n",
      "(Iteration 1989 / 14700) loss: 0.950889\n",
      "(Iteration 1990 / 14700) loss: 1.037734\n",
      "(Iteration 1991 / 14700) loss: 1.120789\n",
      "(Iteration 1992 / 14700) loss: 1.058819\n",
      "(Iteration 1993 / 14700) loss: 1.101380\n",
      "(Iteration 1994 / 14700) loss: 1.132100\n",
      "(Iteration 1995 / 14700) loss: 1.027135\n",
      "(Iteration 1996 / 14700) loss: 1.402510\n",
      "(Iteration 1997 / 14700) loss: 1.378426\n",
      "(Iteration 1998 / 14700) loss: 1.157638\n",
      "(Iteration 1999 / 14700) loss: 1.086479\n",
      "(Iteration 2000 / 14700) loss: 0.986976\n",
      "(Iteration 2001 / 14700) loss: 1.024933\n",
      "(Iteration 2002 / 14700) loss: 1.100373\n",
      "(Iteration 2003 / 14700) loss: 1.103504\n",
      "(Iteration 2004 / 14700) loss: 1.019929\n",
      "(Iteration 2005 / 14700) loss: 0.873274\n",
      "(Iteration 2006 / 14700) loss: 1.307858\n",
      "(Iteration 2007 / 14700) loss: 1.073287\n",
      "(Iteration 2008 / 14700) loss: 1.052912\n",
      "(Iteration 2009 / 14700) loss: 1.340651\n",
      "(Iteration 2010 / 14700) loss: 1.117014\n",
      "(Iteration 2011 / 14700) loss: 1.041313\n",
      "(Iteration 2012 / 14700) loss: 1.096480\n",
      "(Iteration 2013 / 14700) loss: 1.092191\n",
      "(Iteration 2014 / 14700) loss: 1.233017\n",
      "(Iteration 2015 / 14700) loss: 1.069514\n",
      "(Iteration 2016 / 14700) loss: 0.816440\n",
      "(Iteration 2017 / 14700) loss: 0.893295\n",
      "(Iteration 2018 / 14700) loss: 0.819900\n",
      "(Iteration 2019 / 14700) loss: 1.146721\n",
      "(Iteration 2020 / 14700) loss: 0.883852\n",
      "(Iteration 2021 / 14700) loss: 1.015585\n",
      "(Iteration 2022 / 14700) loss: 1.065015\n",
      "(Iteration 2023 / 14700) loss: 1.262520\n",
      "(Iteration 2024 / 14700) loss: 1.166011\n",
      "(Iteration 2025 / 14700) loss: 1.162355\n",
      "(Iteration 2026 / 14700) loss: 1.228262\n",
      "(Iteration 2027 / 14700) loss: 0.987815\n",
      "(Iteration 2028 / 14700) loss: 1.005014\n",
      "(Iteration 2029 / 14700) loss: 1.052032\n",
      "(Iteration 2030 / 14700) loss: 1.257989\n",
      "(Iteration 2031 / 14700) loss: 1.294079\n",
      "(Iteration 2032 / 14700) loss: 1.209244\n",
      "(Iteration 2033 / 14700) loss: 1.017263\n",
      "(Iteration 2034 / 14700) loss: 1.322385\n",
      "(Iteration 2035 / 14700) loss: 0.924112\n",
      "(Iteration 2036 / 14700) loss: 1.018531\n",
      "(Iteration 2037 / 14700) loss: 1.015869\n",
      "(Iteration 2038 / 14700) loss: 1.384227\n",
      "(Iteration 2039 / 14700) loss: 1.047061\n",
      "(Iteration 2040 / 14700) loss: 0.896138\n",
      "(Iteration 2041 / 14700) loss: 1.118941\n",
      "(Iteration 2042 / 14700) loss: 1.046801\n",
      "(Iteration 2043 / 14700) loss: 0.864160\n",
      "(Iteration 2044 / 14700) loss: 1.130993\n",
      "(Iteration 2045 / 14700) loss: 1.134764\n",
      "(Iteration 2046 / 14700) loss: 1.348288\n",
      "(Iteration 2047 / 14700) loss: 1.157853\n",
      "(Iteration 2048 / 14700) loss: 0.910105\n",
      "(Iteration 2049 / 14700) loss: 1.062333\n",
      "(Iteration 2050 / 14700) loss: 1.142363\n",
      "(Iteration 2051 / 14700) loss: 1.459100\n",
      "(Iteration 2052 / 14700) loss: 1.119735\n",
      "(Iteration 2053 / 14700) loss: 1.048183\n",
      "(Iteration 2054 / 14700) loss: 1.044742\n",
      "(Iteration 2055 / 14700) loss: 1.162360\n",
      "(Iteration 2056 / 14700) loss: 1.276019\n",
      "(Iteration 2057 / 14700) loss: 1.135789\n",
      "(Iteration 2058 / 14700) loss: 1.218565\n",
      "(Iteration 2059 / 14700) loss: 1.216083\n",
      "(Iteration 2060 / 14700) loss: 1.311745\n",
      "(Iteration 2061 / 14700) loss: 1.058610\n",
      "(Iteration 2062 / 14700) loss: 1.142537\n",
      "(Iteration 2063 / 14700) loss: 1.044502\n",
      "(Iteration 2064 / 14700) loss: 1.166182\n",
      "(Iteration 2065 / 14700) loss: 0.971395\n",
      "(Iteration 2066 / 14700) loss: 1.282619\n",
      "(Iteration 2067 / 14700) loss: 1.444268\n",
      "(Iteration 2068 / 14700) loss: 0.944093\n",
      "(Iteration 2069 / 14700) loss: 1.084564\n",
      "(Iteration 2070 / 14700) loss: 1.069185\n",
      "(Iteration 2071 / 14700) loss: 0.978087\n",
      "(Iteration 2072 / 14700) loss: 0.911617\n",
      "(Iteration 2073 / 14700) loss: 1.215647\n",
      "(Iteration 2074 / 14700) loss: 0.832413\n",
      "(Iteration 2075 / 14700) loss: 0.842830\n",
      "(Iteration 2076 / 14700) loss: 0.924652\n",
      "(Iteration 2077 / 14700) loss: 0.930349\n",
      "(Iteration 2078 / 14700) loss: 1.516970\n",
      "(Iteration 2079 / 14700) loss: 0.851576\n",
      "(Iteration 2080 / 14700) loss: 1.032479\n",
      "(Iteration 2081 / 14700) loss: 1.065316\n",
      "(Iteration 2082 / 14700) loss: 1.073621\n",
      "(Iteration 2083 / 14700) loss: 0.922942\n",
      "(Iteration 2084 / 14700) loss: 1.184142\n",
      "(Iteration 2085 / 14700) loss: 1.162186\n",
      "(Iteration 2086 / 14700) loss: 1.593482\n",
      "(Iteration 2087 / 14700) loss: 0.939761\n",
      "(Iteration 2088 / 14700) loss: 1.028512\n",
      "(Iteration 2089 / 14700) loss: 1.220315\n",
      "(Iteration 2090 / 14700) loss: 0.911689\n",
      "(Iteration 2091 / 14700) loss: 1.187025\n",
      "(Iteration 2092 / 14700) loss: 0.812892\n",
      "(Iteration 2093 / 14700) loss: 1.084177\n",
      "(Iteration 2094 / 14700) loss: 0.944254\n",
      "(Iteration 2095 / 14700) loss: 1.141088\n",
      "(Iteration 2096 / 14700) loss: 1.229689\n",
      "(Iteration 2097 / 14700) loss: 0.979426\n",
      "(Iteration 2098 / 14700) loss: 1.219287\n",
      "(Iteration 2099 / 14700) loss: 1.032492\n",
      "(Iteration 2100 / 14700) loss: 1.141842\n",
      "(Iteration 2101 / 14700) loss: 0.876992\n",
      "(Iteration 2102 / 14700) loss: 0.972565\n",
      "(Iteration 2103 / 14700) loss: 0.789461\n",
      "(Iteration 2104 / 14700) loss: 0.897785\n",
      "(Iteration 2105 / 14700) loss: 1.115307\n",
      "(Iteration 2106 / 14700) loss: 0.880215\n",
      "(Iteration 2107 / 14700) loss: 0.970898\n",
      "(Iteration 2108 / 14700) loss: 0.996919\n",
      "(Iteration 2109 / 14700) loss: 0.836590\n",
      "(Iteration 2110 / 14700) loss: 1.251584\n",
      "(Iteration 2111 / 14700) loss: 0.980160\n",
      "(Iteration 2112 / 14700) loss: 1.063412\n",
      "(Iteration 2113 / 14700) loss: 1.370802\n",
      "(Iteration 2114 / 14700) loss: 0.913470\n",
      "(Iteration 2115 / 14700) loss: 1.359908\n",
      "(Iteration 2116 / 14700) loss: 0.923849\n",
      "(Iteration 2117 / 14700) loss: 1.131265\n",
      "(Iteration 2118 / 14700) loss: 1.351969\n",
      "(Iteration 2119 / 14700) loss: 1.051863\n",
      "(Iteration 2120 / 14700) loss: 1.145238\n",
      "(Iteration 2121 / 14700) loss: 1.219470\n",
      "(Iteration 2122 / 14700) loss: 0.857041\n",
      "(Iteration 2123 / 14700) loss: 1.174052\n",
      "(Iteration 2124 / 14700) loss: 1.372929\n",
      "(Iteration 2125 / 14700) loss: 1.100964\n",
      "(Iteration 2126 / 14700) loss: 1.131179\n",
      "(Iteration 2127 / 14700) loss: 0.920706\n",
      "(Iteration 2128 / 14700) loss: 1.103493\n",
      "(Iteration 2129 / 14700) loss: 1.068041\n",
      "(Iteration 2130 / 14700) loss: 1.121787\n",
      "(Iteration 2131 / 14700) loss: 1.139039\n",
      "(Iteration 2132 / 14700) loss: 0.808434\n",
      "(Iteration 2133 / 14700) loss: 1.125253\n",
      "(Iteration 2134 / 14700) loss: 1.251747\n",
      "(Iteration 2135 / 14700) loss: 1.137538\n",
      "(Iteration 2136 / 14700) loss: 1.363119\n",
      "(Iteration 2137 / 14700) loss: 1.134905\n",
      "(Iteration 2138 / 14700) loss: 1.020175\n",
      "(Iteration 2139 / 14700) loss: 0.893448\n",
      "(Iteration 2140 / 14700) loss: 0.879457\n",
      "(Iteration 2141 / 14700) loss: 0.971064\n",
      "(Iteration 2142 / 14700) loss: 0.972382\n",
      "(Iteration 2143 / 14700) loss: 1.113738\n",
      "(Iteration 2144 / 14700) loss: 1.273146\n",
      "(Iteration 2145 / 14700) loss: 1.127380\n",
      "(Iteration 2146 / 14700) loss: 1.014866\n",
      "(Iteration 2147 / 14700) loss: 1.164395\n",
      "(Iteration 2148 / 14700) loss: 1.278866\n",
      "(Iteration 2149 / 14700) loss: 1.197417\n",
      "(Iteration 2150 / 14700) loss: 1.029373\n",
      "(Iteration 2151 / 14700) loss: 1.036183\n",
      "(Iteration 2152 / 14700) loss: 1.130886\n",
      "(Iteration 2153 / 14700) loss: 0.989313\n",
      "(Iteration 2154 / 14700) loss: 1.050568\n",
      "(Iteration 2155 / 14700) loss: 1.192113\n",
      "(Iteration 2156 / 14700) loss: 1.116257\n",
      "(Iteration 2157 / 14700) loss: 1.049793\n",
      "(Iteration 2158 / 14700) loss: 0.909237\n",
      "(Iteration 2159 / 14700) loss: 1.041091\n",
      "(Iteration 2160 / 14700) loss: 0.996292\n",
      "(Iteration 2161 / 14700) loss: 1.063611\n",
      "(Iteration 2162 / 14700) loss: 0.847687\n",
      "(Iteration 2163 / 14700) loss: 0.977814\n",
      "(Iteration 2164 / 14700) loss: 1.063075\n",
      "(Iteration 2165 / 14700) loss: 1.285103\n",
      "(Iteration 2166 / 14700) loss: 0.945506\n",
      "(Iteration 2167 / 14700) loss: 1.062530\n",
      "(Iteration 2168 / 14700) loss: 1.315791\n",
      "(Iteration 2169 / 14700) loss: 1.133380\n",
      "(Iteration 2170 / 14700) loss: 0.930749\n",
      "(Iteration 2171 / 14700) loss: 0.882146\n",
      "(Iteration 2172 / 14700) loss: 1.246659\n",
      "(Iteration 2173 / 14700) loss: 0.887301\n",
      "(Iteration 2174 / 14700) loss: 1.139515\n",
      "(Iteration 2175 / 14700) loss: 1.143586\n",
      "(Iteration 2176 / 14700) loss: 0.838211\n",
      "(Iteration 2177 / 14700) loss: 1.128571\n",
      "(Iteration 2178 / 14700) loss: 1.254474\n",
      "(Iteration 2179 / 14700) loss: 0.844938\n",
      "(Iteration 2180 / 14700) loss: 1.008434\n",
      "(Iteration 2181 / 14700) loss: 1.106947\n",
      "(Iteration 2182 / 14700) loss: 1.278645\n",
      "(Iteration 2183 / 14700) loss: 0.879806\n",
      "(Iteration 2184 / 14700) loss: 0.881636\n",
      "(Iteration 2185 / 14700) loss: 0.984957\n",
      "(Iteration 2186 / 14700) loss: 1.109108\n",
      "(Iteration 2187 / 14700) loss: 0.966263\n",
      "(Iteration 2188 / 14700) loss: 1.050334\n",
      "(Iteration 2189 / 14700) loss: 1.010630\n",
      "(Iteration 2190 / 14700) loss: 1.150194\n",
      "(Iteration 2191 / 14700) loss: 1.159232\n",
      "(Iteration 2192 / 14700) loss: 0.817254\n",
      "(Iteration 2193 / 14700) loss: 1.253034\n",
      "(Iteration 2194 / 14700) loss: 1.063637\n",
      "(Iteration 2195 / 14700) loss: 0.918376\n",
      "(Iteration 2196 / 14700) loss: 1.268717\n",
      "(Iteration 2197 / 14700) loss: 0.859423\n",
      "(Iteration 2198 / 14700) loss: 1.049808\n",
      "(Iteration 2199 / 14700) loss: 0.976985\n",
      "(Iteration 2200 / 14700) loss: 1.006732\n",
      "(Iteration 2201 / 14700) loss: 1.066218\n",
      "(Iteration 2202 / 14700) loss: 1.137647\n",
      "(Iteration 2203 / 14700) loss: 0.774259\n",
      "(Iteration 2204 / 14700) loss: 1.028901\n",
      "(Iteration 2205 / 14700) loss: 1.218798\n",
      "(Iteration 2206 / 14700) loss: 1.164912\n",
      "(Iteration 2207 / 14700) loss: 0.735491\n",
      "(Iteration 2208 / 14700) loss: 0.955267\n",
      "(Iteration 2209 / 14700) loss: 1.210600\n",
      "(Iteration 2210 / 14700) loss: 1.392742\n",
      "(Iteration 2211 / 14700) loss: 1.050344\n",
      "(Iteration 2212 / 14700) loss: 0.909492\n",
      "(Iteration 2213 / 14700) loss: 1.224833\n",
      "(Iteration 2214 / 14700) loss: 1.017180\n",
      "(Iteration 2215 / 14700) loss: 1.098618\n",
      "(Iteration 2216 / 14700) loss: 1.122778\n",
      "(Iteration 2217 / 14700) loss: 0.984647\n",
      "(Iteration 2218 / 14700) loss: 1.039308\n",
      "(Iteration 2219 / 14700) loss: 1.120631\n",
      "(Iteration 2220 / 14700) loss: 0.878297\n",
      "(Iteration 2221 / 14700) loss: 0.974260\n",
      "(Iteration 2222 / 14700) loss: 1.028495\n",
      "(Iteration 2223 / 14700) loss: 1.074339\n",
      "(Iteration 2224 / 14700) loss: 1.233000\n",
      "(Iteration 2225 / 14700) loss: 1.144403\n",
      "(Iteration 2226 / 14700) loss: 1.265727\n",
      "(Iteration 2227 / 14700) loss: 0.816762\n",
      "(Iteration 2228 / 14700) loss: 0.921960\n",
      "(Iteration 2229 / 14700) loss: 1.371683\n",
      "(Iteration 2230 / 14700) loss: 1.133304\n",
      "(Iteration 2231 / 14700) loss: 1.036164\n",
      "(Iteration 2232 / 14700) loss: 0.988784\n",
      "(Iteration 2233 / 14700) loss: 0.934946\n",
      "(Iteration 2234 / 14700) loss: 0.958037\n",
      "(Iteration 2235 / 14700) loss: 1.084133\n",
      "(Iteration 2236 / 14700) loss: 1.067980\n",
      "(Iteration 2237 / 14700) loss: 1.455115\n",
      "(Iteration 2238 / 14700) loss: 1.108634\n",
      "(Iteration 2239 / 14700) loss: 1.008572\n",
      "(Iteration 2240 / 14700) loss: 1.045602\n",
      "(Iteration 2241 / 14700) loss: 0.974786\n",
      "(Iteration 2242 / 14700) loss: 1.020428\n",
      "(Iteration 2243 / 14700) loss: 1.470085\n",
      "(Iteration 2244 / 14700) loss: 1.114887\n",
      "(Iteration 2245 / 14700) loss: 1.334302\n",
      "(Iteration 2246 / 14700) loss: 1.089407\n",
      "(Iteration 2247 / 14700) loss: 1.320899\n",
      "(Iteration 2248 / 14700) loss: 1.050859\n",
      "(Iteration 2249 / 14700) loss: 0.912535\n",
      "(Iteration 2250 / 14700) loss: 1.019070\n",
      "(Iteration 2251 / 14700) loss: 0.888769\n",
      "(Iteration 2252 / 14700) loss: 0.895937\n",
      "(Iteration 2253 / 14700) loss: 1.092597\n",
      "(Iteration 2254 / 14700) loss: 1.079652\n",
      "(Iteration 2255 / 14700) loss: 1.112939\n",
      "(Iteration 2256 / 14700) loss: 1.092197\n",
      "(Iteration 2257 / 14700) loss: 1.120016\n",
      "(Iteration 2258 / 14700) loss: 0.829473\n",
      "(Iteration 2259 / 14700) loss: 1.324904\n",
      "(Iteration 2260 / 14700) loss: 0.862186\n",
      "(Iteration 2261 / 14700) loss: 1.095006\n",
      "(Iteration 2262 / 14700) loss: 1.191027\n",
      "(Iteration 2263 / 14700) loss: 1.156107\n",
      "(Iteration 2264 / 14700) loss: 1.120139\n",
      "(Iteration 2265 / 14700) loss: 1.250322\n",
      "(Iteration 2266 / 14700) loss: 1.352178\n",
      "(Iteration 2267 / 14700) loss: 1.207988\n",
      "(Iteration 2268 / 14700) loss: 0.863402\n",
      "(Iteration 2269 / 14700) loss: 1.171910\n",
      "(Iteration 2270 / 14700) loss: 0.734066\n",
      "(Iteration 2271 / 14700) loss: 1.277083\n",
      "(Iteration 2272 / 14700) loss: 1.086278\n",
      "(Iteration 2273 / 14700) loss: 0.930714\n",
      "(Iteration 2274 / 14700) loss: 0.831533\n",
      "(Iteration 2275 / 14700) loss: 1.244033\n",
      "(Iteration 2276 / 14700) loss: 1.028657\n",
      "(Iteration 2277 / 14700) loss: 1.038211\n",
      "(Iteration 2278 / 14700) loss: 0.744684\n",
      "(Iteration 2279 / 14700) loss: 1.096104\n",
      "(Iteration 2280 / 14700) loss: 0.771393\n",
      "(Iteration 2281 / 14700) loss: 1.048791\n",
      "(Iteration 2282 / 14700) loss: 0.941995\n",
      "(Iteration 2283 / 14700) loss: 0.680367\n",
      "(Iteration 2284 / 14700) loss: 1.179134\n",
      "(Iteration 2285 / 14700) loss: 1.012241\n",
      "(Iteration 2286 / 14700) loss: 0.859819\n",
      "(Iteration 2287 / 14700) loss: 1.091957\n",
      "(Iteration 2288 / 14700) loss: 1.197059\n",
      "(Iteration 2289 / 14700) loss: 1.087763\n",
      "(Iteration 2290 / 14700) loss: 0.855829\n",
      "(Iteration 2291 / 14700) loss: 0.950665\n",
      "(Iteration 2292 / 14700) loss: 0.944456\n",
      "(Iteration 2293 / 14700) loss: 0.815702\n",
      "(Iteration 2294 / 14700) loss: 1.178761\n",
      "(Iteration 2295 / 14700) loss: 0.977823\n",
      "(Iteration 2296 / 14700) loss: 1.034657\n",
      "(Iteration 2297 / 14700) loss: 0.982522\n",
      "(Iteration 2298 / 14700) loss: 0.870022\n",
      "(Iteration 2299 / 14700) loss: 0.855989\n",
      "(Iteration 2300 / 14700) loss: 0.829006\n",
      "(Iteration 2301 / 14700) loss: 0.772696\n",
      "(Iteration 2302 / 14700) loss: 1.201346\n",
      "(Iteration 2303 / 14700) loss: 0.986058\n",
      "(Iteration 2304 / 14700) loss: 0.927575\n",
      "(Iteration 2305 / 14700) loss: 0.889526\n",
      "(Iteration 2306 / 14700) loss: 0.971525\n",
      "(Iteration 2307 / 14700) loss: 1.066078\n",
      "(Iteration 2308 / 14700) loss: 0.878437\n",
      "(Iteration 2309 / 14700) loss: 1.252648\n",
      "(Iteration 2310 / 14700) loss: 0.861961\n",
      "(Iteration 2311 / 14700) loss: 1.009350\n",
      "(Iteration 2312 / 14700) loss: 0.932280\n",
      "(Iteration 2313 / 14700) loss: 0.706936\n",
      "(Iteration 2314 / 14700) loss: 1.039448\n",
      "(Iteration 2315 / 14700) loss: 1.364463\n",
      "(Iteration 2316 / 14700) loss: 0.950108\n",
      "(Iteration 2317 / 14700) loss: 1.152722\n",
      "(Iteration 2318 / 14700) loss: 0.934140\n",
      "(Iteration 2319 / 14700) loss: 1.193982\n",
      "(Iteration 2320 / 14700) loss: 0.998003\n",
      "(Iteration 2321 / 14700) loss: 0.998011\n",
      "(Iteration 2322 / 14700) loss: 0.794421\n",
      "(Iteration 2323 / 14700) loss: 1.122134\n",
      "(Iteration 2324 / 14700) loss: 0.843921\n",
      "(Iteration 2325 / 14700) loss: 1.389229\n",
      "(Iteration 2326 / 14700) loss: 1.154700\n",
      "(Iteration 2327 / 14700) loss: 0.917341\n",
      "(Iteration 2328 / 14700) loss: 0.754108\n",
      "(Iteration 2329 / 14700) loss: 0.861176\n",
      "(Iteration 2330 / 14700) loss: 1.266722\n",
      "(Iteration 2331 / 14700) loss: 0.989477\n",
      "(Iteration 2332 / 14700) loss: 0.962002\n",
      "(Iteration 2333 / 14700) loss: 0.987484\n",
      "(Iteration 2334 / 14700) loss: 0.999121\n",
      "(Iteration 2335 / 14700) loss: 1.130909\n",
      "(Iteration 2336 / 14700) loss: 1.113378\n",
      "(Iteration 2337 / 14700) loss: 1.259552\n",
      "(Iteration 2338 / 14700) loss: 1.071596\n",
      "(Iteration 2339 / 14700) loss: 0.851796\n",
      "(Iteration 2340 / 14700) loss: 1.123923\n",
      "(Iteration 2341 / 14700) loss: 1.029883\n",
      "(Iteration 2342 / 14700) loss: 0.991734\n",
      "(Iteration 2343 / 14700) loss: 1.061593\n",
      "(Iteration 2344 / 14700) loss: 1.210212\n",
      "(Iteration 2345 / 14700) loss: 1.167802\n",
      "(Iteration 2346 / 14700) loss: 0.963333\n",
      "(Iteration 2347 / 14700) loss: 1.271940\n",
      "(Iteration 2348 / 14700) loss: 1.067432\n",
      "(Iteration 2349 / 14700) loss: 0.995448\n",
      "(Iteration 2350 / 14700) loss: 1.230053\n",
      "(Iteration 2351 / 14700) loss: 1.205032\n",
      "(Iteration 2352 / 14700) loss: 0.873662\n",
      "(Iteration 2353 / 14700) loss: 1.198427\n",
      "(Iteration 2354 / 14700) loss: 1.027824\n",
      "(Iteration 2355 / 14700) loss: 0.919695\n",
      "(Iteration 2356 / 14700) loss: 0.927815\n",
      "(Iteration 2357 / 14700) loss: 0.874533\n",
      "(Iteration 2358 / 14700) loss: 0.978430\n",
      "(Iteration 2359 / 14700) loss: 0.850054\n",
      "(Iteration 2360 / 14700) loss: 1.416189\n",
      "(Iteration 2361 / 14700) loss: 1.164098\n",
      "(Iteration 2362 / 14700) loss: 1.106320\n",
      "(Iteration 2363 / 14700) loss: 0.954682\n",
      "(Iteration 2364 / 14700) loss: 0.996734\n",
      "(Iteration 2365 / 14700) loss: 1.043801\n",
      "(Iteration 2366 / 14700) loss: 1.014590\n",
      "(Iteration 2367 / 14700) loss: 1.116070\n",
      "(Iteration 2368 / 14700) loss: 1.307779\n",
      "(Iteration 2369 / 14700) loss: 1.070162\n",
      "(Iteration 2370 / 14700) loss: 0.940893\n",
      "(Iteration 2371 / 14700) loss: 0.934457\n",
      "(Iteration 2372 / 14700) loss: 0.997976\n",
      "(Iteration 2373 / 14700) loss: 1.044049\n",
      "(Iteration 2374 / 14700) loss: 0.851593\n",
      "(Iteration 2375 / 14700) loss: 0.946898\n",
      "(Iteration 2376 / 14700) loss: 0.990415\n",
      "(Iteration 2377 / 14700) loss: 0.997841\n",
      "(Iteration 2378 / 14700) loss: 1.090814\n",
      "(Iteration 2379 / 14700) loss: 1.218432\n",
      "(Iteration 2380 / 14700) loss: 1.186995\n",
      "(Iteration 2381 / 14700) loss: 0.886845\n",
      "(Iteration 2382 / 14700) loss: 1.024249\n",
      "(Iteration 2383 / 14700) loss: 1.173392\n",
      "(Iteration 2384 / 14700) loss: 1.014545\n",
      "(Iteration 2385 / 14700) loss: 1.130397\n",
      "(Iteration 2386 / 14700) loss: 1.294403\n",
      "(Iteration 2387 / 14700) loss: 0.895347\n",
      "(Iteration 2388 / 14700) loss: 1.033206\n",
      "(Iteration 2389 / 14700) loss: 1.280153\n",
      "(Iteration 2390 / 14700) loss: 0.978405\n",
      "(Iteration 2391 / 14700) loss: 1.155422\n",
      "(Iteration 2392 / 14700) loss: 1.213404\n",
      "(Iteration 2393 / 14700) loss: 1.155254\n",
      "(Iteration 2394 / 14700) loss: 0.927427\n",
      "(Iteration 2395 / 14700) loss: 1.127378\n",
      "(Iteration 2396 / 14700) loss: 1.286818\n",
      "(Iteration 2397 / 14700) loss: 0.957037\n",
      "(Iteration 2398 / 14700) loss: 0.885044\n",
      "(Iteration 2399 / 14700) loss: 1.277661\n",
      "(Iteration 2400 / 14700) loss: 0.976174\n",
      "(Iteration 2401 / 14700) loss: 0.984787\n",
      "(Iteration 2402 / 14700) loss: 0.906075\n",
      "(Iteration 2403 / 14700) loss: 0.981719\n",
      "(Iteration 2404 / 14700) loss: 1.000390\n",
      "(Iteration 2405 / 14700) loss: 1.036205\n",
      "(Iteration 2406 / 14700) loss: 0.746990\n",
      "(Iteration 2407 / 14700) loss: 1.010447\n",
      "(Iteration 2408 / 14700) loss: 0.996811\n",
      "(Iteration 2409 / 14700) loss: 0.815482\n",
      "(Iteration 2410 / 14700) loss: 0.985345\n",
      "(Iteration 2411 / 14700) loss: 1.048241\n",
      "(Iteration 2412 / 14700) loss: 0.870648\n",
      "(Iteration 2413 / 14700) loss: 1.150925\n",
      "(Iteration 2414 / 14700) loss: 1.422463\n",
      "(Iteration 2415 / 14700) loss: 0.913779\n",
      "(Iteration 2416 / 14700) loss: 1.025206\n",
      "(Iteration 2417 / 14700) loss: 0.978222\n",
      "(Iteration 2418 / 14700) loss: 1.134420\n",
      "(Iteration 2419 / 14700) loss: 0.998504\n",
      "(Iteration 2420 / 14700) loss: 1.011501\n",
      "(Iteration 2421 / 14700) loss: 1.300387\n",
      "(Iteration 2422 / 14700) loss: 0.883601\n",
      "(Iteration 2423 / 14700) loss: 0.896782\n",
      "(Iteration 2424 / 14700) loss: 1.138367\n",
      "(Iteration 2425 / 14700) loss: 1.433432\n",
      "(Iteration 2426 / 14700) loss: 1.087939\n",
      "(Iteration 2427 / 14700) loss: 1.109494\n",
      "(Iteration 2428 / 14700) loss: 0.856423\n",
      "(Iteration 2429 / 14700) loss: 1.006899\n",
      "(Iteration 2430 / 14700) loss: 1.178910\n",
      "(Iteration 2431 / 14700) loss: 1.216921\n",
      "(Iteration 2432 / 14700) loss: 0.947402\n",
      "(Iteration 2433 / 14700) loss: 0.914946\n",
      "(Iteration 2434 / 14700) loss: 0.736262\n",
      "(Iteration 2435 / 14700) loss: 0.953249\n",
      "(Iteration 2436 / 14700) loss: 0.796680\n",
      "(Iteration 2437 / 14700) loss: 1.021804\n",
      "(Iteration 2438 / 14700) loss: 0.968898\n",
      "(Iteration 2439 / 14700) loss: 0.951234\n",
      "(Iteration 2440 / 14700) loss: 1.184305\n",
      "(Iteration 2441 / 14700) loss: 0.923737\n",
      "(Iteration 2442 / 14700) loss: 1.258625\n",
      "(Iteration 2443 / 14700) loss: 0.961715\n",
      "(Iteration 2444 / 14700) loss: 1.088211\n",
      "(Iteration 2445 / 14700) loss: 1.167302\n",
      "(Iteration 2446 / 14700) loss: 1.083119\n",
      "(Iteration 2447 / 14700) loss: 1.341129\n",
      "(Iteration 2448 / 14700) loss: 1.148336\n",
      "(Iteration 2449 / 14700) loss: 0.969217\n",
      "(Iteration 2450 / 14700) loss: 0.722904\n",
      "(Iteration 2451 / 14700) loss: 1.009524\n",
      "(Iteration 2452 / 14700) loss: 1.107979\n",
      "(Iteration 2453 / 14700) loss: 0.926944\n",
      "(Iteration 2454 / 14700) loss: 1.263884\n",
      "(Iteration 2455 / 14700) loss: 0.872335\n",
      "(Iteration 2456 / 14700) loss: 0.919912\n",
      "(Iteration 2457 / 14700) loss: 0.876299\n",
      "(Iteration 2458 / 14700) loss: 1.049077\n",
      "(Iteration 2459 / 14700) loss: 0.963756\n",
      "(Iteration 2460 / 14700) loss: 0.803992\n",
      "(Iteration 2461 / 14700) loss: 1.060286\n",
      "(Iteration 2462 / 14700) loss: 0.736649\n",
      "(Iteration 2463 / 14700) loss: 0.907556\n",
      "(Iteration 2464 / 14700) loss: 1.304843\n",
      "(Iteration 2465 / 14700) loss: 0.895681\n",
      "(Iteration 2466 / 14700) loss: 0.855434\n",
      "(Iteration 2467 / 14700) loss: 0.940888\n",
      "(Iteration 2468 / 14700) loss: 0.926462\n",
      "(Iteration 2469 / 14700) loss: 0.919188\n",
      "(Iteration 2470 / 14700) loss: 1.345273\n",
      "(Iteration 2471 / 14700) loss: 1.005893\n",
      "(Iteration 2472 / 14700) loss: 1.147463\n",
      "(Iteration 2473 / 14700) loss: 1.224181\n",
      "(Iteration 2474 / 14700) loss: 0.917747\n",
      "(Iteration 2475 / 14700) loss: 1.201406\n",
      "(Iteration 2476 / 14700) loss: 0.985254\n",
      "(Iteration 2477 / 14700) loss: 1.188578\n",
      "(Iteration 2478 / 14700) loss: 0.922971\n",
      "(Iteration 2479 / 14700) loss: 1.145757\n",
      "(Iteration 2480 / 14700) loss: 1.046507\n",
      "(Iteration 2481 / 14700) loss: 0.921450\n",
      "(Iteration 2482 / 14700) loss: 1.081933\n",
      "(Iteration 2483 / 14700) loss: 1.126170\n",
      "(Iteration 2484 / 14700) loss: 0.822313\n",
      "(Iteration 2485 / 14700) loss: 1.281892\n",
      "(Iteration 2486 / 14700) loss: 1.064627\n",
      "(Iteration 2487 / 14700) loss: 1.041809\n",
      "(Iteration 2488 / 14700) loss: 1.044950\n",
      "(Iteration 2489 / 14700) loss: 0.908066\n",
      "(Iteration 2490 / 14700) loss: 1.053054\n",
      "(Iteration 2491 / 14700) loss: 1.148071\n",
      "(Iteration 2492 / 14700) loss: 1.107928\n",
      "(Iteration 2493 / 14700) loss: 1.073662\n",
      "(Iteration 2494 / 14700) loss: 0.973732\n",
      "(Iteration 2495 / 14700) loss: 1.169433\n",
      "(Iteration 2496 / 14700) loss: 1.028016\n",
      "(Iteration 2497 / 14700) loss: 1.038204\n",
      "(Iteration 2498 / 14700) loss: 1.215059\n",
      "(Iteration 2499 / 14700) loss: 0.863568\n",
      "(Iteration 2500 / 14700) loss: 0.986677\n",
      "(Iteration 2501 / 14700) loss: 0.948445\n",
      "(Iteration 2502 / 14700) loss: 1.307121\n",
      "(Iteration 2503 / 14700) loss: 1.054381\n",
      "(Iteration 2504 / 14700) loss: 1.074670\n",
      "(Iteration 2505 / 14700) loss: 0.947480\n",
      "(Iteration 2506 / 14700) loss: 1.219326\n",
      "(Iteration 2507 / 14700) loss: 0.857153\n",
      "(Iteration 2508 / 14700) loss: 0.901831\n",
      "(Iteration 2509 / 14700) loss: 0.906912\n",
      "(Iteration 2510 / 14700) loss: 0.897442\n",
      "(Iteration 2511 / 14700) loss: 1.045912\n",
      "(Iteration 2512 / 14700) loss: 1.145650\n",
      "(Iteration 2513 / 14700) loss: 0.936953\n",
      "(Iteration 2514 / 14700) loss: 1.284387\n",
      "(Iteration 2515 / 14700) loss: 0.992363\n",
      "(Iteration 2516 / 14700) loss: 0.831836\n",
      "(Iteration 2517 / 14700) loss: 1.171589\n",
      "(Iteration 2518 / 14700) loss: 0.780103\n",
      "(Iteration 2519 / 14700) loss: 1.143990\n",
      "(Iteration 2520 / 14700) loss: 0.897721\n",
      "(Iteration 2521 / 14700) loss: 0.875733\n",
      "(Iteration 2522 / 14700) loss: 1.134278\n",
      "(Iteration 2523 / 14700) loss: 0.789312\n",
      "(Iteration 2524 / 14700) loss: 0.944138\n",
      "(Iteration 2525 / 14700) loss: 0.969971\n",
      "(Iteration 2526 / 14700) loss: 1.111433\n",
      "(Iteration 2527 / 14700) loss: 1.051079\n",
      "(Iteration 2528 / 14700) loss: 1.173278\n",
      "(Iteration 2529 / 14700) loss: 1.069593\n",
      "(Iteration 2530 / 14700) loss: 1.069687\n",
      "(Iteration 2531 / 14700) loss: 1.034649\n",
      "(Iteration 2532 / 14700) loss: 0.770901\n",
      "(Iteration 2533 / 14700) loss: 1.102216\n",
      "(Iteration 2534 / 14700) loss: 0.948382\n",
      "(Iteration 2535 / 14700) loss: 1.004143\n",
      "(Iteration 2536 / 14700) loss: 1.278947\n",
      "(Iteration 2537 / 14700) loss: 1.089108\n",
      "(Iteration 2538 / 14700) loss: 0.856534\n",
      "(Iteration 2539 / 14700) loss: 0.944221\n",
      "(Iteration 2540 / 14700) loss: 1.048795\n",
      "(Iteration 2541 / 14700) loss: 1.034591\n",
      "(Iteration 2542 / 14700) loss: 1.197474\n",
      "(Iteration 2543 / 14700) loss: 1.001203\n",
      "(Iteration 2544 / 14700) loss: 1.115766\n",
      "(Iteration 2545 / 14700) loss: 0.805848\n",
      "(Iteration 2546 / 14700) loss: 1.076069\n",
      "(Iteration 2547 / 14700) loss: 1.099046\n",
      "(Iteration 2548 / 14700) loss: 1.280855\n",
      "(Iteration 2549 / 14700) loss: 1.199012\n",
      "(Iteration 2550 / 14700) loss: 0.951361\n",
      "(Iteration 2551 / 14700) loss: 0.975603\n",
      "(Iteration 2552 / 14700) loss: 0.921018\n",
      "(Iteration 2553 / 14700) loss: 0.869720\n",
      "(Iteration 2554 / 14700) loss: 0.971141\n",
      "(Iteration 2555 / 14700) loss: 1.066768\n",
      "(Iteration 2556 / 14700) loss: 1.076910\n",
      "(Iteration 2557 / 14700) loss: 1.093169\n",
      "(Iteration 2558 / 14700) loss: 1.107830\n",
      "(Iteration 2559 / 14700) loss: 1.102498\n",
      "(Iteration 2560 / 14700) loss: 0.955058\n",
      "(Iteration 2561 / 14700) loss: 1.026456\n",
      "(Iteration 2562 / 14700) loss: 0.974626\n",
      "(Iteration 2563 / 14700) loss: 0.937022\n",
      "(Iteration 2564 / 14700) loss: 1.110503\n",
      "(Iteration 2565 / 14700) loss: 0.961590\n",
      "(Iteration 2566 / 14700) loss: 0.918134\n",
      "(Iteration 2567 / 14700) loss: 0.892622\n",
      "(Iteration 2568 / 14700) loss: 1.159643\n",
      "(Iteration 2569 / 14700) loss: 0.882069\n",
      "(Iteration 2570 / 14700) loss: 1.177842\n",
      "(Iteration 2571 / 14700) loss: 1.067808\n",
      "(Iteration 2572 / 14700) loss: 0.896967\n",
      "(Iteration 2573 / 14700) loss: 1.038572\n",
      "(Iteration 2574 / 14700) loss: 0.891498\n",
      "(Iteration 2575 / 14700) loss: 1.101365\n",
      "(Iteration 2576 / 14700) loss: 1.023710\n",
      "(Iteration 2577 / 14700) loss: 1.023316\n",
      "(Iteration 2578 / 14700) loss: 1.022770\n",
      "(Iteration 2579 / 14700) loss: 1.036527\n",
      "(Iteration 2580 / 14700) loss: 1.075428\n",
      "(Iteration 2581 / 14700) loss: 0.926904\n",
      "(Iteration 2582 / 14700) loss: 1.042063\n",
      "(Iteration 2583 / 14700) loss: 1.172529\n",
      "(Iteration 2584 / 14700) loss: 0.748973\n",
      "(Iteration 2585 / 14700) loss: 1.059489\n",
      "(Iteration 2586 / 14700) loss: 1.253457\n",
      "(Iteration 2587 / 14700) loss: 0.720656\n",
      "(Iteration 2588 / 14700) loss: 1.031072\n",
      "(Iteration 2589 / 14700) loss: 0.823430\n",
      "(Iteration 2590 / 14700) loss: 0.990189\n",
      "(Iteration 2591 / 14700) loss: 1.047364\n",
      "(Iteration 2592 / 14700) loss: 0.982578\n",
      "(Iteration 2593 / 14700) loss: 1.083195\n",
      "(Iteration 2594 / 14700) loss: 1.241737\n",
      "(Iteration 2595 / 14700) loss: 1.113514\n",
      "(Iteration 2596 / 14700) loss: 0.963191\n",
      "(Iteration 2597 / 14700) loss: 0.857647\n",
      "(Iteration 2598 / 14700) loss: 0.891543\n",
      "(Iteration 2599 / 14700) loss: 0.996172\n",
      "(Iteration 2600 / 14700) loss: 1.329465\n",
      "(Iteration 2601 / 14700) loss: 1.144093\n",
      "(Iteration 2602 / 14700) loss: 1.181365\n",
      "(Iteration 2603 / 14700) loss: 0.844973\n",
      "(Iteration 2604 / 14700) loss: 1.002116\n",
      "(Iteration 2605 / 14700) loss: 0.917223\n",
      "(Iteration 2606 / 14700) loss: 1.095878\n",
      "(Iteration 2607 / 14700) loss: 0.959846\n",
      "(Iteration 2608 / 14700) loss: 0.772295\n",
      "(Iteration 2609 / 14700) loss: 0.901143\n",
      "(Iteration 2610 / 14700) loss: 1.110894\n",
      "(Iteration 2611 / 14700) loss: 0.961198\n",
      "(Iteration 2612 / 14700) loss: 0.994597\n",
      "(Iteration 2613 / 14700) loss: 0.889755\n",
      "(Iteration 2614 / 14700) loss: 0.931791\n",
      "(Iteration 2615 / 14700) loss: 0.966729\n",
      "(Iteration 2616 / 14700) loss: 0.982151\n",
      "(Iteration 2617 / 14700) loss: 1.211874\n",
      "(Iteration 2618 / 14700) loss: 0.944094\n",
      "(Iteration 2619 / 14700) loss: 0.969063\n",
      "(Iteration 2620 / 14700) loss: 0.838709\n",
      "(Iteration 2621 / 14700) loss: 0.884183\n",
      "(Iteration 2622 / 14700) loss: 1.064812\n",
      "(Iteration 2623 / 14700) loss: 0.946792\n",
      "(Iteration 2624 / 14700) loss: 0.940956\n",
      "(Iteration 2625 / 14700) loss: 1.296107\n",
      "(Iteration 2626 / 14700) loss: 0.771073\n",
      "(Iteration 2627 / 14700) loss: 1.118899\n",
      "(Iteration 2628 / 14700) loss: 0.949991\n",
      "(Iteration 2629 / 14700) loss: 0.908015\n",
      "(Iteration 2630 / 14700) loss: 0.726252\n",
      "(Iteration 2631 / 14700) loss: 1.102128\n",
      "(Iteration 2632 / 14700) loss: 0.879898\n",
      "(Iteration 2633 / 14700) loss: 0.830260\n",
      "(Iteration 2634 / 14700) loss: 0.952048\n",
      "(Iteration 2635 / 14700) loss: 0.969563\n",
      "(Iteration 2636 / 14700) loss: 1.001962\n",
      "(Iteration 2637 / 14700) loss: 0.966772\n",
      "(Iteration 2638 / 14700) loss: 0.991693\n",
      "(Iteration 2639 / 14700) loss: 1.026962\n",
      "(Iteration 2640 / 14700) loss: 0.928065\n",
      "(Iteration 2641 / 14700) loss: 0.827176\n",
      "(Iteration 2642 / 14700) loss: 0.834196\n",
      "(Iteration 2643 / 14700) loss: 0.806172\n",
      "(Iteration 2644 / 14700) loss: 1.161852\n",
      "(Iteration 2645 / 14700) loss: 0.919135\n",
      "(Iteration 2646 / 14700) loss: 1.050350\n",
      "(Iteration 2647 / 14700) loss: 1.024198\n",
      "(Iteration 2648 / 14700) loss: 1.020186\n",
      "(Iteration 2649 / 14700) loss: 1.161391\n",
      "(Iteration 2650 / 14700) loss: 0.820566\n",
      "(Iteration 2651 / 14700) loss: 0.883596\n",
      "(Iteration 2652 / 14700) loss: 1.175048\n",
      "(Iteration 2653 / 14700) loss: 0.900868\n",
      "(Iteration 2654 / 14700) loss: 0.895046\n",
      "(Iteration 2655 / 14700) loss: 1.013767\n",
      "(Iteration 2656 / 14700) loss: 0.821770\n",
      "(Iteration 2657 / 14700) loss: 0.828249\n",
      "(Iteration 2658 / 14700) loss: 0.668981\n",
      "(Iteration 2659 / 14700) loss: 0.803855\n",
      "(Iteration 2660 / 14700) loss: 0.968292\n",
      "(Iteration 2661 / 14700) loss: 0.758102\n",
      "(Iteration 2662 / 14700) loss: 0.961186\n",
      "(Iteration 2663 / 14700) loss: 0.932375\n",
      "(Iteration 2664 / 14700) loss: 1.304966\n",
      "(Iteration 2665 / 14700) loss: 1.128005\n",
      "(Iteration 2666 / 14700) loss: 1.148940\n",
      "(Iteration 2667 / 14700) loss: 0.971414\n",
      "(Iteration 2668 / 14700) loss: 0.835173\n",
      "(Iteration 2669 / 14700) loss: 1.107942\n",
      "(Iteration 2670 / 14700) loss: 0.894431\n",
      "(Iteration 2671 / 14700) loss: 1.119549\n",
      "(Iteration 2672 / 14700) loss: 0.910977\n",
      "(Iteration 2673 / 14700) loss: 0.761436\n",
      "(Iteration 2674 / 14700) loss: 1.260330\n",
      "(Iteration 2675 / 14700) loss: 1.133429\n",
      "(Iteration 2676 / 14700) loss: 0.855105\n",
      "(Iteration 2677 / 14700) loss: 0.954000\n",
      "(Iteration 2678 / 14700) loss: 0.907601\n",
      "(Iteration 2679 / 14700) loss: 1.062879\n",
      "(Iteration 2680 / 14700) loss: 1.082019\n",
      "(Iteration 2681 / 14700) loss: 0.952455\n",
      "(Iteration 2682 / 14700) loss: 0.982531\n",
      "(Iteration 2683 / 14700) loss: 0.855801\n",
      "(Iteration 2684 / 14700) loss: 1.061876\n",
      "(Iteration 2685 / 14700) loss: 1.202956\n",
      "(Iteration 2686 / 14700) loss: 0.871245\n",
      "(Iteration 2687 / 14700) loss: 1.219742\n",
      "(Iteration 2688 / 14700) loss: 0.938069\n",
      "(Iteration 2689 / 14700) loss: 1.056297\n",
      "(Iteration 2690 / 14700) loss: 1.068281\n",
      "(Iteration 2691 / 14700) loss: 0.791986\n",
      "(Iteration 2692 / 14700) loss: 0.803439\n",
      "(Iteration 2693 / 14700) loss: 1.321027\n",
      "(Iteration 2694 / 14700) loss: 1.267666\n",
      "(Iteration 2695 / 14700) loss: 1.030697\n",
      "(Iteration 2696 / 14700) loss: 0.937353\n",
      "(Iteration 2697 / 14700) loss: 1.154267\n",
      "(Iteration 2698 / 14700) loss: 1.066638\n",
      "(Iteration 2699 / 14700) loss: 0.742284\n",
      "(Iteration 2700 / 14700) loss: 0.950008\n",
      "(Iteration 2701 / 14700) loss: 0.984071\n",
      "(Iteration 2702 / 14700) loss: 1.008211\n",
      "(Iteration 2703 / 14700) loss: 1.059651\n",
      "(Iteration 2704 / 14700) loss: 0.954265\n",
      "(Iteration 2705 / 14700) loss: 1.005645\n",
      "(Iteration 2706 / 14700) loss: 0.803839\n",
      "(Iteration 2707 / 14700) loss: 1.101889\n",
      "(Iteration 2708 / 14700) loss: 1.155425\n",
      "(Iteration 2709 / 14700) loss: 1.004633\n",
      "(Iteration 2710 / 14700) loss: 0.832031\n",
      "(Iteration 2711 / 14700) loss: 0.768877\n",
      "(Iteration 2712 / 14700) loss: 1.113111\n",
      "(Iteration 2713 / 14700) loss: 1.096842\n",
      "(Iteration 2714 / 14700) loss: 0.879906\n",
      "(Iteration 2715 / 14700) loss: 0.992447\n",
      "(Iteration 2716 / 14700) loss: 1.052700\n",
      "(Iteration 2717 / 14700) loss: 0.780388\n",
      "(Iteration 2718 / 14700) loss: 1.018741\n",
      "(Iteration 2719 / 14700) loss: 1.275340\n",
      "(Iteration 2720 / 14700) loss: 1.177833\n",
      "(Iteration 2721 / 14700) loss: 1.204319\n",
      "(Iteration 2722 / 14700) loss: 0.936989\n",
      "(Iteration 2723 / 14700) loss: 1.156417\n",
      "(Iteration 2724 / 14700) loss: 1.037488\n",
      "(Iteration 2725 / 14700) loss: 0.731906\n",
      "(Iteration 2726 / 14700) loss: 0.891160\n",
      "(Iteration 2727 / 14700) loss: 1.109228\n",
      "(Iteration 2728 / 14700) loss: 0.878084\n",
      "(Iteration 2729 / 14700) loss: 1.062314\n",
      "(Iteration 2730 / 14700) loss: 0.830185\n",
      "(Iteration 2731 / 14700) loss: 1.164356\n",
      "(Iteration 2732 / 14700) loss: 0.776748\n",
      "(Iteration 2733 / 14700) loss: 1.029414\n",
      "(Iteration 2734 / 14700) loss: 0.997957\n",
      "(Iteration 2735 / 14700) loss: 1.033356\n",
      "(Iteration 2736 / 14700) loss: 1.182502\n",
      "(Iteration 2737 / 14700) loss: 0.959865\n",
      "(Iteration 2738 / 14700) loss: 1.028194\n",
      "(Iteration 2739 / 14700) loss: 1.047592\n",
      "(Iteration 2740 / 14700) loss: 0.887972\n",
      "(Iteration 2741 / 14700) loss: 0.996597\n",
      "(Iteration 2742 / 14700) loss: 1.052389\n",
      "(Iteration 2743 / 14700) loss: 1.040866\n",
      "(Iteration 2744 / 14700) loss: 1.084670\n",
      "(Iteration 2745 / 14700) loss: 1.044138\n",
      "(Iteration 2746 / 14700) loss: 0.892961\n",
      "(Iteration 2747 / 14700) loss: 0.816510\n",
      "(Iteration 2748 / 14700) loss: 1.047963\n",
      "(Iteration 2749 / 14700) loss: 1.159987\n",
      "(Iteration 2750 / 14700) loss: 0.987412\n",
      "(Iteration 2751 / 14700) loss: 1.006997\n",
      "(Iteration 2752 / 14700) loss: 0.898710\n",
      "(Iteration 2753 / 14700) loss: 1.156396\n",
      "(Iteration 2754 / 14700) loss: 0.894934\n",
      "(Iteration 2755 / 14700) loss: 0.912292\n",
      "(Iteration 2756 / 14700) loss: 0.897174\n",
      "(Iteration 2757 / 14700) loss: 0.951738\n",
      "(Iteration 2758 / 14700) loss: 0.893592\n",
      "(Iteration 2759 / 14700) loss: 1.087358\n",
      "(Iteration 2760 / 14700) loss: 0.795992\n",
      "(Iteration 2761 / 14700) loss: 0.851933\n",
      "(Iteration 2762 / 14700) loss: 0.945886\n",
      "(Iteration 2763 / 14700) loss: 0.778486\n",
      "(Iteration 2764 / 14700) loss: 0.911113\n",
      "(Iteration 2765 / 14700) loss: 0.943805\n",
      "(Iteration 2766 / 14700) loss: 0.807378\n",
      "(Iteration 2767 / 14700) loss: 1.081579\n",
      "(Iteration 2768 / 14700) loss: 0.811137\n",
      "(Iteration 2769 / 14700) loss: 1.246611\n",
      "(Iteration 2770 / 14700) loss: 0.780321\n",
      "(Iteration 2771 / 14700) loss: 1.011103\n",
      "(Iteration 2772 / 14700) loss: 1.069710\n",
      "(Iteration 2773 / 14700) loss: 0.843660\n",
      "(Iteration 2774 / 14700) loss: 0.811168\n",
      "(Iteration 2775 / 14700) loss: 0.927176\n",
      "(Iteration 2776 / 14700) loss: 0.980786\n",
      "(Iteration 2777 / 14700) loss: 1.139382\n",
      "(Iteration 2778 / 14700) loss: 1.079561\n",
      "(Iteration 2779 / 14700) loss: 0.795540\n",
      "(Iteration 2780 / 14700) loss: 0.920360\n",
      "(Iteration 2781 / 14700) loss: 0.960087\n",
      "(Iteration 2782 / 14700) loss: 0.919345\n",
      "(Iteration 2783 / 14700) loss: 0.780360\n",
      "(Iteration 2784 / 14700) loss: 1.008575\n",
      "(Iteration 2785 / 14700) loss: 1.085066\n",
      "(Iteration 2786 / 14700) loss: 0.802218\n",
      "(Iteration 2787 / 14700) loss: 1.005633\n",
      "(Iteration 2788 / 14700) loss: 1.111907\n",
      "(Iteration 2789 / 14700) loss: 0.948425\n",
      "(Iteration 2790 / 14700) loss: 0.812781\n",
      "(Iteration 2791 / 14700) loss: 0.877459\n",
      "(Iteration 2792 / 14700) loss: 0.846971\n",
      "(Iteration 2793 / 14700) loss: 0.893680\n",
      "(Iteration 2794 / 14700) loss: 1.164648\n",
      "(Iteration 2795 / 14700) loss: 1.059309\n",
      "(Iteration 2796 / 14700) loss: 0.883279\n",
      "(Iteration 2797 / 14700) loss: 1.115449\n",
      "(Iteration 2798 / 14700) loss: 1.178621\n",
      "(Iteration 2799 / 14700) loss: 0.999877\n",
      "(Iteration 2800 / 14700) loss: 0.760158\n",
      "(Iteration 2801 / 14700) loss: 1.201113\n",
      "(Iteration 2802 / 14700) loss: 1.010916\n",
      "(Iteration 2803 / 14700) loss: 1.045964\n",
      "(Iteration 2804 / 14700) loss: 0.892841\n",
      "(Iteration 2805 / 14700) loss: 0.962423\n",
      "(Iteration 2806 / 14700) loss: 0.971701\n",
      "(Iteration 2807 / 14700) loss: 1.068483\n",
      "(Iteration 2808 / 14700) loss: 0.867077\n",
      "(Iteration 2809 / 14700) loss: 1.004736\n",
      "(Iteration 2810 / 14700) loss: 0.818173\n",
      "(Iteration 2811 / 14700) loss: 0.862582\n",
      "(Iteration 2812 / 14700) loss: 1.281007\n",
      "(Iteration 2813 / 14700) loss: 0.835603\n",
      "(Iteration 2814 / 14700) loss: 1.258902\n",
      "(Iteration 2815 / 14700) loss: 0.975951\n",
      "(Iteration 2816 / 14700) loss: 1.172128\n",
      "(Iteration 2817 / 14700) loss: 0.901915\n",
      "(Iteration 2818 / 14700) loss: 1.137052\n",
      "(Iteration 2819 / 14700) loss: 0.873857\n",
      "(Iteration 2820 / 14700) loss: 0.898986\n",
      "(Iteration 2821 / 14700) loss: 1.194806\n",
      "(Iteration 2822 / 14700) loss: 0.823697\n",
      "(Iteration 2823 / 14700) loss: 1.165262\n",
      "(Iteration 2824 / 14700) loss: 1.145221\n",
      "(Iteration 2825 / 14700) loss: 1.124163\n",
      "(Iteration 2826 / 14700) loss: 1.067671\n",
      "(Iteration 2827 / 14700) loss: 1.095597\n",
      "(Iteration 2828 / 14700) loss: 0.946510\n",
      "(Iteration 2829 / 14700) loss: 1.220634\n",
      "(Iteration 2830 / 14700) loss: 0.724227\n",
      "(Iteration 2831 / 14700) loss: 0.803099\n",
      "(Iteration 2832 / 14700) loss: 0.814364\n",
      "(Iteration 2833 / 14700) loss: 0.726627\n",
      "(Iteration 2834 / 14700) loss: 1.144802\n",
      "(Iteration 2835 / 14700) loss: 0.883221\n",
      "(Iteration 2836 / 14700) loss: 1.037742\n",
      "(Iteration 2837 / 14700) loss: 0.991565\n",
      "(Iteration 2838 / 14700) loss: 0.867008\n",
      "(Iteration 2839 / 14700) loss: 0.982882\n",
      "(Iteration 2840 / 14700) loss: 0.797928\n",
      "(Iteration 2841 / 14700) loss: 0.931000\n",
      "(Iteration 2842 / 14700) loss: 1.113459\n",
      "(Iteration 2843 / 14700) loss: 0.921571\n",
      "(Iteration 2844 / 14700) loss: 1.101910\n",
      "(Iteration 2845 / 14700) loss: 1.061774\n",
      "(Iteration 2846 / 14700) loss: 0.912175\n",
      "(Iteration 2847 / 14700) loss: 0.752046\n",
      "(Iteration 2848 / 14700) loss: 1.046641\n",
      "(Iteration 2849 / 14700) loss: 0.981137\n",
      "(Iteration 2850 / 14700) loss: 0.815280\n",
      "(Iteration 2851 / 14700) loss: 1.101823\n",
      "(Iteration 2852 / 14700) loss: 0.788012\n",
      "(Iteration 2853 / 14700) loss: 0.933342\n",
      "(Iteration 2854 / 14700) loss: 0.768207\n",
      "(Iteration 2855 / 14700) loss: 0.722936\n",
      "(Iteration 2856 / 14700) loss: 0.932014\n",
      "(Iteration 2857 / 14700) loss: 0.815082\n",
      "(Iteration 2858 / 14700) loss: 1.128140\n",
      "(Iteration 2859 / 14700) loss: 0.775886\n",
      "(Iteration 2860 / 14700) loss: 0.912640\n",
      "(Iteration 2861 / 14700) loss: 1.047341\n",
      "(Iteration 2862 / 14700) loss: 1.080521\n",
      "(Iteration 2863 / 14700) loss: 0.744243\n",
      "(Iteration 2864 / 14700) loss: 0.924783\n",
      "(Iteration 2865 / 14700) loss: 1.057098\n",
      "(Iteration 2866 / 14700) loss: 0.829699\n",
      "(Iteration 2867 / 14700) loss: 0.934962\n",
      "(Iteration 2868 / 14700) loss: 0.999288\n",
      "(Iteration 2869 / 14700) loss: 1.066356\n",
      "(Iteration 2870 / 14700) loss: 0.996535\n",
      "(Iteration 2871 / 14700) loss: 0.809634\n",
      "(Iteration 2872 / 14700) loss: 0.994611\n",
      "(Iteration 2873 / 14700) loss: 1.006718\n",
      "(Iteration 2874 / 14700) loss: 0.855368\n",
      "(Iteration 2875 / 14700) loss: 1.314201\n",
      "(Iteration 2876 / 14700) loss: 1.045792\n",
      "(Iteration 2877 / 14700) loss: 0.956484\n",
      "(Iteration 2878 / 14700) loss: 1.071086\n",
      "(Iteration 2879 / 14700) loss: 0.812383\n",
      "(Iteration 2880 / 14700) loss: 1.055473\n",
      "(Iteration 2881 / 14700) loss: 0.801310\n",
      "(Iteration 2882 / 14700) loss: 0.919868\n",
      "(Iteration 2883 / 14700) loss: 0.787987\n",
      "(Iteration 2884 / 14700) loss: 0.782167\n",
      "(Iteration 2885 / 14700) loss: 0.919911\n",
      "(Iteration 2886 / 14700) loss: 0.793288\n",
      "(Iteration 2887 / 14700) loss: 0.980202\n",
      "(Iteration 2888 / 14700) loss: 0.702112\n",
      "(Iteration 2889 / 14700) loss: 0.891465\n",
      "(Iteration 2890 / 14700) loss: 0.939634\n",
      "(Iteration 2891 / 14700) loss: 0.920754\n",
      "(Iteration 2892 / 14700) loss: 0.802748\n",
      "(Iteration 2893 / 14700) loss: 0.896360\n",
      "(Iteration 2894 / 14700) loss: 1.001835\n",
      "(Iteration 2895 / 14700) loss: 1.032316\n",
      "(Iteration 2896 / 14700) loss: 0.863512\n",
      "(Iteration 2897 / 14700) loss: 1.028177\n",
      "(Iteration 2898 / 14700) loss: 0.959773\n",
      "(Iteration 2899 / 14700) loss: 1.017153\n",
      "(Iteration 2900 / 14700) loss: 0.885225\n",
      "(Iteration 2901 / 14700) loss: 0.923702\n",
      "(Iteration 2902 / 14700) loss: 1.245143\n",
      "(Iteration 2903 / 14700) loss: 0.772690\n",
      "(Iteration 2904 / 14700) loss: 0.959711\n",
      "(Iteration 2905 / 14700) loss: 0.985466\n",
      "(Iteration 2906 / 14700) loss: 0.807713\n",
      "(Iteration 2907 / 14700) loss: 0.869329\n",
      "(Iteration 2908 / 14700) loss: 0.842948\n",
      "(Iteration 2909 / 14700) loss: 1.023577\n",
      "(Iteration 2910 / 14700) loss: 0.976097\n",
      "(Iteration 2911 / 14700) loss: 1.309544\n",
      "(Iteration 2912 / 14700) loss: 0.777595\n",
      "(Iteration 2913 / 14700) loss: 0.840716\n",
      "(Iteration 2914 / 14700) loss: 1.195966\n",
      "(Iteration 2915 / 14700) loss: 1.085962\n",
      "(Iteration 2916 / 14700) loss: 0.902258\n",
      "(Iteration 2917 / 14700) loss: 0.802748\n",
      "(Iteration 2918 / 14700) loss: 1.024248\n",
      "(Iteration 2919 / 14700) loss: 0.910931\n",
      "(Iteration 2920 / 14700) loss: 1.000555\n",
      "(Iteration 2921 / 14700) loss: 0.982313\n",
      "(Iteration 2922 / 14700) loss: 0.994389\n",
      "(Iteration 2923 / 14700) loss: 0.927309\n",
      "(Iteration 2924 / 14700) loss: 1.007237\n",
      "(Iteration 2925 / 14700) loss: 0.836940\n",
      "(Iteration 2926 / 14700) loss: 1.006843\n",
      "(Iteration 2927 / 14700) loss: 0.913438\n",
      "(Iteration 2928 / 14700) loss: 0.832895\n",
      "(Iteration 2929 / 14700) loss: 1.088320\n",
      "(Iteration 2930 / 14700) loss: 0.801783\n",
      "(Iteration 2931 / 14700) loss: 0.635302\n",
      "(Iteration 2932 / 14700) loss: 1.082041\n",
      "(Iteration 2933 / 14700) loss: 1.074197\n",
      "(Iteration 2934 / 14700) loss: 0.878785\n",
      "(Iteration 2935 / 14700) loss: 0.866791\n",
      "(Iteration 2936 / 14700) loss: 0.987676\n",
      "(Iteration 2937 / 14700) loss: 0.788752\n",
      "(Iteration 2938 / 14700) loss: 0.910370\n",
      "(Iteration 2939 / 14700) loss: 0.906060\n",
      "(Iteration 2940 / 14700) loss: 0.960762\n",
      "(Epoch 3 / 15) train acc: 0.733000; val_acc: 0.710000\n",
      "(Iteration 2941 / 14700) loss: 0.804035\n",
      "(Iteration 2942 / 14700) loss: 1.014431\n",
      "(Iteration 2943 / 14700) loss: 1.118244\n",
      "(Iteration 2944 / 14700) loss: 0.856765\n",
      "(Iteration 2945 / 14700) loss: 1.146389\n",
      "(Iteration 2946 / 14700) loss: 0.972622\n",
      "(Iteration 2947 / 14700) loss: 0.880861\n",
      "(Iteration 2948 / 14700) loss: 0.750951\n",
      "(Iteration 2949 / 14700) loss: 0.947238\n",
      "(Iteration 2950 / 14700) loss: 0.810837\n",
      "(Iteration 2951 / 14700) loss: 1.052471\n",
      "(Iteration 2952 / 14700) loss: 0.994886\n",
      "(Iteration 2953 / 14700) loss: 0.839603\n",
      "(Iteration 2954 / 14700) loss: 0.755809\n",
      "(Iteration 2955 / 14700) loss: 1.088379\n",
      "(Iteration 2956 / 14700) loss: 0.983755\n",
      "(Iteration 2957 / 14700) loss: 0.904013\n",
      "(Iteration 2958 / 14700) loss: 0.999383\n",
      "(Iteration 2959 / 14700) loss: 1.367193\n",
      "(Iteration 2960 / 14700) loss: 0.847655\n",
      "(Iteration 2961 / 14700) loss: 1.015782\n",
      "(Iteration 2962 / 14700) loss: 1.264418\n",
      "(Iteration 2963 / 14700) loss: 0.784121\n",
      "(Iteration 2964 / 14700) loss: 1.096458\n",
      "(Iteration 2965 / 14700) loss: 0.980350\n",
      "(Iteration 2966 / 14700) loss: 0.986443\n",
      "(Iteration 2967 / 14700) loss: 0.778681\n",
      "(Iteration 2968 / 14700) loss: 1.163744\n",
      "(Iteration 2969 / 14700) loss: 1.075370\n",
      "(Iteration 2970 / 14700) loss: 0.739498\n",
      "(Iteration 2971 / 14700) loss: 1.096088\n",
      "(Iteration 2972 / 14700) loss: 1.046055\n",
      "(Iteration 2973 / 14700) loss: 0.711694\n",
      "(Iteration 2974 / 14700) loss: 1.198238\n",
      "(Iteration 2975 / 14700) loss: 0.976908\n",
      "(Iteration 2976 / 14700) loss: 0.648994\n",
      "(Iteration 2977 / 14700) loss: 1.037864\n",
      "(Iteration 2978 / 14700) loss: 0.799720\n",
      "(Iteration 2979 / 14700) loss: 0.929678\n",
      "(Iteration 2980 / 14700) loss: 0.882527\n",
      "(Iteration 2981 / 14700) loss: 0.908880\n",
      "(Iteration 2982 / 14700) loss: 0.688849\n",
      "(Iteration 2983 / 14700) loss: 0.861915\n",
      "(Iteration 2984 / 14700) loss: 0.856098\n",
      "(Iteration 2985 / 14700) loss: 1.234105\n",
      "(Iteration 2986 / 14700) loss: 0.941261\n",
      "(Iteration 2987 / 14700) loss: 0.881561\n",
      "(Iteration 2988 / 14700) loss: 0.991054\n",
      "(Iteration 2989 / 14700) loss: 0.956922\n",
      "(Iteration 2990 / 14700) loss: 0.929288\n",
      "(Iteration 2991 / 14700) loss: 1.006534\n",
      "(Iteration 2992 / 14700) loss: 0.937894\n",
      "(Iteration 2993 / 14700) loss: 0.952222\n",
      "(Iteration 2994 / 14700) loss: 0.953758\n",
      "(Iteration 2995 / 14700) loss: 0.825614\n",
      "(Iteration 2996 / 14700) loss: 1.113109\n",
      "(Iteration 2997 / 14700) loss: 0.699970\n",
      "(Iteration 2998 / 14700) loss: 1.010281\n",
      "(Iteration 2999 / 14700) loss: 0.937531\n",
      "(Iteration 3000 / 14700) loss: 1.244750\n",
      "(Iteration 3001 / 14700) loss: 0.973372\n",
      "(Iteration 3002 / 14700) loss: 1.317088\n",
      "(Iteration 3003 / 14700) loss: 0.961352\n",
      "(Iteration 3004 / 14700) loss: 1.117896\n",
      "(Iteration 3005 / 14700) loss: 1.000116\n",
      "(Iteration 3006 / 14700) loss: 1.047159\n",
      "(Iteration 3007 / 14700) loss: 0.902048\n",
      "(Iteration 3008 / 14700) loss: 0.850157\n",
      "(Iteration 3009 / 14700) loss: 1.251378\n",
      "(Iteration 3010 / 14700) loss: 1.166061\n",
      "(Iteration 3011 / 14700) loss: 0.945786\n",
      "(Iteration 3012 / 14700) loss: 0.790238\n",
      "(Iteration 3013 / 14700) loss: 0.985196\n",
      "(Iteration 3014 / 14700) loss: 0.897004\n",
      "(Iteration 3015 / 14700) loss: 0.774189\n",
      "(Iteration 3016 / 14700) loss: 1.035637\n",
      "(Iteration 3017 / 14700) loss: 0.912855\n",
      "(Iteration 3018 / 14700) loss: 0.734271\n",
      "(Iteration 3019 / 14700) loss: 0.803844\n",
      "(Iteration 3020 / 14700) loss: 0.791719\n",
      "(Iteration 3021 / 14700) loss: 0.952842\n",
      "(Iteration 3022 / 14700) loss: 0.943091\n",
      "(Iteration 3023 / 14700) loss: 1.043874\n",
      "(Iteration 3024 / 14700) loss: 0.933259\n",
      "(Iteration 3025 / 14700) loss: 1.063245\n",
      "(Iteration 3026 / 14700) loss: 1.130939\n",
      "(Iteration 3027 / 14700) loss: 1.143420\n",
      "(Iteration 3028 / 14700) loss: 0.807597\n",
      "(Iteration 3029 / 14700) loss: 0.832910\n",
      "(Iteration 3030 / 14700) loss: 1.032562\n",
      "(Iteration 3031 / 14700) loss: 1.184407\n",
      "(Iteration 3032 / 14700) loss: 0.814207\n",
      "(Iteration 3033 / 14700) loss: 0.885454\n",
      "(Iteration 3034 / 14700) loss: 0.972282\n",
      "(Iteration 3035 / 14700) loss: 0.965275\n",
      "(Iteration 3036 / 14700) loss: 0.915392\n",
      "(Iteration 3037 / 14700) loss: 0.752341\n",
      "(Iteration 3038 / 14700) loss: 0.999393\n",
      "(Iteration 3039 / 14700) loss: 1.159661\n",
      "(Iteration 3040 / 14700) loss: 0.775974\n",
      "(Iteration 3041 / 14700) loss: 1.006638\n",
      "(Iteration 3042 / 14700) loss: 0.710361\n",
      "(Iteration 3043 / 14700) loss: 0.756036\n",
      "(Iteration 3044 / 14700) loss: 0.958136\n",
      "(Iteration 3045 / 14700) loss: 1.109039\n",
      "(Iteration 3046 / 14700) loss: 0.926601\n",
      "(Iteration 3047 / 14700) loss: 1.185689\n",
      "(Iteration 3048 / 14700) loss: 1.216987\n",
      "(Iteration 3049 / 14700) loss: 1.208080\n",
      "(Iteration 3050 / 14700) loss: 0.796981\n",
      "(Iteration 3051 / 14700) loss: 0.898536\n",
      "(Iteration 3052 / 14700) loss: 0.984785\n",
      "(Iteration 3053 / 14700) loss: 0.998513\n",
      "(Iteration 3054 / 14700) loss: 0.965198\n",
      "(Iteration 3055 / 14700) loss: 0.890347\n",
      "(Iteration 3056 / 14700) loss: 0.788481\n",
      "(Iteration 3057 / 14700) loss: 0.805912\n",
      "(Iteration 3058 / 14700) loss: 1.173091\n",
      "(Iteration 3059 / 14700) loss: 0.931112\n",
      "(Iteration 3060 / 14700) loss: 0.914057\n",
      "(Iteration 3061 / 14700) loss: 0.732673\n",
      "(Iteration 3062 / 14700) loss: 0.772705\n",
      "(Iteration 3063 / 14700) loss: 0.816445\n",
      "(Iteration 3064 / 14700) loss: 0.913896\n",
      "(Iteration 3065 / 14700) loss: 0.882194\n",
      "(Iteration 3066 / 14700) loss: 1.225704\n",
      "(Iteration 3067 / 14700) loss: 1.017622\n",
      "(Iteration 3068 / 14700) loss: 1.030269\n",
      "(Iteration 3069 / 14700) loss: 1.191728\n",
      "(Iteration 3070 / 14700) loss: 0.915811\n",
      "(Iteration 3071 / 14700) loss: 0.810463\n",
      "(Iteration 3072 / 14700) loss: 0.936978\n",
      "(Iteration 3073 / 14700) loss: 0.903287\n",
      "(Iteration 3074 / 14700) loss: 0.817664\n",
      "(Iteration 3075 / 14700) loss: 0.976480\n",
      "(Iteration 3076 / 14700) loss: 0.726557\n",
      "(Iteration 3077 / 14700) loss: 0.959878\n",
      "(Iteration 3078 / 14700) loss: 0.877893\n",
      "(Iteration 3079 / 14700) loss: 0.831142\n",
      "(Iteration 3080 / 14700) loss: 0.783916\n",
      "(Iteration 3081 / 14700) loss: 1.017328\n",
      "(Iteration 3082 / 14700) loss: 0.954233\n",
      "(Iteration 3083 / 14700) loss: 0.948402\n",
      "(Iteration 3084 / 14700) loss: 0.968401\n",
      "(Iteration 3085 / 14700) loss: 1.019274\n",
      "(Iteration 3086 / 14700) loss: 0.829493\n",
      "(Iteration 3087 / 14700) loss: 0.809284\n",
      "(Iteration 3088 / 14700) loss: 0.801046\n",
      "(Iteration 3089 / 14700) loss: 1.006616\n",
      "(Iteration 3090 / 14700) loss: 0.875554\n",
      "(Iteration 3091 / 14700) loss: 0.999666\n",
      "(Iteration 3092 / 14700) loss: 0.871656\n",
      "(Iteration 3093 / 14700) loss: 1.053806\n",
      "(Iteration 3094 / 14700) loss: 1.008848\n",
      "(Iteration 3095 / 14700) loss: 0.926575\n",
      "(Iteration 3096 / 14700) loss: 0.817778\n",
      "(Iteration 3097 / 14700) loss: 1.071993\n",
      "(Iteration 3098 / 14700) loss: 0.980227\n",
      "(Iteration 3099 / 14700) loss: 0.827928\n",
      "(Iteration 3100 / 14700) loss: 0.748756\n",
      "(Iteration 3101 / 14700) loss: 1.003570\n",
      "(Iteration 3102 / 14700) loss: 1.029360\n",
      "(Iteration 3103 / 14700) loss: 0.904808\n",
      "(Iteration 3104 / 14700) loss: 0.646243\n",
      "(Iteration 3105 / 14700) loss: 0.834337\n",
      "(Iteration 3106 / 14700) loss: 0.699939\n",
      "(Iteration 3107 / 14700) loss: 0.905051\n",
      "(Iteration 3108 / 14700) loss: 1.234725\n",
      "(Iteration 3109 / 14700) loss: 1.000224\n",
      "(Iteration 3110 / 14700) loss: 0.912644\n",
      "(Iteration 3111 / 14700) loss: 0.874387\n",
      "(Iteration 3112 / 14700) loss: 0.951948\n",
      "(Iteration 3113 / 14700) loss: 0.925767\n",
      "(Iteration 3114 / 14700) loss: 0.755661\n",
      "(Iteration 3115 / 14700) loss: 0.831300\n",
      "(Iteration 3116 / 14700) loss: 0.993587\n",
      "(Iteration 3117 / 14700) loss: 1.346115\n",
      "(Iteration 3118 / 14700) loss: 1.221367\n",
      "(Iteration 3119 / 14700) loss: 1.352559\n",
      "(Iteration 3120 / 14700) loss: 1.164738\n",
      "(Iteration 3121 / 14700) loss: 1.269519\n",
      "(Iteration 3122 / 14700) loss: 1.003339\n",
      "(Iteration 3123 / 14700) loss: 0.826680\n",
      "(Iteration 3124 / 14700) loss: 0.959881\n",
      "(Iteration 3125 / 14700) loss: 0.808617\n",
      "(Iteration 3126 / 14700) loss: 1.019813\n",
      "(Iteration 3127 / 14700) loss: 0.989516\n",
      "(Iteration 3128 / 14700) loss: 0.981275\n",
      "(Iteration 3129 / 14700) loss: 1.135009\n",
      "(Iteration 3130 / 14700) loss: 0.878676\n",
      "(Iteration 3131 / 14700) loss: 1.090215\n",
      "(Iteration 3132 / 14700) loss: 0.878232\n",
      "(Iteration 3133 / 14700) loss: 0.909245\n",
      "(Iteration 3134 / 14700) loss: 1.000098\n",
      "(Iteration 3135 / 14700) loss: 1.004218\n",
      "(Iteration 3136 / 14700) loss: 0.718520\n",
      "(Iteration 3137 / 14700) loss: 1.059101\n",
      "(Iteration 3138 / 14700) loss: 1.027204\n",
      "(Iteration 3139 / 14700) loss: 0.950658\n",
      "(Iteration 3140 / 14700) loss: 1.326184\n",
      "(Iteration 3141 / 14700) loss: 1.060929\n",
      "(Iteration 3142 / 14700) loss: 0.933482\n",
      "(Iteration 3143 / 14700) loss: 1.044438\n",
      "(Iteration 3144 / 14700) loss: 0.780279\n",
      "(Iteration 3145 / 14700) loss: 1.041950\n",
      "(Iteration 3146 / 14700) loss: 0.928373\n",
      "(Iteration 3147 / 14700) loss: 0.846533\n",
      "(Iteration 3148 / 14700) loss: 1.216654\n",
      "(Iteration 3149 / 14700) loss: 0.899322\n",
      "(Iteration 3150 / 14700) loss: 0.803445\n",
      "(Iteration 3151 / 14700) loss: 1.395162\n",
      "(Iteration 3152 / 14700) loss: 1.277079\n",
      "(Iteration 3153 / 14700) loss: 0.937766\n",
      "(Iteration 3154 / 14700) loss: 0.981208\n",
      "(Iteration 3155 / 14700) loss: 0.828641\n",
      "(Iteration 3156 / 14700) loss: 0.740210\n",
      "(Iteration 3157 / 14700) loss: 1.024506\n",
      "(Iteration 3158 / 14700) loss: 1.097652\n",
      "(Iteration 3159 / 14700) loss: 0.896820\n",
      "(Iteration 3160 / 14700) loss: 0.848849\n",
      "(Iteration 3161 / 14700) loss: 1.059383\n",
      "(Iteration 3162 / 14700) loss: 1.084733\n",
      "(Iteration 3163 / 14700) loss: 1.147029\n",
      "(Iteration 3164 / 14700) loss: 1.068538\n",
      "(Iteration 3165 / 14700) loss: 0.973856\n",
      "(Iteration 3166 / 14700) loss: 1.101471\n",
      "(Iteration 3167 / 14700) loss: 0.871167\n",
      "(Iteration 3168 / 14700) loss: 0.821378\n",
      "(Iteration 3169 / 14700) loss: 0.871845\n",
      "(Iteration 3170 / 14700) loss: 0.788357\n",
      "(Iteration 3171 / 14700) loss: 0.965737\n",
      "(Iteration 3172 / 14700) loss: 0.701198\n",
      "(Iteration 3173 / 14700) loss: 1.023904\n",
      "(Iteration 3174 / 14700) loss: 0.932464\n",
      "(Iteration 3175 / 14700) loss: 1.011915\n",
      "(Iteration 3176 / 14700) loss: 0.991556\n",
      "(Iteration 3177 / 14700) loss: 0.945568\n",
      "(Iteration 3178 / 14700) loss: 0.959693\n",
      "(Iteration 3179 / 14700) loss: 0.917730\n",
      "(Iteration 3180 / 14700) loss: 1.034777\n",
      "(Iteration 3181 / 14700) loss: 0.905507\n",
      "(Iteration 3182 / 14700) loss: 0.742550\n",
      "(Iteration 3183 / 14700) loss: 0.811134\n",
      "(Iteration 3184 / 14700) loss: 1.350167\n",
      "(Iteration 3185 / 14700) loss: 1.150482\n",
      "(Iteration 3186 / 14700) loss: 1.121534\n",
      "(Iteration 3187 / 14700) loss: 0.773378\n",
      "(Iteration 3188 / 14700) loss: 0.998817\n",
      "(Iteration 3189 / 14700) loss: 0.836613\n",
      "(Iteration 3190 / 14700) loss: 0.764542\n",
      "(Iteration 3191 / 14700) loss: 0.788239\n",
      "(Iteration 3192 / 14700) loss: 0.908219\n",
      "(Iteration 3193 / 14700) loss: 0.925065\n",
      "(Iteration 3194 / 14700) loss: 0.836660\n",
      "(Iteration 3195 / 14700) loss: 0.904951\n",
      "(Iteration 3196 / 14700) loss: 0.866036\n",
      "(Iteration 3197 / 14700) loss: 0.717192\n",
      "(Iteration 3198 / 14700) loss: 0.919291\n",
      "(Iteration 3199 / 14700) loss: 0.945672\n",
      "(Iteration 3200 / 14700) loss: 0.842388\n",
      "(Iteration 3201 / 14700) loss: 1.132209\n",
      "(Iteration 3202 / 14700) loss: 0.634186\n",
      "(Iteration 3203 / 14700) loss: 0.779315\n",
      "(Iteration 3204 / 14700) loss: 1.054568\n",
      "(Iteration 3205 / 14700) loss: 0.891158\n",
      "(Iteration 3206 / 14700) loss: 1.174508\n",
      "(Iteration 3207 / 14700) loss: 1.081910\n",
      "(Iteration 3208 / 14700) loss: 1.050315\n",
      "(Iteration 3209 / 14700) loss: 1.114474\n",
      "(Iteration 3210 / 14700) loss: 0.828597\n",
      "(Iteration 3211 / 14700) loss: 1.114944\n",
      "(Iteration 3212 / 14700) loss: 1.043820\n",
      "(Iteration 3213 / 14700) loss: 1.097997\n",
      "(Iteration 3214 / 14700) loss: 0.951154\n",
      "(Iteration 3215 / 14700) loss: 0.802196\n",
      "(Iteration 3216 / 14700) loss: 0.881983\n",
      "(Iteration 3217 / 14700) loss: 0.962719\n",
      "(Iteration 3218 / 14700) loss: 1.100036\n",
      "(Iteration 3219 / 14700) loss: 0.833071\n",
      "(Iteration 3220 / 14700) loss: 0.971768\n",
      "(Iteration 3221 / 14700) loss: 0.836933\n",
      "(Iteration 3222 / 14700) loss: 0.999219\n",
      "(Iteration 3223 / 14700) loss: 1.188890\n",
      "(Iteration 3224 / 14700) loss: 0.651348\n",
      "(Iteration 3225 / 14700) loss: 1.034698\n",
      "(Iteration 3226 / 14700) loss: 1.039352\n",
      "(Iteration 3227 / 14700) loss: 1.017438\n",
      "(Iteration 3228 / 14700) loss: 0.874019\n",
      "(Iteration 3229 / 14700) loss: 1.060257\n",
      "(Iteration 3230 / 14700) loss: 0.845801\n",
      "(Iteration 3231 / 14700) loss: 1.301798\n",
      "(Iteration 3232 / 14700) loss: 0.910072\n",
      "(Iteration 3233 / 14700) loss: 1.074119\n",
      "(Iteration 3234 / 14700) loss: 0.989649\n",
      "(Iteration 3235 / 14700) loss: 1.034163\n",
      "(Iteration 3236 / 14700) loss: 0.848607\n",
      "(Iteration 3237 / 14700) loss: 0.994698\n",
      "(Iteration 3238 / 14700) loss: 0.919913\n",
      "(Iteration 3239 / 14700) loss: 0.935620\n",
      "(Iteration 3240 / 14700) loss: 1.002341\n",
      "(Iteration 3241 / 14700) loss: 0.837848\n",
      "(Iteration 3242 / 14700) loss: 0.996266\n",
      "(Iteration 3243 / 14700) loss: 1.104007\n",
      "(Iteration 3244 / 14700) loss: 0.764276\n",
      "(Iteration 3245 / 14700) loss: 1.001532\n",
      "(Iteration 3246 / 14700) loss: 0.608242\n",
      "(Iteration 3247 / 14700) loss: 0.888827\n",
      "(Iteration 3248 / 14700) loss: 0.851010\n",
      "(Iteration 3249 / 14700) loss: 0.948050\n",
      "(Iteration 3250 / 14700) loss: 1.084687\n",
      "(Iteration 3251 / 14700) loss: 0.999081\n",
      "(Iteration 3252 / 14700) loss: 0.911394\n",
      "(Iteration 3253 / 14700) loss: 0.978769\n",
      "(Iteration 3254 / 14700) loss: 1.282632\n",
      "(Iteration 3255 / 14700) loss: 0.974484\n",
      "(Iteration 3256 / 14700) loss: 0.780003\n",
      "(Iteration 3257 / 14700) loss: 0.856086\n",
      "(Iteration 3258 / 14700) loss: 1.047245\n",
      "(Iteration 3259 / 14700) loss: 0.834983\n",
      "(Iteration 3260 / 14700) loss: 1.250597\n",
      "(Iteration 3261 / 14700) loss: 1.208860\n",
      "(Iteration 3262 / 14700) loss: 0.939097\n",
      "(Iteration 3263 / 14700) loss: 0.802453\n",
      "(Iteration 3264 / 14700) loss: 1.071716\n",
      "(Iteration 3265 / 14700) loss: 0.678565\n",
      "(Iteration 3266 / 14700) loss: 0.714738\n",
      "(Iteration 3267 / 14700) loss: 0.917410\n",
      "(Iteration 3268 / 14700) loss: 1.054857\n",
      "(Iteration 3269 / 14700) loss: 0.927125\n",
      "(Iteration 3270 / 14700) loss: 0.869952\n",
      "(Iteration 3271 / 14700) loss: 0.748936\n",
      "(Iteration 3272 / 14700) loss: 1.058107\n",
      "(Iteration 3273 / 14700) loss: 0.737741\n",
      "(Iteration 3274 / 14700) loss: 1.073635\n",
      "(Iteration 3275 / 14700) loss: 0.912611\n",
      "(Iteration 3276 / 14700) loss: 1.038024\n",
      "(Iteration 3277 / 14700) loss: 1.181503\n",
      "(Iteration 3278 / 14700) loss: 0.796673\n",
      "(Iteration 3279 / 14700) loss: 0.786516\n",
      "(Iteration 3280 / 14700) loss: 0.836182\n",
      "(Iteration 3281 / 14700) loss: 0.986402\n",
      "(Iteration 3282 / 14700) loss: 0.738074\n",
      "(Iteration 3283 / 14700) loss: 0.997831\n",
      "(Iteration 3284 / 14700) loss: 0.881202\n",
      "(Iteration 3285 / 14700) loss: 0.914388\n",
      "(Iteration 3286 / 14700) loss: 1.009969\n",
      "(Iteration 3287 / 14700) loss: 0.912125\n",
      "(Iteration 3288 / 14700) loss: 0.956818\n",
      "(Iteration 3289 / 14700) loss: 0.886707\n",
      "(Iteration 3290 / 14700) loss: 0.983946\n",
      "(Iteration 3291 / 14700) loss: 0.780812\n",
      "(Iteration 3292 / 14700) loss: 0.985154\n",
      "(Iteration 3293 / 14700) loss: 0.862558\n",
      "(Iteration 3294 / 14700) loss: 1.127029\n",
      "(Iteration 3295 / 14700) loss: 0.958110\n",
      "(Iteration 3296 / 14700) loss: 0.949498\n",
      "(Iteration 3297 / 14700) loss: 0.797579\n",
      "(Iteration 3298 / 14700) loss: 0.874316\n",
      "(Iteration 3299 / 14700) loss: 0.973741\n",
      "(Iteration 3300 / 14700) loss: 0.966581\n",
      "(Iteration 3301 / 14700) loss: 0.987597\n",
      "(Iteration 3302 / 14700) loss: 0.917086\n",
      "(Iteration 3303 / 14700) loss: 0.974267\n",
      "(Iteration 3304 / 14700) loss: 0.869502\n",
      "(Iteration 3305 / 14700) loss: 0.996666\n",
      "(Iteration 3306 / 14700) loss: 1.018593\n",
      "(Iteration 3307 / 14700) loss: 0.901109\n",
      "(Iteration 3308 / 14700) loss: 0.936652\n",
      "(Iteration 3309 / 14700) loss: 0.769644\n",
      "(Iteration 3310 / 14700) loss: 0.935973\n",
      "(Iteration 3311 / 14700) loss: 0.860873\n",
      "(Iteration 3312 / 14700) loss: 0.982997\n",
      "(Iteration 3313 / 14700) loss: 0.775793\n",
      "(Iteration 3314 / 14700) loss: 0.849456\n",
      "(Iteration 3315 / 14700) loss: 0.927467\n",
      "(Iteration 3316 / 14700) loss: 0.982831\n",
      "(Iteration 3317 / 14700) loss: 0.804920\n",
      "(Iteration 3318 / 14700) loss: 1.023219\n",
      "(Iteration 3319 / 14700) loss: 1.221833\n",
      "(Iteration 3320 / 14700) loss: 0.879474\n",
      "(Iteration 3321 / 14700) loss: 0.771020\n",
      "(Iteration 3322 / 14700) loss: 1.001010\n",
      "(Iteration 3323 / 14700) loss: 1.233314\n",
      "(Iteration 3324 / 14700) loss: 0.696247\n",
      "(Iteration 3325 / 14700) loss: 1.051438\n",
      "(Iteration 3326 / 14700) loss: 1.122936\n",
      "(Iteration 3327 / 14700) loss: 0.898922\n",
      "(Iteration 3328 / 14700) loss: 0.659662\n",
      "(Iteration 3329 / 14700) loss: 1.033918\n",
      "(Iteration 3330 / 14700) loss: 0.741080\n",
      "(Iteration 3331 / 14700) loss: 0.786546\n",
      "(Iteration 3332 / 14700) loss: 1.274463\n",
      "(Iteration 3333 / 14700) loss: 0.858433\n",
      "(Iteration 3334 / 14700) loss: 0.930799\n",
      "(Iteration 3335 / 14700) loss: 0.893150\n",
      "(Iteration 3336 / 14700) loss: 0.778523\n",
      "(Iteration 3337 / 14700) loss: 0.850482\n",
      "(Iteration 3338 / 14700) loss: 0.979424\n",
      "(Iteration 3339 / 14700) loss: 0.706187\n",
      "(Iteration 3340 / 14700) loss: 0.850431\n",
      "(Iteration 3341 / 14700) loss: 0.853863\n",
      "(Iteration 3342 / 14700) loss: 0.874458\n",
      "(Iteration 3343 / 14700) loss: 0.778406\n",
      "(Iteration 3344 / 14700) loss: 0.930797\n",
      "(Iteration 3345 / 14700) loss: 1.039385\n",
      "(Iteration 3346 / 14700) loss: 1.044323\n",
      "(Iteration 3347 / 14700) loss: 0.785464\n",
      "(Iteration 3348 / 14700) loss: 0.976636\n",
      "(Iteration 3349 / 14700) loss: 0.828948\n",
      "(Iteration 3350 / 14700) loss: 1.016404\n",
      "(Iteration 3351 / 14700) loss: 0.978844\n",
      "(Iteration 3352 / 14700) loss: 0.798789\n",
      "(Iteration 3353 / 14700) loss: 0.819719\n",
      "(Iteration 3354 / 14700) loss: 0.769383\n",
      "(Iteration 3355 / 14700) loss: 0.946669\n",
      "(Iteration 3356 / 14700) loss: 0.835747\n",
      "(Iteration 3357 / 14700) loss: 0.873413\n",
      "(Iteration 3358 / 14700) loss: 0.944755\n",
      "(Iteration 3359 / 14700) loss: 0.684761\n",
      "(Iteration 3360 / 14700) loss: 0.991028\n",
      "(Iteration 3361 / 14700) loss: 0.851415\n",
      "(Iteration 3362 / 14700) loss: 0.739153\n",
      "(Iteration 3363 / 14700) loss: 0.910946\n",
      "(Iteration 3364 / 14700) loss: 1.037782\n",
      "(Iteration 3365 / 14700) loss: 1.242715\n",
      "(Iteration 3366 / 14700) loss: 1.273568\n",
      "(Iteration 3367 / 14700) loss: 0.806818\n",
      "(Iteration 3368 / 14700) loss: 1.246224\n",
      "(Iteration 3369 / 14700) loss: 0.848497\n",
      "(Iteration 3370 / 14700) loss: 0.965190\n",
      "(Iteration 3371 / 14700) loss: 0.983268\n",
      "(Iteration 3372 / 14700) loss: 0.676925\n",
      "(Iteration 3373 / 14700) loss: 1.156467\n",
      "(Iteration 3374 / 14700) loss: 0.780700\n",
      "(Iteration 3375 / 14700) loss: 0.849830\n",
      "(Iteration 3376 / 14700) loss: 0.831167\n",
      "(Iteration 3377 / 14700) loss: 0.945028\n",
      "(Iteration 3378 / 14700) loss: 0.922603\n",
      "(Iteration 3379 / 14700) loss: 0.878059\n",
      "(Iteration 3380 / 14700) loss: 1.055623\n",
      "(Iteration 3381 / 14700) loss: 0.892416\n",
      "(Iteration 3382 / 14700) loss: 0.771339\n",
      "(Iteration 3383 / 14700) loss: 0.803967\n",
      "(Iteration 3384 / 14700) loss: 1.165652\n",
      "(Iteration 3385 / 14700) loss: 0.815497\n",
      "(Iteration 3386 / 14700) loss: 1.033879\n",
      "(Iteration 3387 / 14700) loss: 0.725185\n",
      "(Iteration 3388 / 14700) loss: 1.007957\n",
      "(Iteration 3389 / 14700) loss: 0.896078\n",
      "(Iteration 3390 / 14700) loss: 0.856473\n",
      "(Iteration 3391 / 14700) loss: 0.713197\n",
      "(Iteration 3392 / 14700) loss: 0.726723\n",
      "(Iteration 3393 / 14700) loss: 0.805322\n",
      "(Iteration 3394 / 14700) loss: 0.860293\n",
      "(Iteration 3395 / 14700) loss: 0.942286\n",
      "(Iteration 3396 / 14700) loss: 0.670687\n",
      "(Iteration 3397 / 14700) loss: 0.775793\n",
      "(Iteration 3398 / 14700) loss: 0.793276\n",
      "(Iteration 3399 / 14700) loss: 0.709266\n",
      "(Iteration 3400 / 14700) loss: 0.942396\n",
      "(Iteration 3401 / 14700) loss: 1.191513\n",
      "(Iteration 3402 / 14700) loss: 0.787768\n",
      "(Iteration 3403 / 14700) loss: 1.097881\n",
      "(Iteration 3404 / 14700) loss: 0.806919\n",
      "(Iteration 3405 / 14700) loss: 0.985295\n",
      "(Iteration 3406 / 14700) loss: 0.601454\n",
      "(Iteration 3407 / 14700) loss: 0.910702\n",
      "(Iteration 3408 / 14700) loss: 0.994610\n",
      "(Iteration 3409 / 14700) loss: 1.000115\n",
      "(Iteration 3410 / 14700) loss: 0.934948\n",
      "(Iteration 3411 / 14700) loss: 0.788570\n",
      "(Iteration 3412 / 14700) loss: 1.019318\n",
      "(Iteration 3413 / 14700) loss: 1.025186\n",
      "(Iteration 3414 / 14700) loss: 0.921958\n",
      "(Iteration 3415 / 14700) loss: 0.822140\n",
      "(Iteration 3416 / 14700) loss: 0.767602\n",
      "(Iteration 3417 / 14700) loss: 0.856286\n",
      "(Iteration 3418 / 14700) loss: 0.585678\n",
      "(Iteration 3419 / 14700) loss: 0.777872\n",
      "(Iteration 3420 / 14700) loss: 1.023856\n",
      "(Iteration 3421 / 14700) loss: 1.054316\n",
      "(Iteration 3422 / 14700) loss: 1.147583\n",
      "(Iteration 3423 / 14700) loss: 0.883425\n",
      "(Iteration 3424 / 14700) loss: 0.877888\n",
      "(Iteration 3425 / 14700) loss: 0.993630\n",
      "(Iteration 3426 / 14700) loss: 0.644451\n",
      "(Iteration 3427 / 14700) loss: 0.781451\n",
      "(Iteration 3428 / 14700) loss: 1.145462\n",
      "(Iteration 3429 / 14700) loss: 1.204713\n",
      "(Iteration 3430 / 14700) loss: 1.144561\n",
      "(Iteration 3431 / 14700) loss: 0.760482\n",
      "(Iteration 3432 / 14700) loss: 1.078240\n",
      "(Iteration 3433 / 14700) loss: 0.765651\n",
      "(Iteration 3434 / 14700) loss: 0.863803\n",
      "(Iteration 3435 / 14700) loss: 0.853766\n",
      "(Iteration 3436 / 14700) loss: 1.008438\n",
      "(Iteration 3437 / 14700) loss: 0.959765\n",
      "(Iteration 3438 / 14700) loss: 0.929535\n",
      "(Iteration 3439 / 14700) loss: 1.039837\n",
      "(Iteration 3440 / 14700) loss: 0.938627\n",
      "(Iteration 3441 / 14700) loss: 0.874944\n",
      "(Iteration 3442 / 14700) loss: 1.121783\n",
      "(Iteration 3443 / 14700) loss: 1.068226\n",
      "(Iteration 3444 / 14700) loss: 0.998175\n",
      "(Iteration 3445 / 14700) loss: 0.891940\n",
      "(Iteration 3446 / 14700) loss: 1.020825\n",
      "(Iteration 3447 / 14700) loss: 1.138492\n",
      "(Iteration 3448 / 14700) loss: 0.858819\n",
      "(Iteration 3449 / 14700) loss: 0.945327\n",
      "(Iteration 3450 / 14700) loss: 0.777598\n",
      "(Iteration 3451 / 14700) loss: 1.211500\n",
      "(Iteration 3452 / 14700) loss: 0.947264\n",
      "(Iteration 3453 / 14700) loss: 0.832979\n",
      "(Iteration 3454 / 14700) loss: 0.768550\n",
      "(Iteration 3455 / 14700) loss: 0.888464\n",
      "(Iteration 3456 / 14700) loss: 1.155198\n",
      "(Iteration 3457 / 14700) loss: 1.007195\n",
      "(Iteration 3458 / 14700) loss: 0.869588\n",
      "(Iteration 3459 / 14700) loss: 0.982482\n",
      "(Iteration 3460 / 14700) loss: 0.838855\n",
      "(Iteration 3461 / 14700) loss: 0.853001\n",
      "(Iteration 3462 / 14700) loss: 1.016230\n",
      "(Iteration 3463 / 14700) loss: 0.926775\n",
      "(Iteration 3464 / 14700) loss: 1.061675\n",
      "(Iteration 3465 / 14700) loss: 0.919833\n",
      "(Iteration 3466 / 14700) loss: 0.883118\n",
      "(Iteration 3467 / 14700) loss: 1.147857\n",
      "(Iteration 3468 / 14700) loss: 1.054615\n",
      "(Iteration 3469 / 14700) loss: 1.126012\n",
      "(Iteration 3470 / 14700) loss: 0.657264\n",
      "(Iteration 3471 / 14700) loss: 1.222911\n",
      "(Iteration 3472 / 14700) loss: 0.954831\n",
      "(Iteration 3473 / 14700) loss: 1.143178\n",
      "(Iteration 3474 / 14700) loss: 0.809228\n",
      "(Iteration 3475 / 14700) loss: 1.279423\n",
      "(Iteration 3476 / 14700) loss: 0.951133\n",
      "(Iteration 3477 / 14700) loss: 0.727943\n",
      "(Iteration 3478 / 14700) loss: 0.870120\n",
      "(Iteration 3479 / 14700) loss: 0.984287\n",
      "(Iteration 3480 / 14700) loss: 0.996649\n",
      "(Iteration 3481 / 14700) loss: 0.866778\n",
      "(Iteration 3482 / 14700) loss: 1.037265\n",
      "(Iteration 3483 / 14700) loss: 0.945112\n",
      "(Iteration 3484 / 14700) loss: 0.750476\n",
      "(Iteration 3485 / 14700) loss: 0.976221\n",
      "(Iteration 3486 / 14700) loss: 0.714074\n",
      "(Iteration 3487 / 14700) loss: 0.930843\n",
      "(Iteration 3488 / 14700) loss: 0.762693\n",
      "(Iteration 3489 / 14700) loss: 0.996643\n",
      "(Iteration 3490 / 14700) loss: 1.138754\n",
      "(Iteration 3491 / 14700) loss: 0.759053\n",
      "(Iteration 3492 / 14700) loss: 1.014669\n",
      "(Iteration 3493 / 14700) loss: 0.885917\n",
      "(Iteration 3494 / 14700) loss: 0.929973\n",
      "(Iteration 3495 / 14700) loss: 0.911821\n",
      "(Iteration 3496 / 14700) loss: 0.903083\n",
      "(Iteration 3497 / 14700) loss: 1.061711\n",
      "(Iteration 3498 / 14700) loss: 0.965536\n",
      "(Iteration 3499 / 14700) loss: 0.845104\n",
      "(Iteration 3500 / 14700) loss: 1.066391\n",
      "(Iteration 3501 / 14700) loss: 1.175010\n",
      "(Iteration 3502 / 14700) loss: 0.805291\n",
      "(Iteration 3503 / 14700) loss: 0.948528\n",
      "(Iteration 3504 / 14700) loss: 1.080502\n",
      "(Iteration 3505 / 14700) loss: 1.005993\n",
      "(Iteration 3506 / 14700) loss: 0.999575\n",
      "(Iteration 3507 / 14700) loss: 0.865350\n",
      "(Iteration 3508 / 14700) loss: 0.939494\n",
      "(Iteration 3509 / 14700) loss: 0.774721\n",
      "(Iteration 3510 / 14700) loss: 0.866734\n",
      "(Iteration 3511 / 14700) loss: 0.854040\n",
      "(Iteration 3512 / 14700) loss: 0.955908\n",
      "(Iteration 3513 / 14700) loss: 0.818729\n",
      "(Iteration 3514 / 14700) loss: 0.962462\n",
      "(Iteration 3515 / 14700) loss: 1.040360\n",
      "(Iteration 3516 / 14700) loss: 0.894306\n",
      "(Iteration 3517 / 14700) loss: 1.048504\n",
      "(Iteration 3518 / 14700) loss: 0.717722\n",
      "(Iteration 3519 / 14700) loss: 0.811296\n",
      "(Iteration 3520 / 14700) loss: 0.839201\n",
      "(Iteration 3521 / 14700) loss: 0.957401\n",
      "(Iteration 3522 / 14700) loss: 0.988906\n",
      "(Iteration 3523 / 14700) loss: 0.935575\n",
      "(Iteration 3524 / 14700) loss: 0.875083\n",
      "(Iteration 3525 / 14700) loss: 1.176448\n",
      "(Iteration 3526 / 14700) loss: 0.777089\n",
      "(Iteration 3527 / 14700) loss: 0.830731\n",
      "(Iteration 3528 / 14700) loss: 0.956271\n",
      "(Iteration 3529 / 14700) loss: 0.990084\n",
      "(Iteration 3530 / 14700) loss: 0.901006\n",
      "(Iteration 3531 / 14700) loss: 1.008452\n",
      "(Iteration 3532 / 14700) loss: 1.206130\n",
      "(Iteration 3533 / 14700) loss: 0.936836\n",
      "(Iteration 3534 / 14700) loss: 0.791024\n",
      "(Iteration 3535 / 14700) loss: 1.020706\n",
      "(Iteration 3536 / 14700) loss: 0.864877\n",
      "(Iteration 3537 / 14700) loss: 1.198070\n",
      "(Iteration 3538 / 14700) loss: 0.849588\n",
      "(Iteration 3539 / 14700) loss: 0.941445\n",
      "(Iteration 3540 / 14700) loss: 0.944173\n",
      "(Iteration 3541 / 14700) loss: 0.799486\n",
      "(Iteration 3542 / 14700) loss: 1.404628\n",
      "(Iteration 3543 / 14700) loss: 1.006122\n",
      "(Iteration 3544 / 14700) loss: 0.997215\n",
      "(Iteration 3545 / 14700) loss: 0.886566\n",
      "(Iteration 3546 / 14700) loss: 0.851888\n",
      "(Iteration 3547 / 14700) loss: 0.887315\n",
      "(Iteration 3548 / 14700) loss: 0.727124\n",
      "(Iteration 3549 / 14700) loss: 0.913726\n",
      "(Iteration 3550 / 14700) loss: 0.832547\n",
      "(Iteration 3551 / 14700) loss: 0.931252\n",
      "(Iteration 3552 / 14700) loss: 0.938318\n",
      "(Iteration 3553 / 14700) loss: 1.106725\n",
      "(Iteration 3554 / 14700) loss: 1.015031\n",
      "(Iteration 3555 / 14700) loss: 1.017577\n",
      "(Iteration 3556 / 14700) loss: 0.986230\n",
      "(Iteration 3557 / 14700) loss: 0.800918\n",
      "(Iteration 3558 / 14700) loss: 0.885449\n",
      "(Iteration 3559 / 14700) loss: 0.872917\n",
      "(Iteration 3560 / 14700) loss: 0.945275\n",
      "(Iteration 3561 / 14700) loss: 0.869489\n",
      "(Iteration 3562 / 14700) loss: 0.932911\n",
      "(Iteration 3563 / 14700) loss: 0.686758\n",
      "(Iteration 3564 / 14700) loss: 1.092284\n",
      "(Iteration 3565 / 14700) loss: 1.027767\n",
      "(Iteration 3566 / 14700) loss: 0.795448\n",
      "(Iteration 3567 / 14700) loss: 1.128240\n",
      "(Iteration 3568 / 14700) loss: 1.083454\n",
      "(Iteration 3569 / 14700) loss: 0.738410\n",
      "(Iteration 3570 / 14700) loss: 0.902179\n",
      "(Iteration 3571 / 14700) loss: 0.974850\n",
      "(Iteration 3572 / 14700) loss: 0.945515\n",
      "(Iteration 3573 / 14700) loss: 1.109433\n",
      "(Iteration 3574 / 14700) loss: 0.934529\n",
      "(Iteration 3575 / 14700) loss: 0.620830\n",
      "(Iteration 3576 / 14700) loss: 0.970628\n",
      "(Iteration 3577 / 14700) loss: 0.749692\n",
      "(Iteration 3578 / 14700) loss: 0.804796\n",
      "(Iteration 3579 / 14700) loss: 0.937191\n",
      "(Iteration 3580 / 14700) loss: 1.254805\n",
      "(Iteration 3581 / 14700) loss: 0.859718\n",
      "(Iteration 3582 / 14700) loss: 0.887845\n",
      "(Iteration 3583 / 14700) loss: 0.813549\n",
      "(Iteration 3584 / 14700) loss: 1.085720\n",
      "(Iteration 3585 / 14700) loss: 0.683325\n",
      "(Iteration 3586 / 14700) loss: 0.727018\n",
      "(Iteration 3587 / 14700) loss: 1.055103\n",
      "(Iteration 3588 / 14700) loss: 1.031488\n",
      "(Iteration 3589 / 14700) loss: 0.800332\n",
      "(Iteration 3590 / 14700) loss: 0.907663\n",
      "(Iteration 3591 / 14700) loss: 0.659281\n",
      "(Iteration 3592 / 14700) loss: 0.784878\n",
      "(Iteration 3593 / 14700) loss: 0.834343\n",
      "(Iteration 3594 / 14700) loss: 0.804145\n",
      "(Iteration 3595 / 14700) loss: 0.981892\n",
      "(Iteration 3596 / 14700) loss: 0.930692\n",
      "(Iteration 3597 / 14700) loss: 1.008322\n",
      "(Iteration 3598 / 14700) loss: 1.032755\n",
      "(Iteration 3599 / 14700) loss: 0.753491\n",
      "(Iteration 3600 / 14700) loss: 0.962227\n",
      "(Iteration 3601 / 14700) loss: 0.914015\n",
      "(Iteration 3602 / 14700) loss: 0.815461\n",
      "(Iteration 3603 / 14700) loss: 0.547858\n",
      "(Iteration 3604 / 14700) loss: 0.900909\n",
      "(Iteration 3605 / 14700) loss: 0.971572\n",
      "(Iteration 3606 / 14700) loss: 0.797557\n",
      "(Iteration 3607 / 14700) loss: 1.100887\n",
      "(Iteration 3608 / 14700) loss: 0.944647\n",
      "(Iteration 3609 / 14700) loss: 0.540104\n",
      "(Iteration 3610 / 14700) loss: 0.756026\n",
      "(Iteration 3611 / 14700) loss: 0.998393\n",
      "(Iteration 3612 / 14700) loss: 0.709461\n",
      "(Iteration 3613 / 14700) loss: 0.659907\n",
      "(Iteration 3614 / 14700) loss: 0.871338\n",
      "(Iteration 3615 / 14700) loss: 0.705463\n",
      "(Iteration 3616 / 14700) loss: 0.881164\n",
      "(Iteration 3617 / 14700) loss: 0.921006\n",
      "(Iteration 3618 / 14700) loss: 0.680527\n",
      "(Iteration 3619 / 14700) loss: 0.785904\n",
      "(Iteration 3620 / 14700) loss: 0.968051\n",
      "(Iteration 3621 / 14700) loss: 0.778559\n",
      "(Iteration 3622 / 14700) loss: 0.972503\n",
      "(Iteration 3623 / 14700) loss: 0.946550\n",
      "(Iteration 3624 / 14700) loss: 1.027666\n",
      "(Iteration 3625 / 14700) loss: 1.119239\n",
      "(Iteration 3626 / 14700) loss: 0.818125\n",
      "(Iteration 3627 / 14700) loss: 1.047619\n",
      "(Iteration 3628 / 14700) loss: 1.093220\n",
      "(Iteration 3629 / 14700) loss: 0.724912\n",
      "(Iteration 3630 / 14700) loss: 1.240990\n",
      "(Iteration 3631 / 14700) loss: 0.996334\n",
      "(Iteration 3632 / 14700) loss: 0.904224\n",
      "(Iteration 3633 / 14700) loss: 1.165143\n",
      "(Iteration 3634 / 14700) loss: 0.904190\n",
      "(Iteration 3635 / 14700) loss: 0.969466\n",
      "(Iteration 3636 / 14700) loss: 0.807759\n",
      "(Iteration 3637 / 14700) loss: 1.047919\n",
      "(Iteration 3638 / 14700) loss: 0.751346\n",
      "(Iteration 3639 / 14700) loss: 0.904266\n",
      "(Iteration 3640 / 14700) loss: 0.913356\n",
      "(Iteration 3641 / 14700) loss: 0.884650\n",
      "(Iteration 3642 / 14700) loss: 0.963955\n",
      "(Iteration 3643 / 14700) loss: 0.748878\n",
      "(Iteration 3644 / 14700) loss: 0.806209\n",
      "(Iteration 3645 / 14700) loss: 0.896697\n",
      "(Iteration 3646 / 14700) loss: 1.040748\n",
      "(Iteration 3647 / 14700) loss: 0.957355\n",
      "(Iteration 3648 / 14700) loss: 0.780365\n",
      "(Iteration 3649 / 14700) loss: 0.771648\n",
      "(Iteration 3650 / 14700) loss: 0.786374\n",
      "(Iteration 3651 / 14700) loss: 0.667434\n",
      "(Iteration 3652 / 14700) loss: 0.819453\n",
      "(Iteration 3653 / 14700) loss: 1.105517\n",
      "(Iteration 3654 / 14700) loss: 0.965058\n",
      "(Iteration 3655 / 14700) loss: 0.888613\n",
      "(Iteration 3656 / 14700) loss: 0.927061\n",
      "(Iteration 3657 / 14700) loss: 0.941635\n",
      "(Iteration 3658 / 14700) loss: 0.759821\n",
      "(Iteration 3659 / 14700) loss: 0.608718\n",
      "(Iteration 3660 / 14700) loss: 0.715858\n",
      "(Iteration 3661 / 14700) loss: 0.972965\n",
      "(Iteration 3662 / 14700) loss: 0.750567\n",
      "(Iteration 3663 / 14700) loss: 0.929012\n",
      "(Iteration 3664 / 14700) loss: 0.792194\n",
      "(Iteration 3665 / 14700) loss: 1.069419\n",
      "(Iteration 3666 / 14700) loss: 0.838684\n",
      "(Iteration 3667 / 14700) loss: 1.047712\n",
      "(Iteration 3668 / 14700) loss: 0.742810\n",
      "(Iteration 3669 / 14700) loss: 0.909937\n",
      "(Iteration 3670 / 14700) loss: 0.838367\n",
      "(Iteration 3671 / 14700) loss: 0.763103\n",
      "(Iteration 3672 / 14700) loss: 1.263140\n",
      "(Iteration 3673 / 14700) loss: 0.782819\n",
      "(Iteration 3674 / 14700) loss: 0.959132\n",
      "(Iteration 3675 / 14700) loss: 1.009085\n",
      "(Iteration 3676 / 14700) loss: 1.101863\n",
      "(Iteration 3677 / 14700) loss: 0.891192\n",
      "(Iteration 3678 / 14700) loss: 1.129116\n",
      "(Iteration 3679 / 14700) loss: 0.721114\n",
      "(Iteration 3680 / 14700) loss: 1.268419\n",
      "(Iteration 3681 / 14700) loss: 0.785946\n",
      "(Iteration 3682 / 14700) loss: 0.988382\n",
      "(Iteration 3683 / 14700) loss: 0.869340\n",
      "(Iteration 3684 / 14700) loss: 1.022105\n",
      "(Iteration 3685 / 14700) loss: 0.979900\n",
      "(Iteration 3686 / 14700) loss: 0.909981\n",
      "(Iteration 3687 / 14700) loss: 0.947788\n",
      "(Iteration 3688 / 14700) loss: 0.977097\n",
      "(Iteration 3689 / 14700) loss: 0.837887\n",
      "(Iteration 3690 / 14700) loss: 0.879929\n",
      "(Iteration 3691 / 14700) loss: 1.099791\n",
      "(Iteration 3692 / 14700) loss: 0.800761\n",
      "(Iteration 3693 / 14700) loss: 0.814415\n",
      "(Iteration 3694 / 14700) loss: 0.915759\n",
      "(Iteration 3695 / 14700) loss: 0.743265\n",
      "(Iteration 3696 / 14700) loss: 1.221988\n",
      "(Iteration 3697 / 14700) loss: 1.101152\n",
      "(Iteration 3698 / 14700) loss: 1.493316\n",
      "(Iteration 3699 / 14700) loss: 0.845960\n",
      "(Iteration 3700 / 14700) loss: 0.760311\n",
      "(Iteration 3701 / 14700) loss: 0.858234\n",
      "(Iteration 3702 / 14700) loss: 1.039388\n",
      "(Iteration 3703 / 14700) loss: 0.629121\n",
      "(Iteration 3704 / 14700) loss: 0.978975\n",
      "(Iteration 3705 / 14700) loss: 0.697905\n",
      "(Iteration 3706 / 14700) loss: 1.012726\n",
      "(Iteration 3707 / 14700) loss: 0.929635\n",
      "(Iteration 3708 / 14700) loss: 0.699012\n",
      "(Iteration 3709 / 14700) loss: 1.022390\n",
      "(Iteration 3710 / 14700) loss: 0.980847\n",
      "(Iteration 3711 / 14700) loss: 0.745183\n",
      "(Iteration 3712 / 14700) loss: 0.772433\n",
      "(Iteration 3713 / 14700) loss: 0.903539\n",
      "(Iteration 3714 / 14700) loss: 0.716610\n",
      "(Iteration 3715 / 14700) loss: 0.831303\n",
      "(Iteration 3716 / 14700) loss: 0.665540\n",
      "(Iteration 3717 / 14700) loss: 1.003554\n",
      "(Iteration 3718 / 14700) loss: 0.936099\n",
      "(Iteration 3719 / 14700) loss: 1.125796\n",
      "(Iteration 3720 / 14700) loss: 0.958102\n",
      "(Iteration 3721 / 14700) loss: 1.198844\n",
      "(Iteration 3722 / 14700) loss: 0.790249\n",
      "(Iteration 3723 / 14700) loss: 0.816014\n",
      "(Iteration 3724 / 14700) loss: 0.830619\n",
      "(Iteration 3725 / 14700) loss: 0.934309\n",
      "(Iteration 3726 / 14700) loss: 0.876014\n",
      "(Iteration 3727 / 14700) loss: 0.721430\n",
      "(Iteration 3728 / 14700) loss: 1.090853\n",
      "(Iteration 3729 / 14700) loss: 0.998187\n",
      "(Iteration 3730 / 14700) loss: 1.049968\n",
      "(Iteration 3731 / 14700) loss: 1.060547\n",
      "(Iteration 3732 / 14700) loss: 0.792561\n",
      "(Iteration 3733 / 14700) loss: 0.994457\n",
      "(Iteration 3734 / 14700) loss: 1.124025\n",
      "(Iteration 3735 / 14700) loss: 0.990736\n",
      "(Iteration 3736 / 14700) loss: 0.750531\n",
      "(Iteration 3737 / 14700) loss: 0.914002\n",
      "(Iteration 3738 / 14700) loss: 0.917161\n",
      "(Iteration 3739 / 14700) loss: 0.878567\n",
      "(Iteration 3740 / 14700) loss: 1.070219\n",
      "(Iteration 3741 / 14700) loss: 1.040387\n",
      "(Iteration 3742 / 14700) loss: 0.919491\n",
      "(Iteration 3743 / 14700) loss: 1.030757\n",
      "(Iteration 3744 / 14700) loss: 0.781772\n",
      "(Iteration 3745 / 14700) loss: 0.954261\n",
      "(Iteration 3746 / 14700) loss: 0.744901\n",
      "(Iteration 3747 / 14700) loss: 0.983236\n",
      "(Iteration 3748 / 14700) loss: 0.550585\n",
      "(Iteration 3749 / 14700) loss: 0.858689\n",
      "(Iteration 3750 / 14700) loss: 0.928911\n",
      "(Iteration 3751 / 14700) loss: 0.863745\n",
      "(Iteration 3752 / 14700) loss: 1.119698\n",
      "(Iteration 3753 / 14700) loss: 0.752584\n",
      "(Iteration 3754 / 14700) loss: 0.976386\n",
      "(Iteration 3755 / 14700) loss: 0.853051\n",
      "(Iteration 3756 / 14700) loss: 0.782024\n",
      "(Iteration 3757 / 14700) loss: 1.074008\n",
      "(Iteration 3758 / 14700) loss: 1.193629\n",
      "(Iteration 3759 / 14700) loss: 0.845531\n",
      "(Iteration 3760 / 14700) loss: 0.992467\n",
      "(Iteration 3761 / 14700) loss: 1.012950\n",
      "(Iteration 3762 / 14700) loss: 0.957838\n",
      "(Iteration 3763 / 14700) loss: 0.892356\n",
      "(Iteration 3764 / 14700) loss: 0.739536\n",
      "(Iteration 3765 / 14700) loss: 0.805130\n",
      "(Iteration 3766 / 14700) loss: 0.865340\n",
      "(Iteration 3767 / 14700) loss: 1.173031\n",
      "(Iteration 3768 / 14700) loss: 0.781511\n",
      "(Iteration 3769 / 14700) loss: 0.733915\n",
      "(Iteration 3770 / 14700) loss: 0.829674\n",
      "(Iteration 3771 / 14700) loss: 0.916540\n",
      "(Iteration 3772 / 14700) loss: 0.969555\n",
      "(Iteration 3773 / 14700) loss: 0.828482\n",
      "(Iteration 3774 / 14700) loss: 1.065975\n",
      "(Iteration 3775 / 14700) loss: 0.621096\n",
      "(Iteration 3776 / 14700) loss: 0.792733\n",
      "(Iteration 3777 / 14700) loss: 0.948492\n",
      "(Iteration 3778 / 14700) loss: 0.829456\n",
      "(Iteration 3779 / 14700) loss: 0.923521\n",
      "(Iteration 3780 / 14700) loss: 0.804566\n",
      "(Iteration 3781 / 14700) loss: 0.953121\n",
      "(Iteration 3782 / 14700) loss: 1.030884\n",
      "(Iteration 3783 / 14700) loss: 0.634762\n",
      "(Iteration 3784 / 14700) loss: 1.024846\n",
      "(Iteration 3785 / 14700) loss: 0.772329\n",
      "(Iteration 3786 / 14700) loss: 0.764464\n",
      "(Iteration 3787 / 14700) loss: 0.897478\n",
      "(Iteration 3788 / 14700) loss: 0.947261\n",
      "(Iteration 3789 / 14700) loss: 1.101272\n",
      "(Iteration 3790 / 14700) loss: 0.968722\n",
      "(Iteration 3791 / 14700) loss: 0.678757\n",
      "(Iteration 3792 / 14700) loss: 1.110060\n",
      "(Iteration 3793 / 14700) loss: 1.129321\n",
      "(Iteration 3794 / 14700) loss: 0.812918\n",
      "(Iteration 3795 / 14700) loss: 0.935011\n",
      "(Iteration 3796 / 14700) loss: 0.751300\n",
      "(Iteration 3797 / 14700) loss: 1.028144\n",
      "(Iteration 3798 / 14700) loss: 1.096419\n",
      "(Iteration 3799 / 14700) loss: 0.863484\n",
      "(Iteration 3800 / 14700) loss: 1.374529\n",
      "(Iteration 3801 / 14700) loss: 0.757260\n",
      "(Iteration 3802 / 14700) loss: 0.747071\n",
      "(Iteration 3803 / 14700) loss: 0.889429\n",
      "(Iteration 3804 / 14700) loss: 0.836565\n",
      "(Iteration 3805 / 14700) loss: 1.089105\n",
      "(Iteration 3806 / 14700) loss: 0.706704\n",
      "(Iteration 3807 / 14700) loss: 0.668592\n",
      "(Iteration 3808 / 14700) loss: 0.931975\n",
      "(Iteration 3809 / 14700) loss: 0.775734\n",
      "(Iteration 3810 / 14700) loss: 0.931760\n",
      "(Iteration 3811 / 14700) loss: 0.684704\n",
      "(Iteration 3812 / 14700) loss: 0.968988\n",
      "(Iteration 3813 / 14700) loss: 0.695062\n",
      "(Iteration 3814 / 14700) loss: 0.883626\n",
      "(Iteration 3815 / 14700) loss: 0.850405\n",
      "(Iteration 3816 / 14700) loss: 0.953480\n",
      "(Iteration 3817 / 14700) loss: 1.085137\n",
      "(Iteration 3818 / 14700) loss: 0.616738\n",
      "(Iteration 3819 / 14700) loss: 0.768297\n",
      "(Iteration 3820 / 14700) loss: 0.898640\n",
      "(Iteration 3821 / 14700) loss: 1.054080\n",
      "(Iteration 3822 / 14700) loss: 0.622143\n",
      "(Iteration 3823 / 14700) loss: 1.114527\n",
      "(Iteration 3824 / 14700) loss: 0.869625\n",
      "(Iteration 3825 / 14700) loss: 1.255003\n",
      "(Iteration 3826 / 14700) loss: 0.988045\n",
      "(Iteration 3827 / 14700) loss: 0.875901\n",
      "(Iteration 3828 / 14700) loss: 0.986245\n",
      "(Iteration 3829 / 14700) loss: 0.641366\n",
      "(Iteration 3830 / 14700) loss: 0.742481\n",
      "(Iteration 3831 / 14700) loss: 1.019776\n",
      "(Iteration 3832 / 14700) loss: 0.776409\n",
      "(Iteration 3833 / 14700) loss: 0.977184\n",
      "(Iteration 3834 / 14700) loss: 0.961260\n",
      "(Iteration 3835 / 14700) loss: 0.705099\n",
      "(Iteration 3836 / 14700) loss: 0.926284\n",
      "(Iteration 3837 / 14700) loss: 0.976645\n",
      "(Iteration 3838 / 14700) loss: 0.979359\n",
      "(Iteration 3839 / 14700) loss: 1.223092\n",
      "(Iteration 3840 / 14700) loss: 0.841422\n",
      "(Iteration 3841 / 14700) loss: 0.889645\n",
      "(Iteration 3842 / 14700) loss: 0.914525\n",
      "(Iteration 3843 / 14700) loss: 0.936651\n",
      "(Iteration 3844 / 14700) loss: 0.826334\n",
      "(Iteration 3845 / 14700) loss: 0.827727\n",
      "(Iteration 3846 / 14700) loss: 1.062197\n",
      "(Iteration 3847 / 14700) loss: 0.772233\n",
      "(Iteration 3848 / 14700) loss: 0.810803\n",
      "(Iteration 3849 / 14700) loss: 0.989160\n",
      "(Iteration 3850 / 14700) loss: 0.781783\n",
      "(Iteration 3851 / 14700) loss: 0.960277\n",
      "(Iteration 3852 / 14700) loss: 0.964834\n",
      "(Iteration 3853 / 14700) loss: 0.923252\n",
      "(Iteration 3854 / 14700) loss: 0.892989\n",
      "(Iteration 3855 / 14700) loss: 1.050809\n",
      "(Iteration 3856 / 14700) loss: 0.984671\n",
      "(Iteration 3857 / 14700) loss: 0.709039\n",
      "(Iteration 3858 / 14700) loss: 0.914076\n",
      "(Iteration 3859 / 14700) loss: 1.001359\n",
      "(Iteration 3860 / 14700) loss: 0.846452\n",
      "(Iteration 3861 / 14700) loss: 0.787502\n",
      "(Iteration 3862 / 14700) loss: 0.918022\n",
      "(Iteration 3863 / 14700) loss: 0.844943\n",
      "(Iteration 3864 / 14700) loss: 0.802712\n",
      "(Iteration 3865 / 14700) loss: 0.721561\n",
      "(Iteration 3866 / 14700) loss: 1.005114\n",
      "(Iteration 3867 / 14700) loss: 1.091648\n",
      "(Iteration 3868 / 14700) loss: 0.880221\n",
      "(Iteration 3869 / 14700) loss: 0.888547\n",
      "(Iteration 3870 / 14700) loss: 1.205682\n",
      "(Iteration 3871 / 14700) loss: 1.048167\n",
      "(Iteration 3872 / 14700) loss: 1.061419\n",
      "(Iteration 3873 / 14700) loss: 1.003877\n",
      "(Iteration 3874 / 14700) loss: 0.888124\n",
      "(Iteration 3875 / 14700) loss: 1.220393\n",
      "(Iteration 3876 / 14700) loss: 0.957283\n",
      "(Iteration 3877 / 14700) loss: 0.897799\n",
      "(Iteration 3878 / 14700) loss: 0.792846\n",
      "(Iteration 3879 / 14700) loss: 1.009672\n",
      "(Iteration 3880 / 14700) loss: 0.848947\n",
      "(Iteration 3881 / 14700) loss: 1.206695\n",
      "(Iteration 3882 / 14700) loss: 1.039183\n",
      "(Iteration 3883 / 14700) loss: 0.743750\n",
      "(Iteration 3884 / 14700) loss: 1.080134\n",
      "(Iteration 3885 / 14700) loss: 1.093720\n",
      "(Iteration 3886 / 14700) loss: 0.886512\n",
      "(Iteration 3887 / 14700) loss: 0.936383\n",
      "(Iteration 3888 / 14700) loss: 0.774756\n",
      "(Iteration 3889 / 14700) loss: 0.894371\n",
      "(Iteration 3890 / 14700) loss: 0.605772\n",
      "(Iteration 3891 / 14700) loss: 0.908965\n",
      "(Iteration 3892 / 14700) loss: 0.954838\n",
      "(Iteration 3893 / 14700) loss: 1.013913\n",
      "(Iteration 3894 / 14700) loss: 0.753117\n",
      "(Iteration 3895 / 14700) loss: 0.765710\n",
      "(Iteration 3896 / 14700) loss: 0.765240\n",
      "(Iteration 3897 / 14700) loss: 0.653078\n",
      "(Iteration 3898 / 14700) loss: 1.027147\n",
      "(Iteration 3899 / 14700) loss: 0.734773\n",
      "(Iteration 3900 / 14700) loss: 0.679314\n",
      "(Iteration 3901 / 14700) loss: 1.168065\n",
      "(Iteration 3902 / 14700) loss: 0.828010\n",
      "(Iteration 3903 / 14700) loss: 0.877866\n",
      "(Iteration 3904 / 14700) loss: 0.931741\n",
      "(Iteration 3905 / 14700) loss: 0.788155\n",
      "(Iteration 3906 / 14700) loss: 1.026981\n",
      "(Iteration 3907 / 14700) loss: 0.927704\n",
      "(Iteration 3908 / 14700) loss: 0.815243\n",
      "(Iteration 3909 / 14700) loss: 0.795822\n",
      "(Iteration 3910 / 14700) loss: 1.105884\n",
      "(Iteration 3911 / 14700) loss: 0.892521\n",
      "(Iteration 3912 / 14700) loss: 1.025216\n",
      "(Iteration 3913 / 14700) loss: 0.752748\n",
      "(Iteration 3914 / 14700) loss: 0.688738\n",
      "(Iteration 3915 / 14700) loss: 0.803123\n",
      "(Iteration 3916 / 14700) loss: 0.855000\n",
      "(Iteration 3917 / 14700) loss: 0.834282\n",
      "(Iteration 3918 / 14700) loss: 0.962035\n",
      "(Iteration 3919 / 14700) loss: 1.093228\n",
      "(Iteration 3920 / 14700) loss: 1.024389\n",
      "(Epoch 4 / 15) train acc: 0.756000; val_acc: 0.735000\n",
      "(Iteration 3921 / 14700) loss: 0.699095\n",
      "(Iteration 3922 / 14700) loss: 0.669660\n",
      "(Iteration 3923 / 14700) loss: 0.901218\n",
      "(Iteration 3924 / 14700) loss: 1.187030\n",
      "(Iteration 3925 / 14700) loss: 0.666163\n",
      "(Iteration 3926 / 14700) loss: 1.145117\n",
      "(Iteration 3927 / 14700) loss: 1.186648\n",
      "(Iteration 3928 / 14700) loss: 1.109866\n",
      "(Iteration 3929 / 14700) loss: 1.085642\n",
      "(Iteration 3930 / 14700) loss: 1.147503\n",
      "(Iteration 3931 / 14700) loss: 0.933816\n",
      "(Iteration 3932 / 14700) loss: 0.790032\n",
      "(Iteration 3933 / 14700) loss: 0.829543\n",
      "(Iteration 3934 / 14700) loss: 0.965092\n",
      "(Iteration 3935 / 14700) loss: 0.789023\n",
      "(Iteration 3936 / 14700) loss: 0.812180\n",
      "(Iteration 3937 / 14700) loss: 0.666585\n",
      "(Iteration 3938 / 14700) loss: 0.877663\n",
      "(Iteration 3939 / 14700) loss: 0.941207\n",
      "(Iteration 3940 / 14700) loss: 1.036245\n",
      "(Iteration 3941 / 14700) loss: 0.964306\n",
      "(Iteration 3942 / 14700) loss: 1.072572\n",
      "(Iteration 3943 / 14700) loss: 0.881371\n",
      "(Iteration 3944 / 14700) loss: 0.981462\n",
      "(Iteration 3945 / 14700) loss: 0.983090\n",
      "(Iteration 3946 / 14700) loss: 1.050768\n",
      "(Iteration 3947 / 14700) loss: 0.973270\n",
      "(Iteration 3948 / 14700) loss: 0.871420\n",
      "(Iteration 3949 / 14700) loss: 0.818582\n",
      "(Iteration 3950 / 14700) loss: 1.137748\n",
      "(Iteration 3951 / 14700) loss: 0.687360\n",
      "(Iteration 3952 / 14700) loss: 0.861339\n",
      "(Iteration 3953 / 14700) loss: 0.852775\n",
      "(Iteration 3954 / 14700) loss: 0.693030\n",
      "(Iteration 3955 / 14700) loss: 0.619053\n",
      "(Iteration 3956 / 14700) loss: 1.174570\n",
      "(Iteration 3957 / 14700) loss: 1.076735\n",
      "(Iteration 3958 / 14700) loss: 1.134455\n",
      "(Iteration 3959 / 14700) loss: 1.098282\n",
      "(Iteration 3960 / 14700) loss: 0.829721\n",
      "(Iteration 3961 / 14700) loss: 0.774425\n",
      "(Iteration 3962 / 14700) loss: 1.004306\n",
      "(Iteration 3963 / 14700) loss: 0.847418\n",
      "(Iteration 3964 / 14700) loss: 0.828051\n",
      "(Iteration 3965 / 14700) loss: 0.916908\n",
      "(Iteration 3966 / 14700) loss: 0.900170\n",
      "(Iteration 3967 / 14700) loss: 1.033452\n",
      "(Iteration 3968 / 14700) loss: 0.808660\n",
      "(Iteration 3969 / 14700) loss: 0.918955\n",
      "(Iteration 3970 / 14700) loss: 0.769534\n",
      "(Iteration 3971 / 14700) loss: 0.747105\n",
      "(Iteration 3972 / 14700) loss: 0.936915\n",
      "(Iteration 3973 / 14700) loss: 0.897264\n",
      "(Iteration 3974 / 14700) loss: 0.836796\n",
      "(Iteration 3975 / 14700) loss: 0.882291\n",
      "(Iteration 3976 / 14700) loss: 1.147297\n",
      "(Iteration 3977 / 14700) loss: 0.883080\n",
      "(Iteration 3978 / 14700) loss: 0.720586\n",
      "(Iteration 3979 / 14700) loss: 0.694965\n",
      "(Iteration 3980 / 14700) loss: 1.188456\n",
      "(Iteration 3981 / 14700) loss: 0.656190\n",
      "(Iteration 3982 / 14700) loss: 0.800929\n",
      "(Iteration 3983 / 14700) loss: 0.901489\n",
      "(Iteration 3984 / 14700) loss: 0.831767\n",
      "(Iteration 3985 / 14700) loss: 1.106015\n",
      "(Iteration 3986 / 14700) loss: 1.039149\n",
      "(Iteration 3987 / 14700) loss: 0.874673\n",
      "(Iteration 3988 / 14700) loss: 1.054080\n",
      "(Iteration 3989 / 14700) loss: 0.949547\n",
      "(Iteration 3990 / 14700) loss: 0.894968\n",
      "(Iteration 3991 / 14700) loss: 1.006393\n",
      "(Iteration 3992 / 14700) loss: 0.970910\n",
      "(Iteration 3993 / 14700) loss: 0.993838\n",
      "(Iteration 3994 / 14700) loss: 0.711663\n",
      "(Iteration 3995 / 14700) loss: 0.640743\n",
      "(Iteration 3996 / 14700) loss: 0.884810\n",
      "(Iteration 3997 / 14700) loss: 1.051065\n",
      "(Iteration 3998 / 14700) loss: 0.840269\n",
      "(Iteration 3999 / 14700) loss: 0.852878\n",
      "(Iteration 4000 / 14700) loss: 0.811312\n",
      "(Iteration 4001 / 14700) loss: 1.173277\n",
      "(Iteration 4002 / 14700) loss: 0.767393\n",
      "(Iteration 4003 / 14700) loss: 0.695942\n",
      "(Iteration 4004 / 14700) loss: 0.726356\n",
      "(Iteration 4005 / 14700) loss: 0.837967\n",
      "(Iteration 4006 / 14700) loss: 1.029171\n",
      "(Iteration 4007 / 14700) loss: 0.625296\n",
      "(Iteration 4008 / 14700) loss: 0.812720\n",
      "(Iteration 4009 / 14700) loss: 1.024042\n",
      "(Iteration 4010 / 14700) loss: 0.869007\n",
      "(Iteration 4011 / 14700) loss: 0.773060\n",
      "(Iteration 4012 / 14700) loss: 1.268489\n",
      "(Iteration 4013 / 14700) loss: 0.823194\n",
      "(Iteration 4014 / 14700) loss: 1.005319\n",
      "(Iteration 4015 / 14700) loss: 0.615720\n",
      "(Iteration 4016 / 14700) loss: 1.063309\n",
      "(Iteration 4017 / 14700) loss: 0.632424\n",
      "(Iteration 4018 / 14700) loss: 0.827765\n",
      "(Iteration 4019 / 14700) loss: 0.676577\n",
      "(Iteration 4020 / 14700) loss: 0.944627\n",
      "(Iteration 4021 / 14700) loss: 0.759333\n",
      "(Iteration 4022 / 14700) loss: 0.699866\n",
      "(Iteration 4023 / 14700) loss: 0.735369\n",
      "(Iteration 4024 / 14700) loss: 0.764054\n",
      "(Iteration 4025 / 14700) loss: 1.082011\n",
      "(Iteration 4026 / 14700) loss: 0.892495\n",
      "(Iteration 4027 / 14700) loss: 1.090625\n",
      "(Iteration 4028 / 14700) loss: 0.863334\n",
      "(Iteration 4029 / 14700) loss: 1.043790\n",
      "(Iteration 4030 / 14700) loss: 0.729351\n",
      "(Iteration 4031 / 14700) loss: 0.750981\n",
      "(Iteration 4032 / 14700) loss: 0.743959\n",
      "(Iteration 4033 / 14700) loss: 0.939360\n",
      "(Iteration 4034 / 14700) loss: 1.140913\n",
      "(Iteration 4035 / 14700) loss: 0.732712\n",
      "(Iteration 4036 / 14700) loss: 0.843222\n",
      "(Iteration 4037 / 14700) loss: 0.775314\n",
      "(Iteration 4038 / 14700) loss: 0.929920\n",
      "(Iteration 4039 / 14700) loss: 0.665393\n",
      "(Iteration 4040 / 14700) loss: 0.790980\n",
      "(Iteration 4041 / 14700) loss: 0.669386\n",
      "(Iteration 4042 / 14700) loss: 0.870443\n",
      "(Iteration 4043 / 14700) loss: 1.021065\n",
      "(Iteration 4044 / 14700) loss: 1.107950\n",
      "(Iteration 4045 / 14700) loss: 0.901378\n",
      "(Iteration 4046 / 14700) loss: 0.920521\n",
      "(Iteration 4047 / 14700) loss: 0.914604\n",
      "(Iteration 4048 / 14700) loss: 0.921738\n",
      "(Iteration 4049 / 14700) loss: 1.081348\n",
      "(Iteration 4050 / 14700) loss: 0.885544\n",
      "(Iteration 4051 / 14700) loss: 0.793205\n",
      "(Iteration 4052 / 14700) loss: 0.767660\n",
      "(Iteration 4053 / 14700) loss: 1.087609\n",
      "(Iteration 4054 / 14700) loss: 0.789885\n",
      "(Iteration 4055 / 14700) loss: 0.843320\n",
      "(Iteration 4056 / 14700) loss: 1.065388\n",
      "(Iteration 4057 / 14700) loss: 0.785592\n",
      "(Iteration 4058 / 14700) loss: 0.840740\n",
      "(Iteration 4059 / 14700) loss: 0.984968\n",
      "(Iteration 4060 / 14700) loss: 1.074847\n",
      "(Iteration 4061 / 14700) loss: 0.844630\n",
      "(Iteration 4062 / 14700) loss: 0.761644\n",
      "(Iteration 4063 / 14700) loss: 0.841954\n",
      "(Iteration 4064 / 14700) loss: 0.933489\n",
      "(Iteration 4065 / 14700) loss: 0.763845\n",
      "(Iteration 4066 / 14700) loss: 0.820277\n",
      "(Iteration 4067 / 14700) loss: 0.565535\n",
      "(Iteration 4068 / 14700) loss: 1.089682\n",
      "(Iteration 4069 / 14700) loss: 0.656890\n",
      "(Iteration 4070 / 14700) loss: 0.837328\n",
      "(Iteration 4071 / 14700) loss: 0.975154\n",
      "(Iteration 4072 / 14700) loss: 0.996894\n",
      "(Iteration 4073 / 14700) loss: 0.994431\n",
      "(Iteration 4074 / 14700) loss: 0.898104\n",
      "(Iteration 4075 / 14700) loss: 0.763100\n",
      "(Iteration 4076 / 14700) loss: 1.083884\n",
      "(Iteration 4077 / 14700) loss: 0.940737\n",
      "(Iteration 4078 / 14700) loss: 0.917152\n",
      "(Iteration 4079 / 14700) loss: 0.939381\n",
      "(Iteration 4080 / 14700) loss: 0.911336\n",
      "(Iteration 4081 / 14700) loss: 1.182175\n",
      "(Iteration 4082 / 14700) loss: 0.806210\n",
      "(Iteration 4083 / 14700) loss: 0.867762\n",
      "(Iteration 4084 / 14700) loss: 0.759567\n",
      "(Iteration 4085 / 14700) loss: 0.760850\n",
      "(Iteration 4086 / 14700) loss: 0.993539\n",
      "(Iteration 4087 / 14700) loss: 0.905075\n",
      "(Iteration 4088 / 14700) loss: 1.008118\n",
      "(Iteration 4089 / 14700) loss: 0.751875\n",
      "(Iteration 4090 / 14700) loss: 1.007174\n",
      "(Iteration 4091 / 14700) loss: 0.816141\n",
      "(Iteration 4092 / 14700) loss: 0.689067\n",
      "(Iteration 4093 / 14700) loss: 0.762788\n",
      "(Iteration 4094 / 14700) loss: 0.785987\n",
      "(Iteration 4095 / 14700) loss: 0.798439\n",
      "(Iteration 4096 / 14700) loss: 0.821857\n",
      "(Iteration 4097 / 14700) loss: 0.939276\n",
      "(Iteration 4098 / 14700) loss: 0.744119\n",
      "(Iteration 4099 / 14700) loss: 1.032492\n",
      "(Iteration 4100 / 14700) loss: 0.825426\n",
      "(Iteration 4101 / 14700) loss: 0.791289\n",
      "(Iteration 4102 / 14700) loss: 1.018893\n",
      "(Iteration 4103 / 14700) loss: 0.887449\n",
      "(Iteration 4104 / 14700) loss: 0.583683\n",
      "(Iteration 4105 / 14700) loss: 0.710196\n",
      "(Iteration 4106 / 14700) loss: 1.093773\n",
      "(Iteration 4107 / 14700) loss: 1.031585\n",
      "(Iteration 4108 / 14700) loss: 1.130629\n",
      "(Iteration 4109 / 14700) loss: 1.021860\n",
      "(Iteration 4110 / 14700) loss: 0.955606\n",
      "(Iteration 4111 / 14700) loss: 0.955684\n",
      "(Iteration 4112 / 14700) loss: 0.817066\n",
      "(Iteration 4113 / 14700) loss: 0.989450\n",
      "(Iteration 4114 / 14700) loss: 0.805192\n",
      "(Iteration 4115 / 14700) loss: 0.694311\n",
      "(Iteration 4116 / 14700) loss: 0.872568\n",
      "(Iteration 4117 / 14700) loss: 0.882599\n",
      "(Iteration 4118 / 14700) loss: 0.933334\n",
      "(Iteration 4119 / 14700) loss: 0.784035\n",
      "(Iteration 4120 / 14700) loss: 0.702395\n",
      "(Iteration 4121 / 14700) loss: 0.814739\n",
      "(Iteration 4122 / 14700) loss: 1.037909\n",
      "(Iteration 4123 / 14700) loss: 0.770943\n",
      "(Iteration 4124 / 14700) loss: 0.956954\n",
      "(Iteration 4125 / 14700) loss: 0.693314\n",
      "(Iteration 4126 / 14700) loss: 0.680520\n",
      "(Iteration 4127 / 14700) loss: 1.133714\n",
      "(Iteration 4128 / 14700) loss: 0.775580\n",
      "(Iteration 4129 / 14700) loss: 0.933468\n",
      "(Iteration 4130 / 14700) loss: 0.857275\n",
      "(Iteration 4131 / 14700) loss: 1.116307\n",
      "(Iteration 4132 / 14700) loss: 0.954695\n",
      "(Iteration 4133 / 14700) loss: 0.699197\n",
      "(Iteration 4134 / 14700) loss: 1.033040\n",
      "(Iteration 4135 / 14700) loss: 0.977832\n",
      "(Iteration 4136 / 14700) loss: 0.738405\n",
      "(Iteration 4137 / 14700) loss: 1.008769\n",
      "(Iteration 4138 / 14700) loss: 0.928861\n",
      "(Iteration 4139 / 14700) loss: 0.821490\n",
      "(Iteration 4140 / 14700) loss: 0.783349\n",
      "(Iteration 4141 / 14700) loss: 0.941909\n",
      "(Iteration 4142 / 14700) loss: 0.676918\n",
      "(Iteration 4143 / 14700) loss: 1.109330\n",
      "(Iteration 4144 / 14700) loss: 0.889548\n",
      "(Iteration 4145 / 14700) loss: 1.084836\n",
      "(Iteration 4146 / 14700) loss: 0.801486\n",
      "(Iteration 4147 / 14700) loss: 0.821337\n",
      "(Iteration 4148 / 14700) loss: 0.978617\n",
      "(Iteration 4149 / 14700) loss: 0.944406\n",
      "(Iteration 4150 / 14700) loss: 0.659360\n",
      "(Iteration 4151 / 14700) loss: 0.712367\n",
      "(Iteration 4152 / 14700) loss: 0.873680\n",
      "(Iteration 4153 / 14700) loss: 0.929167\n",
      "(Iteration 4154 / 14700) loss: 0.774273\n",
      "(Iteration 4155 / 14700) loss: 0.749839\n",
      "(Iteration 4156 / 14700) loss: 0.992476\n",
      "(Iteration 4157 / 14700) loss: 0.854780\n",
      "(Iteration 4158 / 14700) loss: 1.189877\n",
      "(Iteration 4159 / 14700) loss: 0.774586\n",
      "(Iteration 4160 / 14700) loss: 1.087282\n",
      "(Iteration 4161 / 14700) loss: 1.037395\n",
      "(Iteration 4162 / 14700) loss: 0.920181\n",
      "(Iteration 4163 / 14700) loss: 0.924914\n",
      "(Iteration 4164 / 14700) loss: 0.864897\n",
      "(Iteration 4165 / 14700) loss: 1.026018\n",
      "(Iteration 4166 / 14700) loss: 0.830031\n",
      "(Iteration 4167 / 14700) loss: 0.689377\n",
      "(Iteration 4168 / 14700) loss: 0.746637\n",
      "(Iteration 4169 / 14700) loss: 0.986414\n",
      "(Iteration 4170 / 14700) loss: 0.756790\n",
      "(Iteration 4171 / 14700) loss: 1.253557\n",
      "(Iteration 4172 / 14700) loss: 0.811354\n",
      "(Iteration 4173 / 14700) loss: 0.993399\n",
      "(Iteration 4174 / 14700) loss: 0.791289\n",
      "(Iteration 4175 / 14700) loss: 1.087305\n",
      "(Iteration 4176 / 14700) loss: 0.871183\n",
      "(Iteration 4177 / 14700) loss: 1.103443\n",
      "(Iteration 4178 / 14700) loss: 0.841158\n",
      "(Iteration 4179 / 14700) loss: 0.886499\n",
      "(Iteration 4180 / 14700) loss: 0.986042\n",
      "(Iteration 4181 / 14700) loss: 1.271019\n",
      "(Iteration 4182 / 14700) loss: 0.970733\n",
      "(Iteration 4183 / 14700) loss: 0.747589\n",
      "(Iteration 4184 / 14700) loss: 0.782068\n",
      "(Iteration 4185 / 14700) loss: 1.172403\n",
      "(Iteration 4186 / 14700) loss: 0.995934\n",
      "(Iteration 4187 / 14700) loss: 0.793883\n",
      "(Iteration 4188 / 14700) loss: 0.804749\n",
      "(Iteration 4189 / 14700) loss: 0.984774\n",
      "(Iteration 4190 / 14700) loss: 0.982923\n",
      "(Iteration 4191 / 14700) loss: 1.062712\n",
      "(Iteration 4192 / 14700) loss: 1.016388\n",
      "(Iteration 4193 / 14700) loss: 0.781761\n",
      "(Iteration 4194 / 14700) loss: 0.949933\n",
      "(Iteration 4195 / 14700) loss: 1.004340\n",
      "(Iteration 4196 / 14700) loss: 0.930924\n",
      "(Iteration 4197 / 14700) loss: 0.753815\n",
      "(Iteration 4198 / 14700) loss: 0.891131\n",
      "(Iteration 4199 / 14700) loss: 1.029068\n",
      "(Iteration 4200 / 14700) loss: 0.764517\n",
      "(Iteration 4201 / 14700) loss: 0.738868\n",
      "(Iteration 4202 / 14700) loss: 0.890074\n",
      "(Iteration 4203 / 14700) loss: 1.120845\n",
      "(Iteration 4204 / 14700) loss: 0.717160\n",
      "(Iteration 4205 / 14700) loss: 0.844727\n",
      "(Iteration 4206 / 14700) loss: 0.936549\n",
      "(Iteration 4207 / 14700) loss: 1.157906\n",
      "(Iteration 4208 / 14700) loss: 0.853941\n",
      "(Iteration 4209 / 14700) loss: 0.902819\n",
      "(Iteration 4210 / 14700) loss: 0.875228\n",
      "(Iteration 4211 / 14700) loss: 0.947473\n",
      "(Iteration 4212 / 14700) loss: 0.797713\n",
      "(Iteration 4213 / 14700) loss: 0.946922\n",
      "(Iteration 4214 / 14700) loss: 0.782940\n",
      "(Iteration 4215 / 14700) loss: 1.037316\n",
      "(Iteration 4216 / 14700) loss: 0.801211\n",
      "(Iteration 4217 / 14700) loss: 0.943748\n",
      "(Iteration 4218 / 14700) loss: 0.734062\n",
      "(Iteration 4219 / 14700) loss: 1.021408\n",
      "(Iteration 4220 / 14700) loss: 0.727588\n",
      "(Iteration 4221 / 14700) loss: 0.881729\n",
      "(Iteration 4222 / 14700) loss: 0.739996\n",
      "(Iteration 4223 / 14700) loss: 1.021506\n",
      "(Iteration 4224 / 14700) loss: 0.647859\n",
      "(Iteration 4225 / 14700) loss: 0.823574\n",
      "(Iteration 4226 / 14700) loss: 0.733574\n",
      "(Iteration 4227 / 14700) loss: 1.139736\n",
      "(Iteration 4228 / 14700) loss: 0.967700\n",
      "(Iteration 4229 / 14700) loss: 1.189768\n",
      "(Iteration 4230 / 14700) loss: 0.828576\n",
      "(Iteration 4231 / 14700) loss: 0.920620\n",
      "(Iteration 4232 / 14700) loss: 0.901338\n",
      "(Iteration 4233 / 14700) loss: 0.812040\n",
      "(Iteration 4234 / 14700) loss: 1.041735\n",
      "(Iteration 4235 / 14700) loss: 0.613394\n",
      "(Iteration 4236 / 14700) loss: 0.830066\n",
      "(Iteration 4237 / 14700) loss: 0.793351\n",
      "(Iteration 4238 / 14700) loss: 1.033401\n",
      "(Iteration 4239 / 14700) loss: 0.717003\n",
      "(Iteration 4240 / 14700) loss: 0.695300\n",
      "(Iteration 4241 / 14700) loss: 0.806632\n",
      "(Iteration 4242 / 14700) loss: 0.767012\n",
      "(Iteration 4243 / 14700) loss: 0.706802\n",
      "(Iteration 4244 / 14700) loss: 0.753639\n",
      "(Iteration 4245 / 14700) loss: 1.028175\n",
      "(Iteration 4246 / 14700) loss: 0.945947\n",
      "(Iteration 4247 / 14700) loss: 0.868047\n",
      "(Iteration 4248 / 14700) loss: 0.837191\n",
      "(Iteration 4249 / 14700) loss: 0.797397\n",
      "(Iteration 4250 / 14700) loss: 0.805684\n",
      "(Iteration 4251 / 14700) loss: 0.794905\n",
      "(Iteration 4252 / 14700) loss: 0.789907\n",
      "(Iteration 4253 / 14700) loss: 0.717950\n",
      "(Iteration 4254 / 14700) loss: 0.955093\n",
      "(Iteration 4255 / 14700) loss: 1.114614\n",
      "(Iteration 4256 / 14700) loss: 0.794423\n",
      "(Iteration 4257 / 14700) loss: 0.884812\n",
      "(Iteration 4258 / 14700) loss: 0.684856\n",
      "(Iteration 4259 / 14700) loss: 0.735194\n",
      "(Iteration 4260 / 14700) loss: 1.243132\n",
      "(Iteration 4261 / 14700) loss: 1.163324\n",
      "(Iteration 4262 / 14700) loss: 0.763427\n",
      "(Iteration 4263 / 14700) loss: 0.857918\n",
      "(Iteration 4264 / 14700) loss: 0.768671\n",
      "(Iteration 4265 / 14700) loss: 0.963449\n",
      "(Iteration 4266 / 14700) loss: 0.884104\n",
      "(Iteration 4267 / 14700) loss: 0.901058\n",
      "(Iteration 4268 / 14700) loss: 0.793597\n",
      "(Iteration 4269 / 14700) loss: 0.774700\n",
      "(Iteration 4270 / 14700) loss: 0.769412\n",
      "(Iteration 4271 / 14700) loss: 0.932128\n",
      "(Iteration 4272 / 14700) loss: 0.859003\n",
      "(Iteration 4273 / 14700) loss: 1.087330\n",
      "(Iteration 4274 / 14700) loss: 0.804279\n",
      "(Iteration 4275 / 14700) loss: 0.670563\n",
      "(Iteration 4276 / 14700) loss: 0.666853\n",
      "(Iteration 4277 / 14700) loss: 1.022962\n",
      "(Iteration 4278 / 14700) loss: 0.878012\n",
      "(Iteration 4279 / 14700) loss: 1.099836\n",
      "(Iteration 4280 / 14700) loss: 0.929150\n",
      "(Iteration 4281 / 14700) loss: 0.596308\n",
      "(Iteration 4282 / 14700) loss: 1.017879\n",
      "(Iteration 4283 / 14700) loss: 0.924010\n",
      "(Iteration 4284 / 14700) loss: 0.791897\n",
      "(Iteration 4285 / 14700) loss: 0.736791\n",
      "(Iteration 4286 / 14700) loss: 0.729469\n",
      "(Iteration 4287 / 14700) loss: 1.006133\n",
      "(Iteration 4288 / 14700) loss: 0.869819\n",
      "(Iteration 4289 / 14700) loss: 0.864275\n",
      "(Iteration 4290 / 14700) loss: 0.848826\n",
      "(Iteration 4291 / 14700) loss: 0.831851\n",
      "(Iteration 4292 / 14700) loss: 0.835787\n",
      "(Iteration 4293 / 14700) loss: 0.775499\n",
      "(Iteration 4294 / 14700) loss: 0.844470\n",
      "(Iteration 4295 / 14700) loss: 0.896169\n",
      "(Iteration 4296 / 14700) loss: 0.836539\n",
      "(Iteration 4297 / 14700) loss: 0.823331\n",
      "(Iteration 4298 / 14700) loss: 0.876469\n",
      "(Iteration 4299 / 14700) loss: 0.788175\n",
      "(Iteration 4300 / 14700) loss: 0.999614\n",
      "(Iteration 4301 / 14700) loss: 0.854813\n",
      "(Iteration 4302 / 14700) loss: 0.750317\n",
      "(Iteration 4303 / 14700) loss: 0.938238\n",
      "(Iteration 4304 / 14700) loss: 0.835673\n",
      "(Iteration 4305 / 14700) loss: 0.852969\n",
      "(Iteration 4306 / 14700) loss: 0.854789\n",
      "(Iteration 4307 / 14700) loss: 0.891985\n",
      "(Iteration 4308 / 14700) loss: 0.775091\n",
      "(Iteration 4309 / 14700) loss: 0.969131\n",
      "(Iteration 4310 / 14700) loss: 0.850149\n",
      "(Iteration 4311 / 14700) loss: 0.891771\n",
      "(Iteration 4312 / 14700) loss: 0.673215\n",
      "(Iteration 4313 / 14700) loss: 0.892689\n",
      "(Iteration 4314 / 14700) loss: 1.243349\n",
      "(Iteration 4315 / 14700) loss: 0.996257\n",
      "(Iteration 4316 / 14700) loss: 1.200379\n",
      "(Iteration 4317 / 14700) loss: 0.807757\n",
      "(Iteration 4318 / 14700) loss: 0.753977\n",
      "(Iteration 4319 / 14700) loss: 0.814615\n",
      "(Iteration 4320 / 14700) loss: 0.905084\n",
      "(Iteration 4321 / 14700) loss: 1.106329\n",
      "(Iteration 4322 / 14700) loss: 1.180392\n",
      "(Iteration 4323 / 14700) loss: 0.762215\n",
      "(Iteration 4324 / 14700) loss: 0.795061\n",
      "(Iteration 4325 / 14700) loss: 0.781999\n",
      "(Iteration 4326 / 14700) loss: 0.697953\n",
      "(Iteration 4327 / 14700) loss: 0.743593\n",
      "(Iteration 4328 / 14700) loss: 1.045603\n",
      "(Iteration 4329 / 14700) loss: 1.096425\n",
      "(Iteration 4330 / 14700) loss: 0.899464\n",
      "(Iteration 4331 / 14700) loss: 1.050244\n",
      "(Iteration 4332 / 14700) loss: 0.693251\n",
      "(Iteration 4333 / 14700) loss: 0.791864\n",
      "(Iteration 4334 / 14700) loss: 0.846944\n",
      "(Iteration 4335 / 14700) loss: 0.797336\n",
      "(Iteration 4336 / 14700) loss: 0.946442\n",
      "(Iteration 4337 / 14700) loss: 0.716187\n",
      "(Iteration 4338 / 14700) loss: 0.642231\n",
      "(Iteration 4339 / 14700) loss: 1.001560\n",
      "(Iteration 4340 / 14700) loss: 0.981880\n",
      "(Iteration 4341 / 14700) loss: 0.928113\n",
      "(Iteration 4342 / 14700) loss: 0.849767\n",
      "(Iteration 4343 / 14700) loss: 1.016764\n",
      "(Iteration 4344 / 14700) loss: 0.713511\n",
      "(Iteration 4345 / 14700) loss: 0.971811\n",
      "(Iteration 4346 / 14700) loss: 0.838494\n",
      "(Iteration 4347 / 14700) loss: 0.726768\n",
      "(Iteration 4348 / 14700) loss: 0.815024\n",
      "(Iteration 4349 / 14700) loss: 0.780435\n",
      "(Iteration 4350 / 14700) loss: 0.911288\n",
      "(Iteration 4351 / 14700) loss: 0.723990\n",
      "(Iteration 4352 / 14700) loss: 0.807575\n",
      "(Iteration 4353 / 14700) loss: 0.949556\n",
      "(Iteration 4354 / 14700) loss: 1.030315\n",
      "(Iteration 4355 / 14700) loss: 0.904878\n",
      "(Iteration 4356 / 14700) loss: 1.019288\n",
      "(Iteration 4357 / 14700) loss: 0.721587\n",
      "(Iteration 4358 / 14700) loss: 1.116047\n",
      "(Iteration 4359 / 14700) loss: 1.017597\n",
      "(Iteration 4360 / 14700) loss: 0.962080\n",
      "(Iteration 4361 / 14700) loss: 0.592485\n",
      "(Iteration 4362 / 14700) loss: 1.018387\n",
      "(Iteration 4363 / 14700) loss: 0.934792\n",
      "(Iteration 4364 / 14700) loss: 0.747083\n",
      "(Iteration 4365 / 14700) loss: 0.987222\n",
      "(Iteration 4366 / 14700) loss: 0.949556\n",
      "(Iteration 4367 / 14700) loss: 0.646690\n",
      "(Iteration 4368 / 14700) loss: 0.666063\n",
      "(Iteration 4369 / 14700) loss: 1.140066\n",
      "(Iteration 4370 / 14700) loss: 0.805849\n",
      "(Iteration 4371 / 14700) loss: 0.967101\n",
      "(Iteration 4372 / 14700) loss: 0.881184\n",
      "(Iteration 4373 / 14700) loss: 0.868571\n",
      "(Iteration 4374 / 14700) loss: 0.923938\n",
      "(Iteration 4375 / 14700) loss: 0.963061\n",
      "(Iteration 4376 / 14700) loss: 1.122833\n",
      "(Iteration 4377 / 14700) loss: 0.910869\n",
      "(Iteration 4378 / 14700) loss: 1.107744\n",
      "(Iteration 4379 / 14700) loss: 0.673370\n",
      "(Iteration 4380 / 14700) loss: 0.882180\n",
      "(Iteration 4381 / 14700) loss: 0.806147\n",
      "(Iteration 4382 / 14700) loss: 0.803544\n",
      "(Iteration 4383 / 14700) loss: 0.805061\n",
      "(Iteration 4384 / 14700) loss: 0.850830\n",
      "(Iteration 4385 / 14700) loss: 0.965610\n",
      "(Iteration 4386 / 14700) loss: 0.945269\n",
      "(Iteration 4387 / 14700) loss: 0.925297\n",
      "(Iteration 4388 / 14700) loss: 0.657583\n",
      "(Iteration 4389 / 14700) loss: 0.935324\n",
      "(Iteration 4390 / 14700) loss: 0.630518\n",
      "(Iteration 4391 / 14700) loss: 1.040935\n",
      "(Iteration 4392 / 14700) loss: 0.673365\n",
      "(Iteration 4393 / 14700) loss: 1.103285\n",
      "(Iteration 4394 / 14700) loss: 0.939286\n",
      "(Iteration 4395 / 14700) loss: 1.286494\n",
      "(Iteration 4396 / 14700) loss: 0.735798\n",
      "(Iteration 4397 / 14700) loss: 1.099883\n",
      "(Iteration 4398 / 14700) loss: 1.050943\n",
      "(Iteration 4399 / 14700) loss: 0.882346\n",
      "(Iteration 4400 / 14700) loss: 0.471288\n",
      "(Iteration 4401 / 14700) loss: 0.660939\n",
      "(Iteration 4402 / 14700) loss: 1.052738\n",
      "(Iteration 4403 / 14700) loss: 0.787022\n",
      "(Iteration 4404 / 14700) loss: 0.869831\n",
      "(Iteration 4405 / 14700) loss: 0.657713\n",
      "(Iteration 4406 / 14700) loss: 0.828020\n",
      "(Iteration 4407 / 14700) loss: 0.739618\n",
      "(Iteration 4408 / 14700) loss: 0.838062\n",
      "(Iteration 4409 / 14700) loss: 0.799722\n",
      "(Iteration 4410 / 14700) loss: 1.089139\n",
      "(Iteration 4411 / 14700) loss: 1.090407\n",
      "(Iteration 4412 / 14700) loss: 0.727280\n",
      "(Iteration 4413 / 14700) loss: 0.949055\n",
      "(Iteration 4414 / 14700) loss: 1.079816\n",
      "(Iteration 4415 / 14700) loss: 0.899023\n",
      "(Iteration 4416 / 14700) loss: 0.869812\n",
      "(Iteration 4417 / 14700) loss: 1.255923\n",
      "(Iteration 4418 / 14700) loss: 0.860475\n",
      "(Iteration 4419 / 14700) loss: 0.922743\n",
      "(Iteration 4420 / 14700) loss: 0.804436\n",
      "(Iteration 4421 / 14700) loss: 0.737260\n",
      "(Iteration 4422 / 14700) loss: 0.767034\n",
      "(Iteration 4423 / 14700) loss: 0.800642\n",
      "(Iteration 4424 / 14700) loss: 0.845827\n",
      "(Iteration 4425 / 14700) loss: 0.826900\n",
      "(Iteration 4426 / 14700) loss: 1.108226\n",
      "(Iteration 4427 / 14700) loss: 0.838857\n",
      "(Iteration 4428 / 14700) loss: 0.845038\n",
      "(Iteration 4429 / 14700) loss: 0.742062\n",
      "(Iteration 4430 / 14700) loss: 0.650407\n",
      "(Iteration 4431 / 14700) loss: 0.770014\n",
      "(Iteration 4432 / 14700) loss: 0.691591\n",
      "(Iteration 4433 / 14700) loss: 1.005304\n",
      "(Iteration 4434 / 14700) loss: 0.851110\n",
      "(Iteration 4435 / 14700) loss: 0.800438\n",
      "(Iteration 4436 / 14700) loss: 0.631478\n",
      "(Iteration 4437 / 14700) loss: 0.799068\n",
      "(Iteration 4438 / 14700) loss: 0.800518\n",
      "(Iteration 4439 / 14700) loss: 0.773078\n",
      "(Iteration 4440 / 14700) loss: 0.674413\n",
      "(Iteration 4441 / 14700) loss: 0.840183\n",
      "(Iteration 4442 / 14700) loss: 0.972119\n",
      "(Iteration 4443 / 14700) loss: 0.719538\n",
      "(Iteration 4444 / 14700) loss: 0.886565\n",
      "(Iteration 4445 / 14700) loss: 0.827111\n",
      "(Iteration 4446 / 14700) loss: 0.807227\n",
      "(Iteration 4447 / 14700) loss: 0.664345\n",
      "(Iteration 4448 / 14700) loss: 0.839740\n",
      "(Iteration 4449 / 14700) loss: 0.729481\n",
      "(Iteration 4450 / 14700) loss: 1.076437\n",
      "(Iteration 4451 / 14700) loss: 0.675510\n",
      "(Iteration 4452 / 14700) loss: 0.632017\n",
      "(Iteration 4453 / 14700) loss: 1.164856\n",
      "(Iteration 4454 / 14700) loss: 0.780315\n",
      "(Iteration 4455 / 14700) loss: 0.773050\n",
      "(Iteration 4456 / 14700) loss: 0.840147\n",
      "(Iteration 4457 / 14700) loss: 0.725875\n",
      "(Iteration 4458 / 14700) loss: 0.989704\n",
      "(Iteration 4459 / 14700) loss: 0.828852\n",
      "(Iteration 4460 / 14700) loss: 0.992709\n",
      "(Iteration 4461 / 14700) loss: 0.797786\n",
      "(Iteration 4462 / 14700) loss: 0.785363\n",
      "(Iteration 4463 / 14700) loss: 0.885470\n",
      "(Iteration 4464 / 14700) loss: 0.769913\n",
      "(Iteration 4465 / 14700) loss: 0.756978\n",
      "(Iteration 4466 / 14700) loss: 0.726541\n",
      "(Iteration 4467 / 14700) loss: 0.741044\n",
      "(Iteration 4468 / 14700) loss: 0.908724\n",
      "(Iteration 4469 / 14700) loss: 0.819764\n",
      "(Iteration 4470 / 14700) loss: 0.611972\n",
      "(Iteration 4471 / 14700) loss: 0.826963\n",
      "(Iteration 4472 / 14700) loss: 0.767250\n",
      "(Iteration 4473 / 14700) loss: 0.771629\n",
      "(Iteration 4474 / 14700) loss: 0.891649\n",
      "(Iteration 4475 / 14700) loss: 1.076688\n",
      "(Iteration 4476 / 14700) loss: 0.730864\n",
      "(Iteration 4477 / 14700) loss: 0.953551\n",
      "(Iteration 4478 / 14700) loss: 0.821721\n",
      "(Iteration 4479 / 14700) loss: 0.548197\n",
      "(Iteration 4480 / 14700) loss: 0.839931\n",
      "(Iteration 4481 / 14700) loss: 0.896322\n",
      "(Iteration 4482 / 14700) loss: 0.777218\n",
      "(Iteration 4483 / 14700) loss: 0.924026\n",
      "(Iteration 4484 / 14700) loss: 0.928838\n",
      "(Iteration 4485 / 14700) loss: 0.865450\n",
      "(Iteration 4486 / 14700) loss: 0.704701\n",
      "(Iteration 4487 / 14700) loss: 0.980607\n",
      "(Iteration 4488 / 14700) loss: 1.045585\n",
      "(Iteration 4489 / 14700) loss: 0.716347\n",
      "(Iteration 4490 / 14700) loss: 0.943110\n",
      "(Iteration 4491 / 14700) loss: 1.010423\n",
      "(Iteration 4492 / 14700) loss: 0.799156\n",
      "(Iteration 4493 / 14700) loss: 0.848002\n",
      "(Iteration 4494 / 14700) loss: 0.827146\n",
      "(Iteration 4495 / 14700) loss: 0.699837\n",
      "(Iteration 4496 / 14700) loss: 0.856235\n",
      "(Iteration 4497 / 14700) loss: 1.102067\n",
      "(Iteration 4498 / 14700) loss: 0.890873\n",
      "(Iteration 4499 / 14700) loss: 0.697483\n",
      "(Iteration 4500 / 14700) loss: 1.098142\n",
      "(Iteration 4501 / 14700) loss: 0.842555\n",
      "(Iteration 4502 / 14700) loss: 0.600522\n",
      "(Iteration 4503 / 14700) loss: 0.891762\n",
      "(Iteration 4504 / 14700) loss: 0.908215\n",
      "(Iteration 4505 / 14700) loss: 0.862355\n",
      "(Iteration 4506 / 14700) loss: 1.142539\n",
      "(Iteration 4507 / 14700) loss: 0.816003\n",
      "(Iteration 4508 / 14700) loss: 0.851065\n",
      "(Iteration 4509 / 14700) loss: 0.666797\n",
      "(Iteration 4510 / 14700) loss: 0.885140\n",
      "(Iteration 4511 / 14700) loss: 0.628380\n",
      "(Iteration 4512 / 14700) loss: 0.860646\n",
      "(Iteration 4513 / 14700) loss: 0.686295\n",
      "(Iteration 4514 / 14700) loss: 0.706481\n",
      "(Iteration 4515 / 14700) loss: 1.023321\n",
      "(Iteration 4516 / 14700) loss: 0.858348\n",
      "(Iteration 4517 / 14700) loss: 1.068303\n",
      "(Iteration 4518 / 14700) loss: 0.703971\n",
      "(Iteration 4519 / 14700) loss: 0.938994\n",
      "(Iteration 4520 / 14700) loss: 0.772072\n",
      "(Iteration 4521 / 14700) loss: 0.809320\n",
      "(Iteration 4522 / 14700) loss: 1.017383\n",
      "(Iteration 4523 / 14700) loss: 1.051116\n",
      "(Iteration 4524 / 14700) loss: 0.820549\n",
      "(Iteration 4525 / 14700) loss: 0.898458\n",
      "(Iteration 4526 / 14700) loss: 0.845056\n",
      "(Iteration 4527 / 14700) loss: 0.722250\n",
      "(Iteration 4528 / 14700) loss: 0.819990\n",
      "(Iteration 4529 / 14700) loss: 0.585394\n",
      "(Iteration 4530 / 14700) loss: 0.894208\n",
      "(Iteration 4531 / 14700) loss: 0.830527\n",
      "(Iteration 4532 / 14700) loss: 1.100910\n",
      "(Iteration 4533 / 14700) loss: 1.158622\n",
      "(Iteration 4534 / 14700) loss: 0.732064\n",
      "(Iteration 4535 / 14700) loss: 0.879653\n",
      "(Iteration 4536 / 14700) loss: 0.955360\n",
      "(Iteration 4537 / 14700) loss: 0.710923\n",
      "(Iteration 4538 / 14700) loss: 0.819204\n",
      "(Iteration 4539 / 14700) loss: 0.801573\n",
      "(Iteration 4540 / 14700) loss: 0.816961\n",
      "(Iteration 4541 / 14700) loss: 1.048732\n",
      "(Iteration 4542 / 14700) loss: 0.899174\n",
      "(Iteration 4543 / 14700) loss: 1.007589\n",
      "(Iteration 4544 / 14700) loss: 0.800833\n",
      "(Iteration 4545 / 14700) loss: 0.810864\n",
      "(Iteration 4546 / 14700) loss: 0.800741\n",
      "(Iteration 4547 / 14700) loss: 0.738287\n",
      "(Iteration 4548 / 14700) loss: 0.906051\n",
      "(Iteration 4549 / 14700) loss: 0.853369\n",
      "(Iteration 4550 / 14700) loss: 0.957949\n",
      "(Iteration 4551 / 14700) loss: 0.727026\n",
      "(Iteration 4552 / 14700) loss: 0.868876\n",
      "(Iteration 4553 / 14700) loss: 0.662622\n",
      "(Iteration 4554 / 14700) loss: 0.948591\n",
      "(Iteration 4555 / 14700) loss: 0.882044\n",
      "(Iteration 4556 / 14700) loss: 0.911314\n",
      "(Iteration 4557 / 14700) loss: 0.802550\n",
      "(Iteration 4558 / 14700) loss: 0.853332\n",
      "(Iteration 4559 / 14700) loss: 0.883246\n",
      "(Iteration 4560 / 14700) loss: 1.016717\n",
      "(Iteration 4561 / 14700) loss: 0.800354\n",
      "(Iteration 4562 / 14700) loss: 0.819593\n",
      "(Iteration 4563 / 14700) loss: 1.159046\n",
      "(Iteration 4564 / 14700) loss: 1.048062\n",
      "(Iteration 4565 / 14700) loss: 0.749307\n",
      "(Iteration 4566 / 14700) loss: 0.777579\n",
      "(Iteration 4567 / 14700) loss: 0.863158\n",
      "(Iteration 4568 / 14700) loss: 1.023191\n",
      "(Iteration 4569 / 14700) loss: 0.952116\n",
      "(Iteration 4570 / 14700) loss: 0.866357\n",
      "(Iteration 4571 / 14700) loss: 0.677683\n",
      "(Iteration 4572 / 14700) loss: 0.983607\n",
      "(Iteration 4573 / 14700) loss: 0.752161\n",
      "(Iteration 4574 / 14700) loss: 0.844122\n",
      "(Iteration 4575 / 14700) loss: 0.790245\n",
      "(Iteration 4576 / 14700) loss: 0.780445\n",
      "(Iteration 4577 / 14700) loss: 0.807143\n",
      "(Iteration 4578 / 14700) loss: 0.690894\n",
      "(Iteration 4579 / 14700) loss: 0.991055\n",
      "(Iteration 4580 / 14700) loss: 0.822334\n",
      "(Iteration 4581 / 14700) loss: 0.882012\n",
      "(Iteration 4582 / 14700) loss: 0.602552\n",
      "(Iteration 4583 / 14700) loss: 0.716268\n",
      "(Iteration 4584 / 14700) loss: 0.950690\n",
      "(Iteration 4585 / 14700) loss: 0.961734\n",
      "(Iteration 4586 / 14700) loss: 1.129324\n",
      "(Iteration 4587 / 14700) loss: 1.036367\n",
      "(Iteration 4588 / 14700) loss: 0.662036\n",
      "(Iteration 4589 / 14700) loss: 0.830451\n",
      "(Iteration 4590 / 14700) loss: 0.823106\n",
      "(Iteration 4591 / 14700) loss: 0.536910\n",
      "(Iteration 4592 / 14700) loss: 0.939561\n",
      "(Iteration 4593 / 14700) loss: 0.789039\n",
      "(Iteration 4594 / 14700) loss: 0.544895\n",
      "(Iteration 4595 / 14700) loss: 0.655579\n",
      "(Iteration 4596 / 14700) loss: 0.836284\n",
      "(Iteration 4597 / 14700) loss: 0.924557\n",
      "(Iteration 4598 / 14700) loss: 0.916021\n",
      "(Iteration 4599 / 14700) loss: 0.979436\n",
      "(Iteration 4600 / 14700) loss: 0.645483\n",
      "(Iteration 4601 / 14700) loss: 0.741521\n",
      "(Iteration 4602 / 14700) loss: 0.901961\n",
      "(Iteration 4603 / 14700) loss: 0.856126\n",
      "(Iteration 4604 / 14700) loss: 0.760363\n",
      "(Iteration 4605 / 14700) loss: 1.098803\n",
      "(Iteration 4606 / 14700) loss: 0.685217\n",
      "(Iteration 4607 / 14700) loss: 0.877874\n",
      "(Iteration 4608 / 14700) loss: 0.934343\n",
      "(Iteration 4609 / 14700) loss: 0.828068\n",
      "(Iteration 4610 / 14700) loss: 0.783946\n",
      "(Iteration 4611 / 14700) loss: 0.915735\n",
      "(Iteration 4612 / 14700) loss: 0.932065\n",
      "(Iteration 4613 / 14700) loss: 0.754065\n",
      "(Iteration 4614 / 14700) loss: 0.940521\n",
      "(Iteration 4615 / 14700) loss: 0.964444\n",
      "(Iteration 4616 / 14700) loss: 0.727911\n",
      "(Iteration 4617 / 14700) loss: 0.777151\n",
      "(Iteration 4618 / 14700) loss: 0.808484\n",
      "(Iteration 4619 / 14700) loss: 0.810455\n",
      "(Iteration 4620 / 14700) loss: 0.865952\n",
      "(Iteration 4621 / 14700) loss: 0.846168\n",
      "(Iteration 4622 / 14700) loss: 0.782241\n",
      "(Iteration 4623 / 14700) loss: 0.744311\n",
      "(Iteration 4624 / 14700) loss: 0.796419\n",
      "(Iteration 4625 / 14700) loss: 0.846761\n",
      "(Iteration 4626 / 14700) loss: 1.091430\n",
      "(Iteration 4627 / 14700) loss: 0.772541\n",
      "(Iteration 4628 / 14700) loss: 1.117996\n",
      "(Iteration 4629 / 14700) loss: 0.725183\n",
      "(Iteration 4630 / 14700) loss: 1.117756\n",
      "(Iteration 4631 / 14700) loss: 0.892238\n",
      "(Iteration 4632 / 14700) loss: 0.953977\n",
      "(Iteration 4633 / 14700) loss: 0.761282\n",
      "(Iteration 4634 / 14700) loss: 0.914559\n",
      "(Iteration 4635 / 14700) loss: 0.746271\n",
      "(Iteration 4636 / 14700) loss: 0.926570\n",
      "(Iteration 4637 / 14700) loss: 0.714418\n",
      "(Iteration 4638 / 14700) loss: 0.663512\n",
      "(Iteration 4639 / 14700) loss: 0.890784\n",
      "(Iteration 4640 / 14700) loss: 1.068822\n",
      "(Iteration 4641 / 14700) loss: 0.661730\n",
      "(Iteration 4642 / 14700) loss: 0.680294\n",
      "(Iteration 4643 / 14700) loss: 0.881360\n",
      "(Iteration 4644 / 14700) loss: 0.872015\n",
      "(Iteration 4645 / 14700) loss: 0.979228\n",
      "(Iteration 4646 / 14700) loss: 0.664025\n",
      "(Iteration 4647 / 14700) loss: 0.864301\n",
      "(Iteration 4648 / 14700) loss: 0.896909\n",
      "(Iteration 4649 / 14700) loss: 0.650052\n",
      "(Iteration 4650 / 14700) loss: 1.008604\n",
      "(Iteration 4651 / 14700) loss: 0.798260\n",
      "(Iteration 4652 / 14700) loss: 0.779878\n",
      "(Iteration 4653 / 14700) loss: 0.853284\n",
      "(Iteration 4654 / 14700) loss: 1.102016\n",
      "(Iteration 4655 / 14700) loss: 0.573813\n",
      "(Iteration 4656 / 14700) loss: 0.624715\n",
      "(Iteration 4657 / 14700) loss: 0.737813\n",
      "(Iteration 4658 / 14700) loss: 0.869591\n",
      "(Iteration 4659 / 14700) loss: 0.905408\n",
      "(Iteration 4660 / 14700) loss: 0.826629\n",
      "(Iteration 4661 / 14700) loss: 0.870729\n",
      "(Iteration 4662 / 14700) loss: 0.730884\n",
      "(Iteration 4663 / 14700) loss: 0.689709\n",
      "(Iteration 4664 / 14700) loss: 0.724644\n",
      "(Iteration 4665 / 14700) loss: 0.780017\n",
      "(Iteration 4666 / 14700) loss: 0.698114\n",
      "(Iteration 4667 / 14700) loss: 0.841637\n",
      "(Iteration 4668 / 14700) loss: 0.585154\n",
      "(Iteration 4669 / 14700) loss: 0.965135\n",
      "(Iteration 4670 / 14700) loss: 0.956491\n",
      "(Iteration 4671 / 14700) loss: 0.775845\n",
      "(Iteration 4672 / 14700) loss: 1.158158\n",
      "(Iteration 4673 / 14700) loss: 0.626831\n",
      "(Iteration 4674 / 14700) loss: 0.951196\n",
      "(Iteration 4675 / 14700) loss: 0.724793\n",
      "(Iteration 4676 / 14700) loss: 0.753257\n",
      "(Iteration 4677 / 14700) loss: 0.835504\n",
      "(Iteration 4678 / 14700) loss: 0.649976\n",
      "(Iteration 4679 / 14700) loss: 0.774457\n",
      "(Iteration 4680 / 14700) loss: 0.933665\n",
      "(Iteration 4681 / 14700) loss: 0.732726\n",
      "(Iteration 4682 / 14700) loss: 0.782448\n",
      "(Iteration 4683 / 14700) loss: 0.859399\n",
      "(Iteration 4684 / 14700) loss: 0.690438\n",
      "(Iteration 4685 / 14700) loss: 0.730131\n",
      "(Iteration 4686 / 14700) loss: 0.844290\n",
      "(Iteration 4687 / 14700) loss: 0.880567\n",
      "(Iteration 4688 / 14700) loss: 0.914866\n",
      "(Iteration 4689 / 14700) loss: 0.949446\n",
      "(Iteration 4690 / 14700) loss: 0.793543\n",
      "(Iteration 4691 / 14700) loss: 1.032266\n",
      "(Iteration 4692 / 14700) loss: 0.830611\n",
      "(Iteration 4693 / 14700) loss: 1.033328\n",
      "(Iteration 4694 / 14700) loss: 0.695608\n",
      "(Iteration 4695 / 14700) loss: 0.802935\n",
      "(Iteration 4696 / 14700) loss: 0.603612\n",
      "(Iteration 4697 / 14700) loss: 0.727181\n",
      "(Iteration 4698 / 14700) loss: 0.847382\n",
      "(Iteration 4699 / 14700) loss: 0.789942\n",
      "(Iteration 4700 / 14700) loss: 1.067939\n",
      "(Iteration 4701 / 14700) loss: 0.714264\n",
      "(Iteration 4702 / 14700) loss: 0.515517\n",
      "(Iteration 4703 / 14700) loss: 0.915047\n",
      "(Iteration 4704 / 14700) loss: 0.833374\n",
      "(Iteration 4705 / 14700) loss: 0.587057\n",
      "(Iteration 4706 / 14700) loss: 0.843498\n",
      "(Iteration 4707 / 14700) loss: 0.747994\n",
      "(Iteration 4708 / 14700) loss: 0.832878\n",
      "(Iteration 4709 / 14700) loss: 0.952766\n",
      "(Iteration 4710 / 14700) loss: 1.065033\n",
      "(Iteration 4711 / 14700) loss: 0.741302\n",
      "(Iteration 4712 / 14700) loss: 0.896412\n",
      "(Iteration 4713 / 14700) loss: 0.753355\n",
      "(Iteration 4714 / 14700) loss: 0.637431\n",
      "(Iteration 4715 / 14700) loss: 0.922765\n",
      "(Iteration 4716 / 14700) loss: 1.090446\n",
      "(Iteration 4717 / 14700) loss: 1.021299\n",
      "(Iteration 4718 / 14700) loss: 0.849788\n",
      "(Iteration 4719 / 14700) loss: 0.916737\n",
      "(Iteration 4720 / 14700) loss: 0.813805\n",
      "(Iteration 4721 / 14700) loss: 0.909684\n",
      "(Iteration 4722 / 14700) loss: 0.670299\n",
      "(Iteration 4723 / 14700) loss: 1.036713\n",
      "(Iteration 4724 / 14700) loss: 0.897860\n",
      "(Iteration 4725 / 14700) loss: 0.795163\n",
      "(Iteration 4726 / 14700) loss: 1.080833\n",
      "(Iteration 4727 / 14700) loss: 0.728110\n",
      "(Iteration 4728 / 14700) loss: 0.888698\n",
      "(Iteration 4729 / 14700) loss: 0.648077\n",
      "(Iteration 4730 / 14700) loss: 0.688809\n",
      "(Iteration 4731 / 14700) loss: 0.898416\n",
      "(Iteration 4732 / 14700) loss: 0.720164\n",
      "(Iteration 4733 / 14700) loss: 0.899178\n",
      "(Iteration 4734 / 14700) loss: 0.985192\n",
      "(Iteration 4735 / 14700) loss: 0.914418\n",
      "(Iteration 4736 / 14700) loss: 0.844013\n",
      "(Iteration 4737 / 14700) loss: 0.596221\n",
      "(Iteration 4738 / 14700) loss: 0.656228\n",
      "(Iteration 4739 / 14700) loss: 0.849974\n",
      "(Iteration 4740 / 14700) loss: 0.670354\n",
      "(Iteration 4741 / 14700) loss: 0.969404\n",
      "(Iteration 4742 / 14700) loss: 0.796838\n",
      "(Iteration 4743 / 14700) loss: 0.780917\n",
      "(Iteration 4744 / 14700) loss: 1.184631\n",
      "(Iteration 4745 / 14700) loss: 0.872482\n",
      "(Iteration 4746 / 14700) loss: 0.812360\n",
      "(Iteration 4747 / 14700) loss: 0.679037\n",
      "(Iteration 4748 / 14700) loss: 1.014662\n",
      "(Iteration 4749 / 14700) loss: 0.738055\n",
      "(Iteration 4750 / 14700) loss: 0.868911\n",
      "(Iteration 4751 / 14700) loss: 0.863838\n",
      "(Iteration 4752 / 14700) loss: 1.181801\n",
      "(Iteration 4753 / 14700) loss: 0.699787\n",
      "(Iteration 4754 / 14700) loss: 0.670306\n",
      "(Iteration 4755 / 14700) loss: 1.141247\n",
      "(Iteration 4756 / 14700) loss: 0.732502\n",
      "(Iteration 4757 / 14700) loss: 1.044179\n",
      "(Iteration 4758 / 14700) loss: 0.637641\n",
      "(Iteration 4759 / 14700) loss: 0.952835\n",
      "(Iteration 4760 / 14700) loss: 1.016468\n",
      "(Iteration 4761 / 14700) loss: 0.686324\n",
      "(Iteration 4762 / 14700) loss: 0.683249\n",
      "(Iteration 4763 / 14700) loss: 0.783832\n",
      "(Iteration 4764 / 14700) loss: 1.065838\n",
      "(Iteration 4765 / 14700) loss: 0.960710\n",
      "(Iteration 4766 / 14700) loss: 0.742697\n",
      "(Iteration 4767 / 14700) loss: 0.726275\n",
      "(Iteration 4768 / 14700) loss: 0.985795\n",
      "(Iteration 4769 / 14700) loss: 0.879392\n",
      "(Iteration 4770 / 14700) loss: 0.738463\n",
      "(Iteration 4771 / 14700) loss: 0.754582\n",
      "(Iteration 4772 / 14700) loss: 1.018419\n",
      "(Iteration 4773 / 14700) loss: 1.085624\n",
      "(Iteration 4774 / 14700) loss: 0.916350\n",
      "(Iteration 4775 / 14700) loss: 1.002397\n",
      "(Iteration 4776 / 14700) loss: 1.202819\n",
      "(Iteration 4777 / 14700) loss: 1.095910\n",
      "(Iteration 4778 / 14700) loss: 0.760335\n",
      "(Iteration 4779 / 14700) loss: 0.741261\n",
      "(Iteration 4780 / 14700) loss: 1.103432\n",
      "(Iteration 4781 / 14700) loss: 0.889415\n",
      "(Iteration 4782 / 14700) loss: 0.806453\n",
      "(Iteration 4783 / 14700) loss: 1.054767\n",
      "(Iteration 4784 / 14700) loss: 0.525485\n",
      "(Iteration 4785 / 14700) loss: 0.807499\n",
      "(Iteration 4786 / 14700) loss: 1.141616\n",
      "(Iteration 4787 / 14700) loss: 0.886352\n",
      "(Iteration 4788 / 14700) loss: 0.668056\n",
      "(Iteration 4789 / 14700) loss: 0.778535\n",
      "(Iteration 4790 / 14700) loss: 0.939727\n",
      "(Iteration 4791 / 14700) loss: 0.769160\n",
      "(Iteration 4792 / 14700) loss: 0.971345\n",
      "(Iteration 4793 / 14700) loss: 0.936614\n",
      "(Iteration 4794 / 14700) loss: 0.861339\n",
      "(Iteration 4795 / 14700) loss: 0.879306\n",
      "(Iteration 4796 / 14700) loss: 0.829321\n",
      "(Iteration 4797 / 14700) loss: 0.765312\n",
      "(Iteration 4798 / 14700) loss: 1.061376\n",
      "(Iteration 4799 / 14700) loss: 0.888511\n",
      "(Iteration 4800 / 14700) loss: 0.868400\n",
      "(Iteration 4801 / 14700) loss: 0.815344\n",
      "(Iteration 4802 / 14700) loss: 0.539653\n",
      "(Iteration 4803 / 14700) loss: 0.950203\n",
      "(Iteration 4804 / 14700) loss: 0.906572\n",
      "(Iteration 4805 / 14700) loss: 0.709917\n",
      "(Iteration 4806 / 14700) loss: 0.802153\n",
      "(Iteration 4807 / 14700) loss: 0.683957\n",
      "(Iteration 4808 / 14700) loss: 1.026089\n",
      "(Iteration 4809 / 14700) loss: 0.841267\n",
      "(Iteration 4810 / 14700) loss: 1.019026\n",
      "(Iteration 4811 / 14700) loss: 0.743393\n",
      "(Iteration 4812 / 14700) loss: 0.950886\n",
      "(Iteration 4813 / 14700) loss: 1.207880\n",
      "(Iteration 4814 / 14700) loss: 0.704944\n",
      "(Iteration 4815 / 14700) loss: 0.690139\n",
      "(Iteration 4816 / 14700) loss: 0.523066\n",
      "(Iteration 4817 / 14700) loss: 0.718838\n",
      "(Iteration 4818 / 14700) loss: 0.792035\n",
      "(Iteration 4819 / 14700) loss: 0.848577\n",
      "(Iteration 4820 / 14700) loss: 0.775135\n",
      "(Iteration 4821 / 14700) loss: 0.818058\n",
      "(Iteration 4822 / 14700) loss: 0.693983\n",
      "(Iteration 4823 / 14700) loss: 0.765173\n",
      "(Iteration 4824 / 14700) loss: 0.777277\n",
      "(Iteration 4825 / 14700) loss: 1.005943\n",
      "(Iteration 4826 / 14700) loss: 1.021131\n",
      "(Iteration 4827 / 14700) loss: 0.686962\n",
      "(Iteration 4828 / 14700) loss: 0.840861\n",
      "(Iteration 4829 / 14700) loss: 1.097987\n",
      "(Iteration 4830 / 14700) loss: 1.042656\n",
      "(Iteration 4831 / 14700) loss: 0.743027\n",
      "(Iteration 4832 / 14700) loss: 0.676230\n",
      "(Iteration 4833 / 14700) loss: 0.768824\n",
      "(Iteration 4834 / 14700) loss: 1.095384\n",
      "(Iteration 4835 / 14700) loss: 0.680992\n",
      "(Iteration 4836 / 14700) loss: 1.003528\n",
      "(Iteration 4837 / 14700) loss: 0.906563\n",
      "(Iteration 4838 / 14700) loss: 0.812196\n",
      "(Iteration 4839 / 14700) loss: 0.747652\n",
      "(Iteration 4840 / 14700) loss: 1.364139\n",
      "(Iteration 4841 / 14700) loss: 0.812235\n",
      "(Iteration 4842 / 14700) loss: 0.750090\n",
      "(Iteration 4843 / 14700) loss: 1.070444\n",
      "(Iteration 4844 / 14700) loss: 0.811401\n",
      "(Iteration 4845 / 14700) loss: 0.893601\n",
      "(Iteration 4846 / 14700) loss: 0.754735\n",
      "(Iteration 4847 / 14700) loss: 0.899801\n",
      "(Iteration 4848 / 14700) loss: 1.012755\n",
      "(Iteration 4849 / 14700) loss: 0.968626\n",
      "(Iteration 4850 / 14700) loss: 0.858901\n",
      "(Iteration 4851 / 14700) loss: 0.993236\n",
      "(Iteration 4852 / 14700) loss: 0.879516\n",
      "(Iteration 4853 / 14700) loss: 0.881411\n",
      "(Iteration 4854 / 14700) loss: 0.754921\n",
      "(Iteration 4855 / 14700) loss: 1.022949\n",
      "(Iteration 4856 / 14700) loss: 0.827285\n",
      "(Iteration 4857 / 14700) loss: 0.947031\n",
      "(Iteration 4858 / 14700) loss: 0.720760\n",
      "(Iteration 4859 / 14700) loss: 0.937053\n",
      "(Iteration 4860 / 14700) loss: 0.758197\n",
      "(Iteration 4861 / 14700) loss: 0.755928\n",
      "(Iteration 4862 / 14700) loss: 1.078143\n",
      "(Iteration 4863 / 14700) loss: 0.926436\n",
      "(Iteration 4864 / 14700) loss: 1.169885\n",
      "(Iteration 4865 / 14700) loss: 0.622819\n",
      "(Iteration 4866 / 14700) loss: 0.754642\n",
      "(Iteration 4867 / 14700) loss: 0.757399\n",
      "(Iteration 4868 / 14700) loss: 0.746896\n",
      "(Iteration 4869 / 14700) loss: 1.196469\n",
      "(Iteration 4870 / 14700) loss: 0.860891\n",
      "(Iteration 4871 / 14700) loss: 0.782940\n",
      "(Iteration 4872 / 14700) loss: 0.994477\n",
      "(Iteration 4873 / 14700) loss: 0.688846\n",
      "(Iteration 4874 / 14700) loss: 0.846460\n",
      "(Iteration 4875 / 14700) loss: 0.762488\n",
      "(Iteration 4876 / 14700) loss: 1.033386\n",
      "(Iteration 4877 / 14700) loss: 0.880424\n",
      "(Iteration 4878 / 14700) loss: 0.785645\n",
      "(Iteration 4879 / 14700) loss: 1.311141\n",
      "(Iteration 4880 / 14700) loss: 0.717122\n",
      "(Iteration 4881 / 14700) loss: 0.768526\n",
      "(Iteration 4882 / 14700) loss: 0.764047\n",
      "(Iteration 4883 / 14700) loss: 0.780374\n",
      "(Iteration 4884 / 14700) loss: 0.745181\n",
      "(Iteration 4885 / 14700) loss: 0.731431\n",
      "(Iteration 4886 / 14700) loss: 0.967383\n",
      "(Iteration 4887 / 14700) loss: 1.048266\n",
      "(Iteration 4888 / 14700) loss: 0.521941\n",
      "(Iteration 4889 / 14700) loss: 0.770898\n",
      "(Iteration 4890 / 14700) loss: 0.700169\n",
      "(Iteration 4891 / 14700) loss: 1.001962\n",
      "(Iteration 4892 / 14700) loss: 0.882219\n",
      "(Iteration 4893 / 14700) loss: 0.677019\n",
      "(Iteration 4894 / 14700) loss: 0.877823\n",
      "(Iteration 4895 / 14700) loss: 0.817944\n",
      "(Iteration 4896 / 14700) loss: 0.915505\n",
      "(Iteration 4897 / 14700) loss: 0.717255\n",
      "(Iteration 4898 / 14700) loss: 0.750592\n",
      "(Iteration 4899 / 14700) loss: 0.856575\n",
      "(Iteration 4900 / 14700) loss: 0.772053\n",
      "(Epoch 5 / 15) train acc: 0.777000; val_acc: 0.754000\n",
      "(Iteration 4901 / 14700) loss: 0.959693\n",
      "(Iteration 4902 / 14700) loss: 0.645307\n",
      "(Iteration 4903 / 14700) loss: 0.771345\n",
      "(Iteration 4904 / 14700) loss: 1.018395\n",
      "(Iteration 4905 / 14700) loss: 0.794934\n",
      "(Iteration 4906 / 14700) loss: 0.774251\n",
      "(Iteration 4907 / 14700) loss: 0.734860\n",
      "(Iteration 4908 / 14700) loss: 0.713235\n",
      "(Iteration 4909 / 14700) loss: 0.855712\n",
      "(Iteration 4910 / 14700) loss: 0.748109\n",
      "(Iteration 4911 / 14700) loss: 0.866621\n",
      "(Iteration 4912 / 14700) loss: 0.882951\n",
      "(Iteration 4913 / 14700) loss: 1.121593\n",
      "(Iteration 4914 / 14700) loss: 0.732844\n",
      "(Iteration 4915 / 14700) loss: 0.814806\n",
      "(Iteration 4916 / 14700) loss: 0.673836\n",
      "(Iteration 4917 / 14700) loss: 0.704595\n",
      "(Iteration 4918 / 14700) loss: 0.802737\n",
      "(Iteration 4919 / 14700) loss: 1.033677\n",
      "(Iteration 4920 / 14700) loss: 0.580107\n",
      "(Iteration 4921 / 14700) loss: 1.156340\n",
      "(Iteration 4922 / 14700) loss: 0.807904\n",
      "(Iteration 4923 / 14700) loss: 0.983321\n",
      "(Iteration 4924 / 14700) loss: 0.821174\n",
      "(Iteration 4925 / 14700) loss: 0.679515\n",
      "(Iteration 4926 / 14700) loss: 0.963146\n",
      "(Iteration 4927 / 14700) loss: 0.754690\n",
      "(Iteration 4928 / 14700) loss: 0.911817\n",
      "(Iteration 4929 / 14700) loss: 0.976392\n",
      "(Iteration 4930 / 14700) loss: 0.834575\n",
      "(Iteration 4931 / 14700) loss: 0.709009\n",
      "(Iteration 4932 / 14700) loss: 1.033723\n",
      "(Iteration 4933 / 14700) loss: 0.884547\n",
      "(Iteration 4934 / 14700) loss: 0.920694\n",
      "(Iteration 4935 / 14700) loss: 0.862920\n",
      "(Iteration 4936 / 14700) loss: 0.785168\n",
      "(Iteration 4937 / 14700) loss: 0.728290\n",
      "(Iteration 4938 / 14700) loss: 0.963420\n",
      "(Iteration 4939 / 14700) loss: 1.253770\n",
      "(Iteration 4940 / 14700) loss: 0.783054\n",
      "(Iteration 4941 / 14700) loss: 0.922460\n",
      "(Iteration 4942 / 14700) loss: 0.991446\n",
      "(Iteration 4943 / 14700) loss: 0.848398\n",
      "(Iteration 4944 / 14700) loss: 0.762921\n",
      "(Iteration 4945 / 14700) loss: 1.088595\n",
      "(Iteration 4946 / 14700) loss: 0.650320\n",
      "(Iteration 4947 / 14700) loss: 0.827741\n",
      "(Iteration 4948 / 14700) loss: 0.622660\n",
      "(Iteration 4949 / 14700) loss: 1.102097\n",
      "(Iteration 4950 / 14700) loss: 0.913036\n",
      "(Iteration 4951 / 14700) loss: 0.921946\n",
      "(Iteration 4952 / 14700) loss: 0.977629\n",
      "(Iteration 4953 / 14700) loss: 0.999623\n",
      "(Iteration 4954 / 14700) loss: 0.985537\n",
      "(Iteration 4955 / 14700) loss: 0.823971\n",
      "(Iteration 4956 / 14700) loss: 0.785608\n",
      "(Iteration 4957 / 14700) loss: 0.728689\n",
      "(Iteration 4958 / 14700) loss: 0.699279\n",
      "(Iteration 4959 / 14700) loss: 0.664115\n",
      "(Iteration 4960 / 14700) loss: 0.881809\n",
      "(Iteration 4961 / 14700) loss: 0.881218\n",
      "(Iteration 4962 / 14700) loss: 0.670062\n",
      "(Iteration 4963 / 14700) loss: 0.635231\n",
      "(Iteration 4964 / 14700) loss: 0.588099\n",
      "(Iteration 4965 / 14700) loss: 1.022608\n",
      "(Iteration 4966 / 14700) loss: 0.844416\n",
      "(Iteration 4967 / 14700) loss: 0.624372\n",
      "(Iteration 4968 / 14700) loss: 0.749966\n",
      "(Iteration 4969 / 14700) loss: 0.876318\n",
      "(Iteration 4970 / 14700) loss: 0.852609\n",
      "(Iteration 4971 / 14700) loss: 0.836786\n",
      "(Iteration 4972 / 14700) loss: 1.037131\n",
      "(Iteration 4973 / 14700) loss: 0.831243\n",
      "(Iteration 4974 / 14700) loss: 0.691831\n",
      "(Iteration 4975 / 14700) loss: 0.756739\n",
      "(Iteration 4976 / 14700) loss: 0.729677\n",
      "(Iteration 4977 / 14700) loss: 0.938924\n",
      "(Iteration 4978 / 14700) loss: 1.047945\n",
      "(Iteration 4979 / 14700) loss: 0.903655\n",
      "(Iteration 4980 / 14700) loss: 0.917343\n",
      "(Iteration 4981 / 14700) loss: 0.846249\n",
      "(Iteration 4982 / 14700) loss: 0.643864\n",
      "(Iteration 4983 / 14700) loss: 0.718710\n",
      "(Iteration 4984 / 14700) loss: 0.995280\n",
      "(Iteration 4985 / 14700) loss: 0.976606\n",
      "(Iteration 4986 / 14700) loss: 0.808199\n",
      "(Iteration 4987 / 14700) loss: 0.870904\n",
      "(Iteration 4988 / 14700) loss: 1.028188\n",
      "(Iteration 4989 / 14700) loss: 0.808787\n",
      "(Iteration 4990 / 14700) loss: 0.940819\n",
      "(Iteration 4991 / 14700) loss: 0.947302\n",
      "(Iteration 4992 / 14700) loss: 0.739128\n",
      "(Iteration 4993 / 14700) loss: 0.768499\n",
      "(Iteration 4994 / 14700) loss: 0.912768\n",
      "(Iteration 4995 / 14700) loss: 0.853881\n",
      "(Iteration 4996 / 14700) loss: 0.935584\n",
      "(Iteration 4997 / 14700) loss: 0.905505\n",
      "(Iteration 4998 / 14700) loss: 0.706364\n",
      "(Iteration 4999 / 14700) loss: 0.768986\n",
      "(Iteration 5000 / 14700) loss: 1.117866\n",
      "(Iteration 5001 / 14700) loss: 0.771871\n",
      "(Iteration 5002 / 14700) loss: 0.988071\n",
      "(Iteration 5003 / 14700) loss: 0.908430\n",
      "(Iteration 5004 / 14700) loss: 0.935465\n",
      "(Iteration 5005 / 14700) loss: 0.744395\n",
      "(Iteration 5006 / 14700) loss: 0.822513\n",
      "(Iteration 5007 / 14700) loss: 0.870214\n",
      "(Iteration 5008 / 14700) loss: 0.757675\n",
      "(Iteration 5009 / 14700) loss: 0.806237\n",
      "(Iteration 5010 / 14700) loss: 0.983653\n",
      "(Iteration 5011 / 14700) loss: 0.695478\n",
      "(Iteration 5012 / 14700) loss: 0.783646\n",
      "(Iteration 5013 / 14700) loss: 0.700235\n",
      "(Iteration 5014 / 14700) loss: 0.734041\n",
      "(Iteration 5015 / 14700) loss: 0.678015\n",
      "(Iteration 5016 / 14700) loss: 0.676983\n",
      "(Iteration 5017 / 14700) loss: 0.820524\n",
      "(Iteration 5018 / 14700) loss: 1.031700\n",
      "(Iteration 5019 / 14700) loss: 0.685299\n",
      "(Iteration 5020 / 14700) loss: 0.869509\n",
      "(Iteration 5021 / 14700) loss: 0.819806\n",
      "(Iteration 5022 / 14700) loss: 0.962145\n",
      "(Iteration 5023 / 14700) loss: 0.764569\n",
      "(Iteration 5024 / 14700) loss: 1.137499\n",
      "(Iteration 5025 / 14700) loss: 0.977385\n",
      "(Iteration 5026 / 14700) loss: 0.506812\n",
      "(Iteration 5027 / 14700) loss: 0.790161\n",
      "(Iteration 5028 / 14700) loss: 0.880082\n",
      "(Iteration 5029 / 14700) loss: 0.660681\n",
      "(Iteration 5030 / 14700) loss: 0.801216\n",
      "(Iteration 5031 / 14700) loss: 0.735482\n",
      "(Iteration 5032 / 14700) loss: 0.892220\n",
      "(Iteration 5033 / 14700) loss: 0.633692\n",
      "(Iteration 5034 / 14700) loss: 0.748575\n",
      "(Iteration 5035 / 14700) loss: 0.806861\n",
      "(Iteration 5036 / 14700) loss: 0.924104\n",
      "(Iteration 5037 / 14700) loss: 0.838843\n",
      "(Iteration 5038 / 14700) loss: 0.916175\n",
      "(Iteration 5039 / 14700) loss: 0.938996\n",
      "(Iteration 5040 / 14700) loss: 0.672935\n",
      "(Iteration 5041 / 14700) loss: 0.810791\n",
      "(Iteration 5042 / 14700) loss: 0.887104\n",
      "(Iteration 5043 / 14700) loss: 0.879025\n",
      "(Iteration 5044 / 14700) loss: 0.758922\n",
      "(Iteration 5045 / 14700) loss: 0.821351\n",
      "(Iteration 5046 / 14700) loss: 0.779640\n",
      "(Iteration 5047 / 14700) loss: 0.996148\n",
      "(Iteration 5048 / 14700) loss: 0.929443\n",
      "(Iteration 5049 / 14700) loss: 1.143515\n",
      "(Iteration 5050 / 14700) loss: 0.933709\n",
      "(Iteration 5051 / 14700) loss: 0.813588\n",
      "(Iteration 5052 / 14700) loss: 0.729936\n",
      "(Iteration 5053 / 14700) loss: 0.903450\n",
      "(Iteration 5054 / 14700) loss: 0.948613\n",
      "(Iteration 5055 / 14700) loss: 0.688386\n",
      "(Iteration 5056 / 14700) loss: 0.785244\n",
      "(Iteration 5057 / 14700) loss: 1.133783\n",
      "(Iteration 5058 / 14700) loss: 0.911770\n",
      "(Iteration 5059 / 14700) loss: 0.800222\n",
      "(Iteration 5060 / 14700) loss: 0.657256\n",
      "(Iteration 5061 / 14700) loss: 0.689360\n",
      "(Iteration 5062 / 14700) loss: 0.755501\n",
      "(Iteration 5063 / 14700) loss: 0.799679\n",
      "(Iteration 5064 / 14700) loss: 0.895742\n",
      "(Iteration 5065 / 14700) loss: 1.078131\n",
      "(Iteration 5066 / 14700) loss: 0.837710\n",
      "(Iteration 5067 / 14700) loss: 0.685043\n",
      "(Iteration 5068 / 14700) loss: 0.657681\n",
      "(Iteration 5069 / 14700) loss: 0.830465\n",
      "(Iteration 5070 / 14700) loss: 0.940911\n",
      "(Iteration 5071 / 14700) loss: 0.909447\n",
      "(Iteration 5072 / 14700) loss: 1.050413\n",
      "(Iteration 5073 / 14700) loss: 1.076340\n",
      "(Iteration 5074 / 14700) loss: 0.872832\n",
      "(Iteration 5075 / 14700) loss: 1.074373\n",
      "(Iteration 5076 / 14700) loss: 1.039486\n",
      "(Iteration 5077 / 14700) loss: 1.041635\n",
      "(Iteration 5078 / 14700) loss: 0.899917\n",
      "(Iteration 5079 / 14700) loss: 0.923272\n",
      "(Iteration 5080 / 14700) loss: 0.910008\n",
      "(Iteration 5081 / 14700) loss: 0.791487\n",
      "(Iteration 5082 / 14700) loss: 0.673649\n",
      "(Iteration 5083 / 14700) loss: 0.996952\n",
      "(Iteration 5084 / 14700) loss: 0.714827\n",
      "(Iteration 5085 / 14700) loss: 0.781382\n",
      "(Iteration 5086 / 14700) loss: 0.774633\n",
      "(Iteration 5087 / 14700) loss: 0.869140\n",
      "(Iteration 5088 / 14700) loss: 0.842026\n",
      "(Iteration 5089 / 14700) loss: 0.853577\n",
      "(Iteration 5090 / 14700) loss: 0.914457\n",
      "(Iteration 5091 / 14700) loss: 0.726050\n",
      "(Iteration 5092 / 14700) loss: 0.744926\n",
      "(Iteration 5093 / 14700) loss: 1.166164\n",
      "(Iteration 5094 / 14700) loss: 0.885064\n",
      "(Iteration 5095 / 14700) loss: 0.768063\n",
      "(Iteration 5096 / 14700) loss: 0.903175\n",
      "(Iteration 5097 / 14700) loss: 0.746865\n",
      "(Iteration 5098 / 14700) loss: 0.796604\n",
      "(Iteration 5099 / 14700) loss: 0.953845\n",
      "(Iteration 5100 / 14700) loss: 0.847881\n",
      "(Iteration 5101 / 14700) loss: 0.889806\n",
      "(Iteration 5102 / 14700) loss: 0.834529\n",
      "(Iteration 5103 / 14700) loss: 0.807354\n",
      "(Iteration 5104 / 14700) loss: 0.661301\n",
      "(Iteration 5105 / 14700) loss: 0.735303\n",
      "(Iteration 5106 / 14700) loss: 0.838657\n",
      "(Iteration 5107 / 14700) loss: 0.724908\n",
      "(Iteration 5108 / 14700) loss: 0.950801\n",
      "(Iteration 5109 / 14700) loss: 0.733099\n",
      "(Iteration 5110 / 14700) loss: 0.884461\n",
      "(Iteration 5111 / 14700) loss: 0.799107\n",
      "(Iteration 5112 / 14700) loss: 0.918264\n",
      "(Iteration 5113 / 14700) loss: 0.768222\n",
      "(Iteration 5114 / 14700) loss: 0.935531\n",
      "(Iteration 5115 / 14700) loss: 0.846621\n",
      "(Iteration 5116 / 14700) loss: 0.679097\n",
      "(Iteration 5117 / 14700) loss: 0.558225\n",
      "(Iteration 5118 / 14700) loss: 0.715194\n",
      "(Iteration 5119 / 14700) loss: 0.741521\n",
      "(Iteration 5120 / 14700) loss: 1.071417\n",
      "(Iteration 5121 / 14700) loss: 1.063877\n",
      "(Iteration 5122 / 14700) loss: 0.765803\n",
      "(Iteration 5123 / 14700) loss: 0.859547\n",
      "(Iteration 5124 / 14700) loss: 0.807139\n",
      "(Iteration 5125 / 14700) loss: 0.994314\n",
      "(Iteration 5126 / 14700) loss: 1.137773\n",
      "(Iteration 5127 / 14700) loss: 0.893308\n",
      "(Iteration 5128 / 14700) loss: 0.777476\n",
      "(Iteration 5129 / 14700) loss: 0.946439\n",
      "(Iteration 5130 / 14700) loss: 0.787872\n",
      "(Iteration 5131 / 14700) loss: 0.866507\n",
      "(Iteration 5132 / 14700) loss: 0.738140\n",
      "(Iteration 5133 / 14700) loss: 0.991024\n",
      "(Iteration 5134 / 14700) loss: 1.047663\n",
      "(Iteration 5135 / 14700) loss: 0.690024\n",
      "(Iteration 5136 / 14700) loss: 0.856747\n",
      "(Iteration 5137 / 14700) loss: 0.741710\n",
      "(Iteration 5138 / 14700) loss: 0.835198\n",
      "(Iteration 5139 / 14700) loss: 0.788402\n",
      "(Iteration 5140 / 14700) loss: 1.078493\n",
      "(Iteration 5141 / 14700) loss: 0.799296\n",
      "(Iteration 5142 / 14700) loss: 0.924756\n",
      "(Iteration 5143 / 14700) loss: 0.854503\n",
      "(Iteration 5144 / 14700) loss: 0.801379\n",
      "(Iteration 5145 / 14700) loss: 0.851272\n",
      "(Iteration 5146 / 14700) loss: 0.817310\n",
      "(Iteration 5147 / 14700) loss: 0.985480\n",
      "(Iteration 5148 / 14700) loss: 0.903306\n",
      "(Iteration 5149 / 14700) loss: 0.823756\n",
      "(Iteration 5150 / 14700) loss: 0.890510\n",
      "(Iteration 5151 / 14700) loss: 0.811192\n",
      "(Iteration 5152 / 14700) loss: 0.482274\n",
      "(Iteration 5153 / 14700) loss: 0.897367\n",
      "(Iteration 5154 / 14700) loss: 0.710218\n",
      "(Iteration 5155 / 14700) loss: 0.930822\n",
      "(Iteration 5156 / 14700) loss: 0.948134\n",
      "(Iteration 5157 / 14700) loss: 0.755378\n",
      "(Iteration 5158 / 14700) loss: 0.629826\n",
      "(Iteration 5159 / 14700) loss: 0.869739\n",
      "(Iteration 5160 / 14700) loss: 0.782221\n",
      "(Iteration 5161 / 14700) loss: 0.817222\n",
      "(Iteration 5162 / 14700) loss: 0.835876\n",
      "(Iteration 5163 / 14700) loss: 0.657976\n",
      "(Iteration 5164 / 14700) loss: 0.735488\n",
      "(Iteration 5165 / 14700) loss: 1.005385\n",
      "(Iteration 5166 / 14700) loss: 0.743901\n",
      "(Iteration 5167 / 14700) loss: 1.042819\n",
      "(Iteration 5168 / 14700) loss: 0.745074\n",
      "(Iteration 5169 / 14700) loss: 1.048978\n",
      "(Iteration 5170 / 14700) loss: 0.755699\n",
      "(Iteration 5171 / 14700) loss: 0.848108\n",
      "(Iteration 5172 / 14700) loss: 0.731804\n",
      "(Iteration 5173 / 14700) loss: 0.819422\n",
      "(Iteration 5174 / 14700) loss: 0.736177\n",
      "(Iteration 5175 / 14700) loss: 0.647263\n",
      "(Iteration 5176 / 14700) loss: 0.754331\n",
      "(Iteration 5177 / 14700) loss: 0.882684\n",
      "(Iteration 5178 / 14700) loss: 1.118703\n",
      "(Iteration 5179 / 14700) loss: 0.777169\n",
      "(Iteration 5180 / 14700) loss: 0.925452\n",
      "(Iteration 5181 / 14700) loss: 0.804542\n",
      "(Iteration 5182 / 14700) loss: 0.480211\n",
      "(Iteration 5183 / 14700) loss: 0.776550\n",
      "(Iteration 5184 / 14700) loss: 0.647649\n",
      "(Iteration 5185 / 14700) loss: 1.068214\n",
      "(Iteration 5186 / 14700) loss: 0.799818\n",
      "(Iteration 5187 / 14700) loss: 0.655771\n",
      "(Iteration 5188 / 14700) loss: 0.927276\n",
      "(Iteration 5189 / 14700) loss: 0.912753\n",
      "(Iteration 5190 / 14700) loss: 0.696275\n",
      "(Iteration 5191 / 14700) loss: 1.177087\n",
      "(Iteration 5192 / 14700) loss: 0.932607\n",
      "(Iteration 5193 / 14700) loss: 0.792220\n",
      "(Iteration 5194 / 14700) loss: 1.031641\n",
      "(Iteration 5195 / 14700) loss: 0.817017\n",
      "(Iteration 5196 / 14700) loss: 0.558241\n",
      "(Iteration 5197 / 14700) loss: 0.622709\n",
      "(Iteration 5198 / 14700) loss: 0.736734\n",
      "(Iteration 5199 / 14700) loss: 0.782431\n",
      "(Iteration 5200 / 14700) loss: 0.648142\n",
      "(Iteration 5201 / 14700) loss: 0.743630\n",
      "(Iteration 5202 / 14700) loss: 0.718134\n",
      "(Iteration 5203 / 14700) loss: 0.835653\n",
      "(Iteration 5204 / 14700) loss: 1.000258\n",
      "(Iteration 5205 / 14700) loss: 0.908419\n",
      "(Iteration 5206 / 14700) loss: 0.709251\n",
      "(Iteration 5207 / 14700) loss: 0.685664\n",
      "(Iteration 5208 / 14700) loss: 0.940097\n",
      "(Iteration 5209 / 14700) loss: 0.901628\n",
      "(Iteration 5210 / 14700) loss: 1.036498\n",
      "(Iteration 5211 / 14700) loss: 1.175193\n",
      "(Iteration 5212 / 14700) loss: 0.693352\n",
      "(Iteration 5213 / 14700) loss: 0.605894\n",
      "(Iteration 5214 / 14700) loss: 0.733184\n",
      "(Iteration 5215 / 14700) loss: 0.676150\n",
      "(Iteration 5216 / 14700) loss: 0.881950\n",
      "(Iteration 5217 / 14700) loss: 0.871713\n",
      "(Iteration 5218 / 14700) loss: 0.989174\n",
      "(Iteration 5219 / 14700) loss: 0.948864\n",
      "(Iteration 5220 / 14700) loss: 0.827291\n",
      "(Iteration 5221 / 14700) loss: 0.755135\n",
      "(Iteration 5222 / 14700) loss: 0.620585\n",
      "(Iteration 5223 / 14700) loss: 0.536038\n",
      "(Iteration 5224 / 14700) loss: 0.671474\n",
      "(Iteration 5225 / 14700) loss: 0.890697\n",
      "(Iteration 5226 / 14700) loss: 0.950429\n",
      "(Iteration 5227 / 14700) loss: 0.981344\n",
      "(Iteration 5228 / 14700) loss: 0.934571\n",
      "(Iteration 5229 / 14700) loss: 0.891472\n",
      "(Iteration 5230 / 14700) loss: 0.938429\n",
      "(Iteration 5231 / 14700) loss: 1.001164\n",
      "(Iteration 5232 / 14700) loss: 0.954772\n",
      "(Iteration 5233 / 14700) loss: 0.994207\n",
      "(Iteration 5234 / 14700) loss: 0.893211\n",
      "(Iteration 5235 / 14700) loss: 1.027187\n",
      "(Iteration 5236 / 14700) loss: 0.689692\n",
      "(Iteration 5237 / 14700) loss: 0.842144\n",
      "(Iteration 5238 / 14700) loss: 0.737919\n",
      "(Iteration 5239 / 14700) loss: 0.828792\n",
      "(Iteration 5240 / 14700) loss: 0.896038\n",
      "(Iteration 5241 / 14700) loss: 0.853082\n",
      "(Iteration 5242 / 14700) loss: 0.736878\n",
      "(Iteration 5243 / 14700) loss: 0.605938\n",
      "(Iteration 5244 / 14700) loss: 0.804619\n",
      "(Iteration 5245 / 14700) loss: 0.631808\n",
      "(Iteration 5246 / 14700) loss: 0.801335\n",
      "(Iteration 5247 / 14700) loss: 0.706618\n",
      "(Iteration 5248 / 14700) loss: 0.772175\n",
      "(Iteration 5249 / 14700) loss: 0.600164\n",
      "(Iteration 5250 / 14700) loss: 0.791874\n",
      "(Iteration 5251 / 14700) loss: 0.856507\n",
      "(Iteration 5252 / 14700) loss: 0.692677\n",
      "(Iteration 5253 / 14700) loss: 0.814075\n",
      "(Iteration 5254 / 14700) loss: 0.898000\n",
      "(Iteration 5255 / 14700) loss: 1.192326\n",
      "(Iteration 5256 / 14700) loss: 0.978110\n",
      "(Iteration 5257 / 14700) loss: 0.759525\n",
      "(Iteration 5258 / 14700) loss: 1.405904\n",
      "(Iteration 5259 / 14700) loss: 0.716638\n",
      "(Iteration 5260 / 14700) loss: 0.845205\n",
      "(Iteration 5261 / 14700) loss: 0.691434\n",
      "(Iteration 5262 / 14700) loss: 0.880622\n",
      "(Iteration 5263 / 14700) loss: 0.953003\n",
      "(Iteration 5264 / 14700) loss: 0.612087\n",
      "(Iteration 5265 / 14700) loss: 0.768329\n",
      "(Iteration 5266 / 14700) loss: 0.786489\n",
      "(Iteration 5267 / 14700) loss: 0.894257\n",
      "(Iteration 5268 / 14700) loss: 0.672210\n",
      "(Iteration 5269 / 14700) loss: 0.923393\n",
      "(Iteration 5270 / 14700) loss: 0.814468\n",
      "(Iteration 5271 / 14700) loss: 0.736824\n",
      "(Iteration 5272 / 14700) loss: 0.757797\n",
      "(Iteration 5273 / 14700) loss: 0.845200\n",
      "(Iteration 5274 / 14700) loss: 0.869401\n",
      "(Iteration 5275 / 14700) loss: 0.795495\n",
      "(Iteration 5276 / 14700) loss: 0.696319\n",
      "(Iteration 5277 / 14700) loss: 0.783589\n",
      "(Iteration 5278 / 14700) loss: 0.968813\n",
      "(Iteration 5279 / 14700) loss: 0.678340\n",
      "(Iteration 5280 / 14700) loss: 0.603599\n",
      "(Iteration 5281 / 14700) loss: 0.802281\n",
      "(Iteration 5282 / 14700) loss: 0.995704\n",
      "(Iteration 5283 / 14700) loss: 0.999891\n",
      "(Iteration 5284 / 14700) loss: 0.620054\n",
      "(Iteration 5285 / 14700) loss: 0.857799\n",
      "(Iteration 5286 / 14700) loss: 0.668742\n",
      "(Iteration 5287 / 14700) loss: 0.979392\n",
      "(Iteration 5288 / 14700) loss: 0.928106\n",
      "(Iteration 5289 / 14700) loss: 0.673829\n",
      "(Iteration 5290 / 14700) loss: 0.955035\n",
      "(Iteration 5291 / 14700) loss: 0.937493\n",
      "(Iteration 5292 / 14700) loss: 0.945834\n",
      "(Iteration 5293 / 14700) loss: 0.778545\n",
      "(Iteration 5294 / 14700) loss: 0.864064\n",
      "(Iteration 5295 / 14700) loss: 0.868802\n",
      "(Iteration 5296 / 14700) loss: 0.804318\n",
      "(Iteration 5297 / 14700) loss: 0.854651\n",
      "(Iteration 5298 / 14700) loss: 1.023378\n",
      "(Iteration 5299 / 14700) loss: 0.776112\n",
      "(Iteration 5300 / 14700) loss: 0.772772\n",
      "(Iteration 5301 / 14700) loss: 1.040437\n",
      "(Iteration 5302 / 14700) loss: 0.533799\n",
      "(Iteration 5303 / 14700) loss: 0.709297\n",
      "(Iteration 5304 / 14700) loss: 0.636257\n",
      "(Iteration 5305 / 14700) loss: 0.772168\n",
      "(Iteration 5306 / 14700) loss: 0.877774\n",
      "(Iteration 5307 / 14700) loss: 1.041229\n",
      "(Iteration 5308 / 14700) loss: 0.791770\n",
      "(Iteration 5309 / 14700) loss: 1.020225\n",
      "(Iteration 5310 / 14700) loss: 0.633074\n",
      "(Iteration 5311 / 14700) loss: 0.821896\n",
      "(Iteration 5312 / 14700) loss: 0.598147\n",
      "(Iteration 5313 / 14700) loss: 0.762992\n",
      "(Iteration 5314 / 14700) loss: 0.893392\n",
      "(Iteration 5315 / 14700) loss: 0.938339\n",
      "(Iteration 5316 / 14700) loss: 0.746955\n",
      "(Iteration 5317 / 14700) loss: 0.755542\n",
      "(Iteration 5318 / 14700) loss: 0.755745\n",
      "(Iteration 5319 / 14700) loss: 0.914093\n",
      "(Iteration 5320 / 14700) loss: 0.946576\n",
      "(Iteration 5321 / 14700) loss: 1.077435\n",
      "(Iteration 5322 / 14700) loss: 0.528739\n",
      "(Iteration 5323 / 14700) loss: 0.971517\n",
      "(Iteration 5324 / 14700) loss: 0.631424\n",
      "(Iteration 5325 / 14700) loss: 0.707505\n",
      "(Iteration 5326 / 14700) loss: 0.855450\n",
      "(Iteration 5327 / 14700) loss: 0.705664\n",
      "(Iteration 5328 / 14700) loss: 0.600196\n",
      "(Iteration 5329 / 14700) loss: 0.924360\n",
      "(Iteration 5330 / 14700) loss: 1.341971\n",
      "(Iteration 5331 / 14700) loss: 0.944658\n",
      "(Iteration 5332 / 14700) loss: 0.918758\n",
      "(Iteration 5333 / 14700) loss: 0.884872\n",
      "(Iteration 5334 / 14700) loss: 0.934203\n",
      "(Iteration 5335 / 14700) loss: 0.766555\n",
      "(Iteration 5336 / 14700) loss: 0.853559\n",
      "(Iteration 5337 / 14700) loss: 0.747962\n",
      "(Iteration 5338 / 14700) loss: 0.878912\n",
      "(Iteration 5339 / 14700) loss: 0.931337\n",
      "(Iteration 5340 / 14700) loss: 0.853445\n",
      "(Iteration 5341 / 14700) loss: 0.936916\n",
      "(Iteration 5342 / 14700) loss: 0.895193\n",
      "(Iteration 5343 / 14700) loss: 1.025432\n",
      "(Iteration 5344 / 14700) loss: 0.661230\n",
      "(Iteration 5345 / 14700) loss: 0.729502\n",
      "(Iteration 5346 / 14700) loss: 0.965692\n",
      "(Iteration 5347 / 14700) loss: 0.779691\n",
      "(Iteration 5348 / 14700) loss: 0.832560\n",
      "(Iteration 5349 / 14700) loss: 0.628849\n",
      "(Iteration 5350 / 14700) loss: 0.748109\n",
      "(Iteration 5351 / 14700) loss: 0.793461\n",
      "(Iteration 5352 / 14700) loss: 0.669789\n",
      "(Iteration 5353 / 14700) loss: 0.626070\n",
      "(Iteration 5354 / 14700) loss: 0.893175\n",
      "(Iteration 5355 / 14700) loss: 0.750966\n",
      "(Iteration 5356 / 14700) loss: 0.777378\n",
      "(Iteration 5357 / 14700) loss: 0.548814\n",
      "(Iteration 5358 / 14700) loss: 1.022067\n",
      "(Iteration 5359 / 14700) loss: 0.873387\n",
      "(Iteration 5360 / 14700) loss: 1.025628\n",
      "(Iteration 5361 / 14700) loss: 0.716340\n",
      "(Iteration 5362 / 14700) loss: 0.651036\n",
      "(Iteration 5363 / 14700) loss: 0.739456\n",
      "(Iteration 5364 / 14700) loss: 0.995246\n",
      "(Iteration 5365 / 14700) loss: 0.669829\n",
      "(Iteration 5366 / 14700) loss: 0.824136\n",
      "(Iteration 5367 / 14700) loss: 0.990916\n",
      "(Iteration 5368 / 14700) loss: 0.985440\n",
      "(Iteration 5369 / 14700) loss: 0.935055\n",
      "(Iteration 5370 / 14700) loss: 1.070455\n",
      "(Iteration 5371 / 14700) loss: 0.980282\n",
      "(Iteration 5372 / 14700) loss: 0.709009\n",
      "(Iteration 5373 / 14700) loss: 0.830724\n",
      "(Iteration 5374 / 14700) loss: 1.132714\n",
      "(Iteration 5375 / 14700) loss: 0.765483\n",
      "(Iteration 5376 / 14700) loss: 0.893218\n",
      "(Iteration 5377 / 14700) loss: 0.763809\n",
      "(Iteration 5378 / 14700) loss: 0.617838\n",
      "(Iteration 5379 / 14700) loss: 0.624303\n",
      "(Iteration 5380 / 14700) loss: 0.708367\n",
      "(Iteration 5381 / 14700) loss: 0.989792\n",
      "(Iteration 5382 / 14700) loss: 0.755549\n",
      "(Iteration 5383 / 14700) loss: 1.077385\n",
      "(Iteration 5384 / 14700) loss: 0.779173\n",
      "(Iteration 5385 / 14700) loss: 0.764476\n",
      "(Iteration 5386 / 14700) loss: 0.980531\n",
      "(Iteration 5387 / 14700) loss: 0.769836\n",
      "(Iteration 5388 / 14700) loss: 0.765810\n",
      "(Iteration 5389 / 14700) loss: 0.859356\n",
      "(Iteration 5390 / 14700) loss: 1.012203\n",
      "(Iteration 5391 / 14700) loss: 0.914199\n",
      "(Iteration 5392 / 14700) loss: 0.645123\n",
      "(Iteration 5393 / 14700) loss: 0.831686\n",
      "(Iteration 5394 / 14700) loss: 0.784246\n",
      "(Iteration 5395 / 14700) loss: 0.583308\n",
      "(Iteration 5396 / 14700) loss: 0.781153\n",
      "(Iteration 5397 / 14700) loss: 0.665771\n",
      "(Iteration 5398 / 14700) loss: 0.671015\n",
      "(Iteration 5399 / 14700) loss: 0.852337\n",
      "(Iteration 5400 / 14700) loss: 0.717528\n",
      "(Iteration 5401 / 14700) loss: 1.132587\n",
      "(Iteration 5402 / 14700) loss: 0.896103\n",
      "(Iteration 5403 / 14700) loss: 0.896107\n",
      "(Iteration 5404 / 14700) loss: 0.783025\n",
      "(Iteration 5405 / 14700) loss: 0.745931\n",
      "(Iteration 5406 / 14700) loss: 0.831036\n",
      "(Iteration 5407 / 14700) loss: 0.786516\n",
      "(Iteration 5408 / 14700) loss: 0.964463\n",
      "(Iteration 5409 / 14700) loss: 0.911863\n",
      "(Iteration 5410 / 14700) loss: 0.827403\n",
      "(Iteration 5411 / 14700) loss: 1.021728\n",
      "(Iteration 5412 / 14700) loss: 0.976947\n",
      "(Iteration 5413 / 14700) loss: 1.162057\n",
      "(Iteration 5414 / 14700) loss: 0.858081\n",
      "(Iteration 5415 / 14700) loss: 0.775958\n",
      "(Iteration 5416 / 14700) loss: 1.028747\n",
      "(Iteration 5417 / 14700) loss: 0.750422\n",
      "(Iteration 5418 / 14700) loss: 0.669989\n",
      "(Iteration 5419 / 14700) loss: 0.796239\n",
      "(Iteration 5420 / 14700) loss: 0.756965\n",
      "(Iteration 5421 / 14700) loss: 0.963811\n",
      "(Iteration 5422 / 14700) loss: 0.681762\n",
      "(Iteration 5423 / 14700) loss: 0.609192\n",
      "(Iteration 5424 / 14700) loss: 0.780834\n",
      "(Iteration 5425 / 14700) loss: 0.686681\n",
      "(Iteration 5426 / 14700) loss: 0.964275\n",
      "(Iteration 5427 / 14700) loss: 0.788308\n",
      "(Iteration 5428 / 14700) loss: 0.854949\n",
      "(Iteration 5429 / 14700) loss: 0.848706\n",
      "(Iteration 5430 / 14700) loss: 0.894644\n",
      "(Iteration 5431 / 14700) loss: 0.897838\n",
      "(Iteration 5432 / 14700) loss: 0.811257\n",
      "(Iteration 5433 / 14700) loss: 0.809980\n",
      "(Iteration 5434 / 14700) loss: 0.804573\n",
      "(Iteration 5435 / 14700) loss: 0.842854\n",
      "(Iteration 5436 / 14700) loss: 0.971163\n",
      "(Iteration 5437 / 14700) loss: 1.102202\n",
      "(Iteration 5438 / 14700) loss: 0.652477\n",
      "(Iteration 5439 / 14700) loss: 0.887380\n",
      "(Iteration 5440 / 14700) loss: 0.641780\n",
      "(Iteration 5441 / 14700) loss: 0.873142\n",
      "(Iteration 5442 / 14700) loss: 0.879730\n",
      "(Iteration 5443 / 14700) loss: 0.768700\n",
      "(Iteration 5444 / 14700) loss: 0.758665\n",
      "(Iteration 5445 / 14700) loss: 1.024874\n",
      "(Iteration 5446 / 14700) loss: 1.034765\n",
      "(Iteration 5447 / 14700) loss: 0.924031\n",
      "(Iteration 5448 / 14700) loss: 0.834092\n",
      "(Iteration 5449 / 14700) loss: 0.880056\n",
      "(Iteration 5450 / 14700) loss: 0.819484\n",
      "(Iteration 5451 / 14700) loss: 0.802132\n",
      "(Iteration 5452 / 14700) loss: 0.655165\n",
      "(Iteration 5453 / 14700) loss: 1.292892\n",
      "(Iteration 5454 / 14700) loss: 0.665036\n",
      "(Iteration 5455 / 14700) loss: 0.828407\n",
      "(Iteration 5456 / 14700) loss: 0.920531\n",
      "(Iteration 5457 / 14700) loss: 0.613720\n",
      "(Iteration 5458 / 14700) loss: 0.874225\n",
      "(Iteration 5459 / 14700) loss: 0.499074\n",
      "(Iteration 5460 / 14700) loss: 0.825950\n",
      "(Iteration 5461 / 14700) loss: 0.820025\n",
      "(Iteration 5462 / 14700) loss: 0.838387\n",
      "(Iteration 5463 / 14700) loss: 0.663461\n",
      "(Iteration 5464 / 14700) loss: 0.610708\n",
      "(Iteration 5465 / 14700) loss: 1.182248\n",
      "(Iteration 5466 / 14700) loss: 0.866704\n",
      "(Iteration 5467 / 14700) loss: 0.872322\n",
      "(Iteration 5468 / 14700) loss: 0.926004\n",
      "(Iteration 5469 / 14700) loss: 0.790074\n",
      "(Iteration 5470 / 14700) loss: 0.689088\n",
      "(Iteration 5471 / 14700) loss: 0.839584\n",
      "(Iteration 5472 / 14700) loss: 1.152096\n",
      "(Iteration 5473 / 14700) loss: 0.561312\n",
      "(Iteration 5474 / 14700) loss: 0.987454\n",
      "(Iteration 5475 / 14700) loss: 1.000920\n",
      "(Iteration 5476 / 14700) loss: 0.746699\n",
      "(Iteration 5477 / 14700) loss: 0.940370\n",
      "(Iteration 5478 / 14700) loss: 0.659181\n",
      "(Iteration 5479 / 14700) loss: 0.835586\n",
      "(Iteration 5480 / 14700) loss: 0.600476\n",
      "(Iteration 5481 / 14700) loss: 0.919198\n",
      "(Iteration 5482 / 14700) loss: 0.830915\n",
      "(Iteration 5483 / 14700) loss: 0.736822\n",
      "(Iteration 5484 / 14700) loss: 0.967770\n",
      "(Iteration 5485 / 14700) loss: 0.665342\n",
      "(Iteration 5486 / 14700) loss: 0.865536\n",
      "(Iteration 5487 / 14700) loss: 0.702017\n",
      "(Iteration 5488 / 14700) loss: 1.021865\n",
      "(Iteration 5489 / 14700) loss: 0.956689\n",
      "(Iteration 5490 / 14700) loss: 0.708712\n",
      "(Iteration 5491 / 14700) loss: 1.090885\n",
      "(Iteration 5492 / 14700) loss: 0.828854\n",
      "(Iteration 5493 / 14700) loss: 0.768833\n",
      "(Iteration 5494 / 14700) loss: 0.846237\n",
      "(Iteration 5495 / 14700) loss: 0.991798\n",
      "(Iteration 5496 / 14700) loss: 0.892668\n",
      "(Iteration 5497 / 14700) loss: 0.922200\n",
      "(Iteration 5498 / 14700) loss: 0.771124\n",
      "(Iteration 5499 / 14700) loss: 0.771702\n",
      "(Iteration 5500 / 14700) loss: 0.767474\n",
      "(Iteration 5501 / 14700) loss: 0.875352\n",
      "(Iteration 5502 / 14700) loss: 0.729530\n",
      "(Iteration 5503 / 14700) loss: 0.765023\n",
      "(Iteration 5504 / 14700) loss: 0.860558\n",
      "(Iteration 5505 / 14700) loss: 0.934531\n",
      "(Iteration 5506 / 14700) loss: 0.921117\n",
      "(Iteration 5507 / 14700) loss: 0.799905\n",
      "(Iteration 5508 / 14700) loss: 1.032656\n",
      "(Iteration 5509 / 14700) loss: 0.908227\n",
      "(Iteration 5510 / 14700) loss: 0.683248\n",
      "(Iteration 5511 / 14700) loss: 1.179185\n",
      "(Iteration 5512 / 14700) loss: 0.724934\n",
      "(Iteration 5513 / 14700) loss: 0.861200\n",
      "(Iteration 5514 / 14700) loss: 0.756744\n",
      "(Iteration 5515 / 14700) loss: 0.756976\n",
      "(Iteration 5516 / 14700) loss: 0.895439\n",
      "(Iteration 5517 / 14700) loss: 0.685584\n",
      "(Iteration 5518 / 14700) loss: 0.982997\n",
      "(Iteration 5519 / 14700) loss: 0.692201\n",
      "(Iteration 5520 / 14700) loss: 0.558199\n",
      "(Iteration 5521 / 14700) loss: 0.765264\n",
      "(Iteration 5522 / 14700) loss: 0.610217\n",
      "(Iteration 5523 / 14700) loss: 0.819018\n",
      "(Iteration 5524 / 14700) loss: 0.917167\n",
      "(Iteration 5525 / 14700) loss: 0.729451\n",
      "(Iteration 5526 / 14700) loss: 0.795034\n",
      "(Iteration 5527 / 14700) loss: 0.766400\n",
      "(Iteration 5528 / 14700) loss: 0.624490\n",
      "(Iteration 5529 / 14700) loss: 0.839528\n",
      "(Iteration 5530 / 14700) loss: 0.837056\n",
      "(Iteration 5531 / 14700) loss: 1.047877\n",
      "(Iteration 5532 / 14700) loss: 1.086906\n",
      "(Iteration 5533 / 14700) loss: 0.708308\n",
      "(Iteration 5534 / 14700) loss: 0.871446\n",
      "(Iteration 5535 / 14700) loss: 0.766583\n",
      "(Iteration 5536 / 14700) loss: 0.723522\n",
      "(Iteration 5537 / 14700) loss: 0.605139\n",
      "(Iteration 5538 / 14700) loss: 0.793119\n",
      "(Iteration 5539 / 14700) loss: 0.764788\n",
      "(Iteration 5540 / 14700) loss: 0.582297\n",
      "(Iteration 5541 / 14700) loss: 0.762367\n",
      "(Iteration 5542 / 14700) loss: 0.787092\n",
      "(Iteration 5543 / 14700) loss: 0.722109\n",
      "(Iteration 5544 / 14700) loss: 0.800551\n",
      "(Iteration 5545 / 14700) loss: 0.902063\n",
      "(Iteration 5546 / 14700) loss: 0.577279\n",
      "(Iteration 5547 / 14700) loss: 0.852425\n",
      "(Iteration 5548 / 14700) loss: 0.549751\n",
      "(Iteration 5549 / 14700) loss: 1.090320\n",
      "(Iteration 5550 / 14700) loss: 0.697539\n",
      "(Iteration 5551 / 14700) loss: 0.658832\n",
      "(Iteration 5552 / 14700) loss: 0.797563\n",
      "(Iteration 5553 / 14700) loss: 0.681190\n",
      "(Iteration 5554 / 14700) loss: 0.747992\n",
      "(Iteration 5555 / 14700) loss: 1.196373\n",
      "(Iteration 5556 / 14700) loss: 0.935558\n",
      "(Iteration 5557 / 14700) loss: 0.895661\n",
      "(Iteration 5558 / 14700) loss: 0.609369\n",
      "(Iteration 5559 / 14700) loss: 0.747920\n",
      "(Iteration 5560 / 14700) loss: 0.689840\n",
      "(Iteration 5561 / 14700) loss: 0.668627\n",
      "(Iteration 5562 / 14700) loss: 0.920823\n",
      "(Iteration 5563 / 14700) loss: 1.049263\n",
      "(Iteration 5564 / 14700) loss: 0.971079\n",
      "(Iteration 5565 / 14700) loss: 0.574487\n",
      "(Iteration 5566 / 14700) loss: 0.833065\n",
      "(Iteration 5567 / 14700) loss: 0.982348\n",
      "(Iteration 5568 / 14700) loss: 1.029647\n",
      "(Iteration 5569 / 14700) loss: 0.978473\n",
      "(Iteration 5570 / 14700) loss: 1.126020\n",
      "(Iteration 5571 / 14700) loss: 0.829523\n",
      "(Iteration 5572 / 14700) loss: 0.750860\n",
      "(Iteration 5573 / 14700) loss: 0.917346\n",
      "(Iteration 5574 / 14700) loss: 0.714004\n",
      "(Iteration 5575 / 14700) loss: 0.940210\n",
      "(Iteration 5576 / 14700) loss: 0.816203\n",
      "(Iteration 5577 / 14700) loss: 0.775659\n",
      "(Iteration 5578 / 14700) loss: 0.667858\n",
      "(Iteration 5579 / 14700) loss: 0.838805\n",
      "(Iteration 5580 / 14700) loss: 0.789015\n",
      "(Iteration 5581 / 14700) loss: 0.812621\n",
      "(Iteration 5582 / 14700) loss: 0.952439\n",
      "(Iteration 5583 / 14700) loss: 0.753782\n",
      "(Iteration 5584 / 14700) loss: 0.860638\n",
      "(Iteration 5585 / 14700) loss: 0.491119\n",
      "(Iteration 5586 / 14700) loss: 0.855183\n",
      "(Iteration 5587 / 14700) loss: 0.929503\n",
      "(Iteration 5588 / 14700) loss: 0.732685\n",
      "(Iteration 5589 / 14700) loss: 0.985285\n",
      "(Iteration 5590 / 14700) loss: 1.017714\n",
      "(Iteration 5591 / 14700) loss: 0.910441\n",
      "(Iteration 5592 / 14700) loss: 1.147397\n",
      "(Iteration 5593 / 14700) loss: 0.480898\n",
      "(Iteration 5594 / 14700) loss: 1.076175\n",
      "(Iteration 5595 / 14700) loss: 0.862277\n",
      "(Iteration 5596 / 14700) loss: 0.730812\n",
      "(Iteration 5597 / 14700) loss: 0.932901\n",
      "(Iteration 5598 / 14700) loss: 0.860704\n",
      "(Iteration 5599 / 14700) loss: 1.141656\n",
      "(Iteration 5600 / 14700) loss: 0.797364\n",
      "(Iteration 5601 / 14700) loss: 0.745235\n",
      "(Iteration 5602 / 14700) loss: 0.742418\n",
      "(Iteration 5603 / 14700) loss: 0.904457\n",
      "(Iteration 5604 / 14700) loss: 0.753875\n",
      "(Iteration 5605 / 14700) loss: 0.814848\n",
      "(Iteration 5606 / 14700) loss: 0.756048\n",
      "(Iteration 5607 / 14700) loss: 0.943767\n",
      "(Iteration 5608 / 14700) loss: 0.812484\n",
      "(Iteration 5609 / 14700) loss: 0.765825\n",
      "(Iteration 5610 / 14700) loss: 0.870201\n",
      "(Iteration 5611 / 14700) loss: 0.844585\n",
      "(Iteration 5612 / 14700) loss: 0.906454\n",
      "(Iteration 5613 / 14700) loss: 0.879046\n",
      "(Iteration 5614 / 14700) loss: 0.862214\n",
      "(Iteration 5615 / 14700) loss: 0.924152\n",
      "(Iteration 5616 / 14700) loss: 1.205780\n",
      "(Iteration 5617 / 14700) loss: 0.858507\n",
      "(Iteration 5618 / 14700) loss: 0.589937\n",
      "(Iteration 5619 / 14700) loss: 0.924343\n",
      "(Iteration 5620 / 14700) loss: 0.679889\n",
      "(Iteration 5621 / 14700) loss: 0.878578\n",
      "(Iteration 5622 / 14700) loss: 0.763397\n",
      "(Iteration 5623 / 14700) loss: 0.857581\n",
      "(Iteration 5624 / 14700) loss: 0.854706\n",
      "(Iteration 5625 / 14700) loss: 1.005353\n",
      "(Iteration 5626 / 14700) loss: 0.729828\n",
      "(Iteration 5627 / 14700) loss: 0.823382\n",
      "(Iteration 5628 / 14700) loss: 0.816215\n",
      "(Iteration 5629 / 14700) loss: 0.749044\n",
      "(Iteration 5630 / 14700) loss: 0.755956\n",
      "(Iteration 5631 / 14700) loss: 0.758823\n",
      "(Iteration 5632 / 14700) loss: 0.987156\n",
      "(Iteration 5633 / 14700) loss: 0.836136\n",
      "(Iteration 5634 / 14700) loss: 0.793864\n",
      "(Iteration 5635 / 14700) loss: 1.057868\n",
      "(Iteration 5636 / 14700) loss: 0.842241\n",
      "(Iteration 5637 / 14700) loss: 0.718489\n",
      "(Iteration 5638 / 14700) loss: 0.918326\n",
      "(Iteration 5639 / 14700) loss: 0.836427\n",
      "(Iteration 5640 / 14700) loss: 0.832408\n",
      "(Iteration 5641 / 14700) loss: 0.794864\n",
      "(Iteration 5642 / 14700) loss: 0.761319\n",
      "(Iteration 5643 / 14700) loss: 0.629881\n",
      "(Iteration 5644 / 14700) loss: 0.815068\n",
      "(Iteration 5645 / 14700) loss: 0.597323\n",
      "(Iteration 5646 / 14700) loss: 0.841822\n",
      "(Iteration 5647 / 14700) loss: 0.870640\n",
      "(Iteration 5648 / 14700) loss: 0.723616\n",
      "(Iteration 5649 / 14700) loss: 0.625868\n",
      "(Iteration 5650 / 14700) loss: 0.795268\n",
      "(Iteration 5651 / 14700) loss: 0.751140\n",
      "(Iteration 5652 / 14700) loss: 0.881472\n",
      "(Iteration 5653 / 14700) loss: 0.851943\n",
      "(Iteration 5654 / 14700) loss: 0.879467\n",
      "(Iteration 5655 / 14700) loss: 1.039594\n",
      "(Iteration 5656 / 14700) loss: 0.605135\n",
      "(Iteration 5657 / 14700) loss: 0.924382\n",
      "(Iteration 5658 / 14700) loss: 0.732553\n",
      "(Iteration 5659 / 14700) loss: 0.554394\n",
      "(Iteration 5660 / 14700) loss: 0.871371\n",
      "(Iteration 5661 / 14700) loss: 0.845278\n",
      "(Iteration 5662 / 14700) loss: 0.661119\n",
      "(Iteration 5663 / 14700) loss: 1.140064\n",
      "(Iteration 5664 / 14700) loss: 0.732555\n",
      "(Iteration 5665 / 14700) loss: 0.881070\n",
      "(Iteration 5666 / 14700) loss: 0.826924\n",
      "(Iteration 5667 / 14700) loss: 1.060651\n",
      "(Iteration 5668 / 14700) loss: 0.675104\n",
      "(Iteration 5669 / 14700) loss: 0.666502\n",
      "(Iteration 5670 / 14700) loss: 0.710794\n",
      "(Iteration 5671 / 14700) loss: 0.752373\n",
      "(Iteration 5672 / 14700) loss: 0.657048\n",
      "(Iteration 5673 / 14700) loss: 0.669285\n",
      "(Iteration 5674 / 14700) loss: 0.816623\n",
      "(Iteration 5675 / 14700) loss: 0.710732\n",
      "(Iteration 5676 / 14700) loss: 0.760815\n",
      "(Iteration 5677 / 14700) loss: 0.966387\n",
      "(Iteration 5678 / 14700) loss: 0.825917\n",
      "(Iteration 5679 / 14700) loss: 0.747865\n",
      "(Iteration 5680 / 14700) loss: 0.818033\n",
      "(Iteration 5681 / 14700) loss: 0.852762\n",
      "(Iteration 5682 / 14700) loss: 0.823368\n",
      "(Iteration 5683 / 14700) loss: 0.933508\n",
      "(Iteration 5684 / 14700) loss: 0.695739\n",
      "(Iteration 5685 / 14700) loss: 0.619254\n",
      "(Iteration 5686 / 14700) loss: 0.666002\n",
      "(Iteration 5687 / 14700) loss: 1.058220\n",
      "(Iteration 5688 / 14700) loss: 0.923174\n",
      "(Iteration 5689 / 14700) loss: 0.795666\n",
      "(Iteration 5690 / 14700) loss: 0.832035\n",
      "(Iteration 5691 / 14700) loss: 0.675335\n",
      "(Iteration 5692 / 14700) loss: 0.762299\n",
      "(Iteration 5693 / 14700) loss: 0.779293\n",
      "(Iteration 5694 / 14700) loss: 1.181409\n",
      "(Iteration 5695 / 14700) loss: 0.751713\n",
      "(Iteration 5696 / 14700) loss: 0.737177\n",
      "(Iteration 5697 / 14700) loss: 0.589593\n",
      "(Iteration 5698 / 14700) loss: 0.628378\n",
      "(Iteration 5699 / 14700) loss: 0.715894\n",
      "(Iteration 5700 / 14700) loss: 0.635778\n",
      "(Iteration 5701 / 14700) loss: 1.128181\n",
      "(Iteration 5702 / 14700) loss: 1.038053\n",
      "(Iteration 5703 / 14700) loss: 0.748532\n",
      "(Iteration 5704 / 14700) loss: 1.115700\n",
      "(Iteration 5705 / 14700) loss: 1.132014\n",
      "(Iteration 5706 / 14700) loss: 0.694750\n",
      "(Iteration 5707 / 14700) loss: 0.680174\n",
      "(Iteration 5708 / 14700) loss: 0.841731\n",
      "(Iteration 5709 / 14700) loss: 0.916016\n",
      "(Iteration 5710 / 14700) loss: 0.779049\n",
      "(Iteration 5711 / 14700) loss: 0.777495\n",
      "(Iteration 5712 / 14700) loss: 0.686773\n",
      "(Iteration 5713 / 14700) loss: 0.827556\n",
      "(Iteration 5714 / 14700) loss: 0.766422\n",
      "(Iteration 5715 / 14700) loss: 0.646246\n",
      "(Iteration 5716 / 14700) loss: 0.899905\n",
      "(Iteration 5717 / 14700) loss: 0.769471\n",
      "(Iteration 5718 / 14700) loss: 0.820416\n",
      "(Iteration 5719 / 14700) loss: 1.024284\n",
      "(Iteration 5720 / 14700) loss: 0.863602\n",
      "(Iteration 5721 / 14700) loss: 0.547264\n",
      "(Iteration 5722 / 14700) loss: 0.670872\n",
      "(Iteration 5723 / 14700) loss: 0.846945\n",
      "(Iteration 5724 / 14700) loss: 0.944569\n",
      "(Iteration 5725 / 14700) loss: 0.711315\n",
      "(Iteration 5726 / 14700) loss: 0.965887\n",
      "(Iteration 5727 / 14700) loss: 0.848415\n",
      "(Iteration 5728 / 14700) loss: 0.786326\n",
      "(Iteration 5729 / 14700) loss: 0.829799\n",
      "(Iteration 5730 / 14700) loss: 0.776471\n",
      "(Iteration 5731 / 14700) loss: 0.617206\n",
      "(Iteration 5732 / 14700) loss: 0.831592\n",
      "(Iteration 5733 / 14700) loss: 0.604001\n",
      "(Iteration 5734 / 14700) loss: 0.611012\n",
      "(Iteration 5735 / 14700) loss: 0.856946\n",
      "(Iteration 5736 / 14700) loss: 0.824554\n",
      "(Iteration 5737 / 14700) loss: 0.773675\n",
      "(Iteration 5738 / 14700) loss: 0.711807\n",
      "(Iteration 5739 / 14700) loss: 0.687961\n",
      "(Iteration 5740 / 14700) loss: 0.731454\n",
      "(Iteration 5741 / 14700) loss: 0.852307\n",
      "(Iteration 5742 / 14700) loss: 0.804496\n",
      "(Iteration 5743 / 14700) loss: 0.614040\n",
      "(Iteration 5744 / 14700) loss: 0.641113\n",
      "(Iteration 5745 / 14700) loss: 0.688041\n",
      "(Iteration 5746 / 14700) loss: 0.740651\n",
      "(Iteration 5747 / 14700) loss: 0.721943\n",
      "(Iteration 5748 / 14700) loss: 0.913924\n",
      "(Iteration 5749 / 14700) loss: 0.992891\n",
      "(Iteration 5750 / 14700) loss: 0.998377\n",
      "(Iteration 5751 / 14700) loss: 1.065762\n",
      "(Iteration 5752 / 14700) loss: 0.722255\n",
      "(Iteration 5753 / 14700) loss: 0.772197\n",
      "(Iteration 5754 / 14700) loss: 0.537327\n",
      "(Iteration 5755 / 14700) loss: 0.961727\n",
      "(Iteration 5756 / 14700) loss: 0.755744\n",
      "(Iteration 5757 / 14700) loss: 0.533734\n",
      "(Iteration 5758 / 14700) loss: 0.937230\n",
      "(Iteration 5759 / 14700) loss: 0.828706\n",
      "(Iteration 5760 / 14700) loss: 0.789370\n",
      "(Iteration 5761 / 14700) loss: 1.067552\n",
      "(Iteration 5762 / 14700) loss: 0.776516\n",
      "(Iteration 5763 / 14700) loss: 0.803304\n",
      "(Iteration 5764 / 14700) loss: 0.791107\n",
      "(Iteration 5765 / 14700) loss: 0.833803\n",
      "(Iteration 5766 / 14700) loss: 1.314758\n",
      "(Iteration 5767 / 14700) loss: 1.120472\n",
      "(Iteration 5768 / 14700) loss: 0.811736\n",
      "(Iteration 5769 / 14700) loss: 0.828608\n",
      "(Iteration 5770 / 14700) loss: 0.577731\n",
      "(Iteration 5771 / 14700) loss: 0.703414\n",
      "(Iteration 5772 / 14700) loss: 0.721808\n",
      "(Iteration 5773 / 14700) loss: 0.829182\n",
      "(Iteration 5774 / 14700) loss: 0.712313\n",
      "(Iteration 5775 / 14700) loss: 0.629132\n",
      "(Iteration 5776 / 14700) loss: 1.041684\n",
      "(Iteration 5777 / 14700) loss: 0.788884\n",
      "(Iteration 5778 / 14700) loss: 0.843263\n",
      "(Iteration 5779 / 14700) loss: 0.889879\n",
      "(Iteration 5780 / 14700) loss: 0.823985\n",
      "(Iteration 5781 / 14700) loss: 0.706305\n",
      "(Iteration 5782 / 14700) loss: 0.954862\n",
      "(Iteration 5783 / 14700) loss: 0.576309\n",
      "(Iteration 5784 / 14700) loss: 0.737189\n",
      "(Iteration 5785 / 14700) loss: 0.864409\n",
      "(Iteration 5786 / 14700) loss: 0.873126\n",
      "(Iteration 5787 / 14700) loss: 0.827463\n",
      "(Iteration 5788 / 14700) loss: 0.550897\n",
      "(Iteration 5789 / 14700) loss: 0.904008\n",
      "(Iteration 5790 / 14700) loss: 0.675792\n",
      "(Iteration 5791 / 14700) loss: 0.841490\n",
      "(Iteration 5792 / 14700) loss: 0.831118\n",
      "(Iteration 5793 / 14700) loss: 0.991336\n",
      "(Iteration 5794 / 14700) loss: 0.684462\n",
      "(Iteration 5795 / 14700) loss: 0.905508\n",
      "(Iteration 5796 / 14700) loss: 0.810042\n",
      "(Iteration 5797 / 14700) loss: 0.691250\n",
      "(Iteration 5798 / 14700) loss: 0.683256\n",
      "(Iteration 5799 / 14700) loss: 0.823285\n",
      "(Iteration 5800 / 14700) loss: 0.710476\n",
      "(Iteration 5801 / 14700) loss: 0.752019\n",
      "(Iteration 5802 / 14700) loss: 0.934387\n",
      "(Iteration 5803 / 14700) loss: 0.688859\n",
      "(Iteration 5804 / 14700) loss: 0.799528\n",
      "(Iteration 5805 / 14700) loss: 0.668117\n",
      "(Iteration 5806 / 14700) loss: 0.998525\n",
      "(Iteration 5807 / 14700) loss: 0.682843\n",
      "(Iteration 5808 / 14700) loss: 0.698861\n",
      "(Iteration 5809 / 14700) loss: 0.647033\n",
      "(Iteration 5810 / 14700) loss: 0.874967\n",
      "(Iteration 5811 / 14700) loss: 0.945978\n",
      "(Iteration 5812 / 14700) loss: 1.025589\n",
      "(Iteration 5813 / 14700) loss: 0.729904\n",
      "(Iteration 5814 / 14700) loss: 0.761326\n",
      "(Iteration 5815 / 14700) loss: 0.863464\n",
      "(Iteration 5816 / 14700) loss: 0.891189\n",
      "(Iteration 5817 / 14700) loss: 1.144518\n",
      "(Iteration 5818 / 14700) loss: 0.665921\n",
      "(Iteration 5819 / 14700) loss: 0.735911\n",
      "(Iteration 5820 / 14700) loss: 1.275993\n",
      "(Iteration 5821 / 14700) loss: 0.745442\n",
      "(Iteration 5822 / 14700) loss: 0.533629\n",
      "(Iteration 5823 / 14700) loss: 0.838779\n",
      "(Iteration 5824 / 14700) loss: 0.788792\n",
      "(Iteration 5825 / 14700) loss: 0.655934\n",
      "(Iteration 5826 / 14700) loss: 0.876748\n",
      "(Iteration 5827 / 14700) loss: 0.817274\n",
      "(Iteration 5828 / 14700) loss: 0.581675\n",
      "(Iteration 5829 / 14700) loss: 0.858865\n",
      "(Iteration 5830 / 14700) loss: 1.041998\n",
      "(Iteration 5831 / 14700) loss: 0.607906\n",
      "(Iteration 5832 / 14700) loss: 0.909241\n",
      "(Iteration 5833 / 14700) loss: 0.601262\n",
      "(Iteration 5834 / 14700) loss: 1.143565\n",
      "(Iteration 5835 / 14700) loss: 1.019961\n",
      "(Iteration 5836 / 14700) loss: 0.814534\n",
      "(Iteration 5837 / 14700) loss: 0.880390\n",
      "(Iteration 5838 / 14700) loss: 0.682506\n",
      "(Iteration 5839 / 14700) loss: 0.645887\n",
      "(Iteration 5840 / 14700) loss: 0.529070\n",
      "(Iteration 5841 / 14700) loss: 0.909409\n",
      "(Iteration 5842 / 14700) loss: 0.513614\n",
      "(Iteration 5843 / 14700) loss: 1.070792\n",
      "(Iteration 5844 / 14700) loss: 0.851120\n",
      "(Iteration 5845 / 14700) loss: 0.908897\n",
      "(Iteration 5846 / 14700) loss: 0.797317\n",
      "(Iteration 5847 / 14700) loss: 0.711437\n",
      "(Iteration 5848 / 14700) loss: 0.648554\n",
      "(Iteration 5849 / 14700) loss: 0.931516\n",
      "(Iteration 5850 / 14700) loss: 0.967962\n",
      "(Iteration 5851 / 14700) loss: 0.884739\n",
      "(Iteration 5852 / 14700) loss: 0.908314\n",
      "(Iteration 5853 / 14700) loss: 0.599263\n",
      "(Iteration 5854 / 14700) loss: 1.029442\n",
      "(Iteration 5855 / 14700) loss: 0.886396\n",
      "(Iteration 5856 / 14700) loss: 0.721330\n",
      "(Iteration 5857 / 14700) loss: 0.771697\n",
      "(Iteration 5858 / 14700) loss: 0.768490\n",
      "(Iteration 5859 / 14700) loss: 0.871847\n",
      "(Iteration 5860 / 14700) loss: 0.707336\n",
      "(Iteration 5861 / 14700) loss: 0.614061\n",
      "(Iteration 5862 / 14700) loss: 1.060193\n",
      "(Iteration 5863 / 14700) loss: 0.834227\n",
      "(Iteration 5864 / 14700) loss: 0.871607\n",
      "(Iteration 5865 / 14700) loss: 1.045271\n",
      "(Iteration 5866 / 14700) loss: 0.687355\n",
      "(Iteration 5867 / 14700) loss: 0.817082\n",
      "(Iteration 5868 / 14700) loss: 0.958204\n",
      "(Iteration 5869 / 14700) loss: 0.665462\n",
      "(Iteration 5870 / 14700) loss: 0.742378\n",
      "(Iteration 5871 / 14700) loss: 0.885855\n",
      "(Iteration 5872 / 14700) loss: 0.750855\n",
      "(Iteration 5873 / 14700) loss: 0.571651\n",
      "(Iteration 5874 / 14700) loss: 0.892033\n",
      "(Iteration 5875 / 14700) loss: 0.849619\n",
      "(Iteration 5876 / 14700) loss: 0.690701\n",
      "(Iteration 5877 / 14700) loss: 0.958029\n",
      "(Iteration 5878 / 14700) loss: 0.772935\n",
      "(Iteration 5879 / 14700) loss: 1.073414\n",
      "(Iteration 5880 / 14700) loss: 0.518401\n",
      "(Epoch 6 / 15) train acc: 0.800000; val_acc: 0.743000\n",
      "(Iteration 5881 / 14700) loss: 0.675081\n",
      "(Iteration 5882 / 14700) loss: 0.780630\n",
      "(Iteration 5883 / 14700) loss: 0.704579\n",
      "(Iteration 5884 / 14700) loss: 0.785705\n",
      "(Iteration 5885 / 14700) loss: 0.852639\n",
      "(Iteration 5886 / 14700) loss: 0.779597\n",
      "(Iteration 5887 / 14700) loss: 0.982003\n",
      "(Iteration 5888 / 14700) loss: 0.805162\n",
      "(Iteration 5889 / 14700) loss: 0.679511\n",
      "(Iteration 5890 / 14700) loss: 1.039288\n",
      "(Iteration 5891 / 14700) loss: 0.982701\n",
      "(Iteration 5892 / 14700) loss: 0.735415\n",
      "(Iteration 5893 / 14700) loss: 0.719769\n",
      "(Iteration 5894 / 14700) loss: 0.646484\n",
      "(Iteration 5895 / 14700) loss: 0.875243\n",
      "(Iteration 5896 / 14700) loss: 0.654247\n",
      "(Iteration 5897 / 14700) loss: 0.906120\n",
      "(Iteration 5898 / 14700) loss: 0.767731\n",
      "(Iteration 5899 / 14700) loss: 0.854153\n",
      "(Iteration 5900 / 14700) loss: 0.631745\n",
      "(Iteration 5901 / 14700) loss: 0.793389\n",
      "(Iteration 5902 / 14700) loss: 0.836862\n",
      "(Iteration 5903 / 14700) loss: 0.779075\n",
      "(Iteration 5904 / 14700) loss: 0.963526\n",
      "(Iteration 5905 / 14700) loss: 0.739312\n",
      "(Iteration 5906 / 14700) loss: 0.745515\n",
      "(Iteration 5907 / 14700) loss: 0.752887\n",
      "(Iteration 5908 / 14700) loss: 0.862945\n",
      "(Iteration 5909 / 14700) loss: 0.739041\n",
      "(Iteration 5910 / 14700) loss: 0.811015\n",
      "(Iteration 5911 / 14700) loss: 0.955559\n",
      "(Iteration 5912 / 14700) loss: 0.806692\n",
      "(Iteration 5913 / 14700) loss: 0.742797\n",
      "(Iteration 5914 / 14700) loss: 0.904629\n",
      "(Iteration 5915 / 14700) loss: 0.780571\n",
      "(Iteration 5916 / 14700) loss: 0.702116\n",
      "(Iteration 5917 / 14700) loss: 0.688407\n",
      "(Iteration 5918 / 14700) loss: 0.757053\n",
      "(Iteration 5919 / 14700) loss: 0.753711\n",
      "(Iteration 5920 / 14700) loss: 0.613898\n",
      "(Iteration 5921 / 14700) loss: 0.876203\n",
      "(Iteration 5922 / 14700) loss: 0.643594\n",
      "(Iteration 5923 / 14700) loss: 0.617389\n",
      "(Iteration 5924 / 14700) loss: 0.677283\n",
      "(Iteration 5925 / 14700) loss: 0.695086\n",
      "(Iteration 5926 / 14700) loss: 1.013801\n",
      "(Iteration 5927 / 14700) loss: 0.770305\n",
      "(Iteration 5928 / 14700) loss: 0.809347\n",
      "(Iteration 5929 / 14700) loss: 0.826956\n",
      "(Iteration 5930 / 14700) loss: 0.675337\n",
      "(Iteration 5931 / 14700) loss: 0.790173\n",
      "(Iteration 5932 / 14700) loss: 0.705694\n",
      "(Iteration 5933 / 14700) loss: 0.744470\n",
      "(Iteration 5934 / 14700) loss: 0.746378\n",
      "(Iteration 5935 / 14700) loss: 1.067052\n",
      "(Iteration 5936 / 14700) loss: 0.500366\n",
      "(Iteration 5937 / 14700) loss: 0.674517\n",
      "(Iteration 5938 / 14700) loss: 0.855831\n",
      "(Iteration 5939 / 14700) loss: 0.624315\n",
      "(Iteration 5940 / 14700) loss: 0.747481\n",
      "(Iteration 5941 / 14700) loss: 0.924586\n",
      "(Iteration 5942 / 14700) loss: 0.883792\n",
      "(Iteration 5943 / 14700) loss: 1.054617\n",
      "(Iteration 5944 / 14700) loss: 0.581323\n",
      "(Iteration 5945 / 14700) loss: 0.844504\n",
      "(Iteration 5946 / 14700) loss: 0.696610\n",
      "(Iteration 5947 / 14700) loss: 0.812995\n",
      "(Iteration 5948 / 14700) loss: 0.897689\n",
      "(Iteration 5949 / 14700) loss: 0.945749\n",
      "(Iteration 5950 / 14700) loss: 1.027719\n",
      "(Iteration 5951 / 14700) loss: 0.880152\n",
      "(Iteration 5952 / 14700) loss: 0.908720\n",
      "(Iteration 5953 / 14700) loss: 0.825650\n",
      "(Iteration 5954 / 14700) loss: 0.832894\n",
      "(Iteration 5955 / 14700) loss: 0.705973\n",
      "(Iteration 5956 / 14700) loss: 0.633865\n",
      "(Iteration 5957 / 14700) loss: 0.869193\n",
      "(Iteration 5958 / 14700) loss: 0.639231\n",
      "(Iteration 5959 / 14700) loss: 0.651541\n",
      "(Iteration 5960 / 14700) loss: 0.545842\n",
      "(Iteration 5961 / 14700) loss: 0.887621\n",
      "(Iteration 5962 / 14700) loss: 0.751421\n",
      "(Iteration 5963 / 14700) loss: 0.961994\n",
      "(Iteration 5964 / 14700) loss: 0.646174\n",
      "(Iteration 5965 / 14700) loss: 0.705750\n",
      "(Iteration 5966 / 14700) loss: 0.842683\n",
      "(Iteration 5967 / 14700) loss: 0.706368\n",
      "(Iteration 5968 / 14700) loss: 0.608963\n",
      "(Iteration 5969 / 14700) loss: 0.807788\n",
      "(Iteration 5970 / 14700) loss: 0.669531\n",
      "(Iteration 5971 / 14700) loss: 0.842391\n",
      "(Iteration 5972 / 14700) loss: 0.665532\n",
      "(Iteration 5973 / 14700) loss: 0.834163\n",
      "(Iteration 5974 / 14700) loss: 0.886103\n",
      "(Iteration 5975 / 14700) loss: 0.975193\n",
      "(Iteration 5976 / 14700) loss: 0.874985\n",
      "(Iteration 5977 / 14700) loss: 0.946198\n",
      "(Iteration 5978 / 14700) loss: 0.757064\n",
      "(Iteration 5979 / 14700) loss: 0.769526\n",
      "(Iteration 5980 / 14700) loss: 1.040015\n",
      "(Iteration 5981 / 14700) loss: 0.744132\n",
      "(Iteration 5982 / 14700) loss: 0.617612\n",
      "(Iteration 5983 / 14700) loss: 0.780813\n",
      "(Iteration 5984 / 14700) loss: 0.762069\n",
      "(Iteration 5985 / 14700) loss: 0.956370\n",
      "(Iteration 5986 / 14700) loss: 0.724165\n",
      "(Iteration 5987 / 14700) loss: 1.009193\n",
      "(Iteration 5988 / 14700) loss: 0.798227\n",
      "(Iteration 5989 / 14700) loss: 0.918814\n",
      "(Iteration 5990 / 14700) loss: 1.033710\n",
      "(Iteration 5991 / 14700) loss: 0.688892\n",
      "(Iteration 5992 / 14700) loss: 0.950424\n",
      "(Iteration 5993 / 14700) loss: 0.754965\n",
      "(Iteration 5994 / 14700) loss: 0.955234\n",
      "(Iteration 5995 / 14700) loss: 0.876364\n",
      "(Iteration 5996 / 14700) loss: 1.053630\n",
      "(Iteration 5997 / 14700) loss: 0.884695\n",
      "(Iteration 5998 / 14700) loss: 1.115787\n",
      "(Iteration 5999 / 14700) loss: 0.565584\n",
      "(Iteration 6000 / 14700) loss: 0.855207\n",
      "(Iteration 6001 / 14700) loss: 0.716371\n",
      "(Iteration 6002 / 14700) loss: 0.733001\n",
      "(Iteration 6003 / 14700) loss: 0.739638\n",
      "(Iteration 6004 / 14700) loss: 0.798068\n",
      "(Iteration 6005 / 14700) loss: 0.843558\n",
      "(Iteration 6006 / 14700) loss: 0.963895\n",
      "(Iteration 6007 / 14700) loss: 0.965083\n",
      "(Iteration 6008 / 14700) loss: 0.796596\n",
      "(Iteration 6009 / 14700) loss: 0.931105\n",
      "(Iteration 6010 / 14700) loss: 0.897710\n",
      "(Iteration 6011 / 14700) loss: 0.792001\n",
      "(Iteration 6012 / 14700) loss: 0.660192\n",
      "(Iteration 6013 / 14700) loss: 0.597574\n",
      "(Iteration 6014 / 14700) loss: 1.210746\n",
      "(Iteration 6015 / 14700) loss: 0.776387\n",
      "(Iteration 6016 / 14700) loss: 0.776243\n",
      "(Iteration 6017 / 14700) loss: 0.865005\n",
      "(Iteration 6018 / 14700) loss: 0.722058\n",
      "(Iteration 6019 / 14700) loss: 0.672118\n",
      "(Iteration 6020 / 14700) loss: 0.921760\n",
      "(Iteration 6021 / 14700) loss: 0.594389\n",
      "(Iteration 6022 / 14700) loss: 0.847253\n",
      "(Iteration 6023 / 14700) loss: 0.740680\n",
      "(Iteration 6024 / 14700) loss: 0.671254\n",
      "(Iteration 6025 / 14700) loss: 0.943693\n",
      "(Iteration 6026 / 14700) loss: 0.857107\n",
      "(Iteration 6027 / 14700) loss: 0.815671\n",
      "(Iteration 6028 / 14700) loss: 0.969401\n",
      "(Iteration 6029 / 14700) loss: 1.240099\n",
      "(Iteration 6030 / 14700) loss: 0.844892\n",
      "(Iteration 6031 / 14700) loss: 0.782819\n",
      "(Iteration 6032 / 14700) loss: 0.928510\n",
      "(Iteration 6033 / 14700) loss: 1.005480\n",
      "(Iteration 6034 / 14700) loss: 0.735945\n",
      "(Iteration 6035 / 14700) loss: 0.916427\n",
      "(Iteration 6036 / 14700) loss: 0.980255\n",
      "(Iteration 6037 / 14700) loss: 0.999569\n",
      "(Iteration 6038 / 14700) loss: 0.750185\n",
      "(Iteration 6039 / 14700) loss: 0.707739\n",
      "(Iteration 6040 / 14700) loss: 1.144739\n",
      "(Iteration 6041 / 14700) loss: 0.793471\n",
      "(Iteration 6042 / 14700) loss: 1.136529\n",
      "(Iteration 6043 / 14700) loss: 0.937907\n",
      "(Iteration 6044 / 14700) loss: 0.807126\n",
      "(Iteration 6045 / 14700) loss: 0.770523\n",
      "(Iteration 6046 / 14700) loss: 0.843376\n",
      "(Iteration 6047 / 14700) loss: 1.106108\n",
      "(Iteration 6048 / 14700) loss: 0.970547\n",
      "(Iteration 6049 / 14700) loss: 0.732224\n",
      "(Iteration 6050 / 14700) loss: 0.645525\n",
      "(Iteration 6051 / 14700) loss: 0.843243\n",
      "(Iteration 6052 / 14700) loss: 0.664188\n",
      "(Iteration 6053 / 14700) loss: 0.973383\n",
      "(Iteration 6054 / 14700) loss: 0.859285\n",
      "(Iteration 6055 / 14700) loss: 0.921195\n",
      "(Iteration 6056 / 14700) loss: 0.933851\n",
      "(Iteration 6057 / 14700) loss: 0.661046\n",
      "(Iteration 6058 / 14700) loss: 0.794037\n",
      "(Iteration 6059 / 14700) loss: 0.786517\n",
      "(Iteration 6060 / 14700) loss: 0.847961\n",
      "(Iteration 6061 / 14700) loss: 0.876463\n",
      "(Iteration 6062 / 14700) loss: 0.732796\n",
      "(Iteration 6063 / 14700) loss: 0.884115\n",
      "(Iteration 6064 / 14700) loss: 0.811204\n",
      "(Iteration 6065 / 14700) loss: 0.826136\n",
      "(Iteration 6066 / 14700) loss: 0.592677\n",
      "(Iteration 6067 / 14700) loss: 0.855501\n",
      "(Iteration 6068 / 14700) loss: 0.791762\n",
      "(Iteration 6069 / 14700) loss: 0.692347\n",
      "(Iteration 6070 / 14700) loss: 0.577965\n",
      "(Iteration 6071 / 14700) loss: 0.791767\n",
      "(Iteration 6072 / 14700) loss: 0.972098\n",
      "(Iteration 6073 / 14700) loss: 1.015825\n",
      "(Iteration 6074 / 14700) loss: 0.822767\n",
      "(Iteration 6075 / 14700) loss: 0.790359\n",
      "(Iteration 6076 / 14700) loss: 0.993919\n",
      "(Iteration 6077 / 14700) loss: 0.538493\n",
      "(Iteration 6078 / 14700) loss: 0.682701\n",
      "(Iteration 6079 / 14700) loss: 0.817062\n",
      "(Iteration 6080 / 14700) loss: 1.000046\n",
      "(Iteration 6081 / 14700) loss: 1.008355\n",
      "(Iteration 6082 / 14700) loss: 0.844104\n",
      "(Iteration 6083 / 14700) loss: 0.776119\n",
      "(Iteration 6084 / 14700) loss: 0.937905\n",
      "(Iteration 6085 / 14700) loss: 0.775543\n",
      "(Iteration 6086 / 14700) loss: 0.836962\n",
      "(Iteration 6087 / 14700) loss: 0.950052\n",
      "(Iteration 6088 / 14700) loss: 0.716431\n",
      "(Iteration 6089 / 14700) loss: 0.908438\n",
      "(Iteration 6090 / 14700) loss: 0.948328\n",
      "(Iteration 6091 / 14700) loss: 0.863306\n",
      "(Iteration 6092 / 14700) loss: 0.631700\n",
      "(Iteration 6093 / 14700) loss: 0.933741\n",
      "(Iteration 6094 / 14700) loss: 0.990567\n",
      "(Iteration 6095 / 14700) loss: 0.829708\n",
      "(Iteration 6096 / 14700) loss: 0.668610\n",
      "(Iteration 6097 / 14700) loss: 0.805626\n",
      "(Iteration 6098 / 14700) loss: 0.865587\n",
      "(Iteration 6099 / 14700) loss: 0.593813\n",
      "(Iteration 6100 / 14700) loss: 0.847302\n",
      "(Iteration 6101 / 14700) loss: 0.650341\n",
      "(Iteration 6102 / 14700) loss: 0.958694\n",
      "(Iteration 6103 / 14700) loss: 0.837709\n",
      "(Iteration 6104 / 14700) loss: 0.729307\n",
      "(Iteration 6105 / 14700) loss: 0.818715\n",
      "(Iteration 6106 / 14700) loss: 0.772972\n",
      "(Iteration 6107 / 14700) loss: 0.840580\n",
      "(Iteration 6108 / 14700) loss: 0.727922\n",
      "(Iteration 6109 / 14700) loss: 0.717646\n",
      "(Iteration 6110 / 14700) loss: 0.823268\n",
      "(Iteration 6111 / 14700) loss: 0.804822\n",
      "(Iteration 6112 / 14700) loss: 0.744388\n",
      "(Iteration 6113 / 14700) loss: 0.848086\n",
      "(Iteration 6114 / 14700) loss: 0.851537\n",
      "(Iteration 6115 / 14700) loss: 0.581050\n",
      "(Iteration 6116 / 14700) loss: 0.983835\n",
      "(Iteration 6117 / 14700) loss: 0.856588\n",
      "(Iteration 6118 / 14700) loss: 0.701447\n",
      "(Iteration 6119 / 14700) loss: 0.800339\n",
      "(Iteration 6120 / 14700) loss: 0.643237\n",
      "(Iteration 6121 / 14700) loss: 0.939157\n",
      "(Iteration 6122 / 14700) loss: 0.755581\n",
      "(Iteration 6123 / 14700) loss: 0.780484\n",
      "(Iteration 6124 / 14700) loss: 0.837065\n",
      "(Iteration 6125 / 14700) loss: 0.716990\n",
      "(Iteration 6126 / 14700) loss: 0.858222\n",
      "(Iteration 6127 / 14700) loss: 0.565890\n",
      "(Iteration 6128 / 14700) loss: 0.664168\n",
      "(Iteration 6129 / 14700) loss: 0.478839\n",
      "(Iteration 6130 / 14700) loss: 0.789293\n",
      "(Iteration 6131 / 14700) loss: 0.783340\n",
      "(Iteration 6132 / 14700) loss: 0.744763\n",
      "(Iteration 6133 / 14700) loss: 0.944529\n",
      "(Iteration 6134 / 14700) loss: 0.725042\n",
      "(Iteration 6135 / 14700) loss: 0.493898\n",
      "(Iteration 6136 / 14700) loss: 0.906684\n",
      "(Iteration 6137 / 14700) loss: 0.779685\n",
      "(Iteration 6138 / 14700) loss: 0.705050\n",
      "(Iteration 6139 / 14700) loss: 0.652743\n",
      "(Iteration 6140 / 14700) loss: 0.712564\n",
      "(Iteration 6141 / 14700) loss: 0.955934\n",
      "(Iteration 6142 / 14700) loss: 0.965760\n",
      "(Iteration 6143 / 14700) loss: 0.743245\n",
      "(Iteration 6144 / 14700) loss: 0.696675\n",
      "(Iteration 6145 / 14700) loss: 0.706979\n",
      "(Iteration 6146 / 14700) loss: 0.741445\n",
      "(Iteration 6147 / 14700) loss: 0.986336\n",
      "(Iteration 6148 / 14700) loss: 1.069154\n",
      "(Iteration 6149 / 14700) loss: 0.943504\n",
      "(Iteration 6150 / 14700) loss: 0.784713\n",
      "(Iteration 6151 / 14700) loss: 0.774870\n",
      "(Iteration 6152 / 14700) loss: 0.963550\n",
      "(Iteration 6153 / 14700) loss: 0.704610\n",
      "(Iteration 6154 / 14700) loss: 0.871355\n",
      "(Iteration 6155 / 14700) loss: 0.892253\n",
      "(Iteration 6156 / 14700) loss: 0.865839\n",
      "(Iteration 6157 / 14700) loss: 0.926199\n",
      "(Iteration 6158 / 14700) loss: 1.169569\n",
      "(Iteration 6159 / 14700) loss: 0.727994\n",
      "(Iteration 6160 / 14700) loss: 0.808797\n",
      "(Iteration 6161 / 14700) loss: 1.013262\n",
      "(Iteration 6162 / 14700) loss: 0.841876\n",
      "(Iteration 6163 / 14700) loss: 0.750091\n",
      "(Iteration 6164 / 14700) loss: 0.549020\n",
      "(Iteration 6165 / 14700) loss: 0.991779\n",
      "(Iteration 6166 / 14700) loss: 0.847886\n",
      "(Iteration 6167 / 14700) loss: 0.796821\n",
      "(Iteration 6168 / 14700) loss: 0.920645\n",
      "(Iteration 6169 / 14700) loss: 0.670998\n",
      "(Iteration 6170 / 14700) loss: 0.682655\n",
      "(Iteration 6171 / 14700) loss: 0.763319\n",
      "(Iteration 6172 / 14700) loss: 0.768422\n",
      "(Iteration 6173 / 14700) loss: 1.042640\n",
      "(Iteration 6174 / 14700) loss: 0.740403\n",
      "(Iteration 6175 / 14700) loss: 0.911010\n",
      "(Iteration 6176 / 14700) loss: 0.791423\n",
      "(Iteration 6177 / 14700) loss: 0.886878\n",
      "(Iteration 6178 / 14700) loss: 0.694737\n",
      "(Iteration 6179 / 14700) loss: 0.736615\n",
      "(Iteration 6180 / 14700) loss: 0.927463\n",
      "(Iteration 6181 / 14700) loss: 0.928234\n",
      "(Iteration 6182 / 14700) loss: 0.939307\n",
      "(Iteration 6183 / 14700) loss: 0.867941\n",
      "(Iteration 6184 / 14700) loss: 0.751110\n",
      "(Iteration 6185 / 14700) loss: 0.669259\n",
      "(Iteration 6186 / 14700) loss: 0.864298\n",
      "(Iteration 6187 / 14700) loss: 0.779891\n",
      "(Iteration 6188 / 14700) loss: 0.716229\n",
      "(Iteration 6189 / 14700) loss: 0.800449\n",
      "(Iteration 6190 / 14700) loss: 0.610870\n",
      "(Iteration 6191 / 14700) loss: 0.704168\n",
      "(Iteration 6192 / 14700) loss: 0.821002\n",
      "(Iteration 6193 / 14700) loss: 0.657317\n",
      "(Iteration 6194 / 14700) loss: 0.675479\n",
      "(Iteration 6195 / 14700) loss: 0.695493\n",
      "(Iteration 6196 / 14700) loss: 1.083468\n",
      "(Iteration 6197 / 14700) loss: 1.090515\n",
      "(Iteration 6198 / 14700) loss: 0.763032\n",
      "(Iteration 6199 / 14700) loss: 0.862140\n",
      "(Iteration 6200 / 14700) loss: 0.837320\n",
      "(Iteration 6201 / 14700) loss: 0.588732\n",
      "(Iteration 6202 / 14700) loss: 0.991728\n",
      "(Iteration 6203 / 14700) loss: 0.806307\n",
      "(Iteration 6204 / 14700) loss: 0.651367\n",
      "(Iteration 6205 / 14700) loss: 0.924913\n",
      "(Iteration 6206 / 14700) loss: 0.858722\n",
      "(Iteration 6207 / 14700) loss: 0.891902\n",
      "(Iteration 6208 / 14700) loss: 0.753234\n",
      "(Iteration 6209 / 14700) loss: 0.786610\n",
      "(Iteration 6210 / 14700) loss: 0.642365\n",
      "(Iteration 6211 / 14700) loss: 0.786537\n",
      "(Iteration 6212 / 14700) loss: 0.858619\n",
      "(Iteration 6213 / 14700) loss: 0.731176\n",
      "(Iteration 6214 / 14700) loss: 0.787145\n",
      "(Iteration 6215 / 14700) loss: 0.825119\n",
      "(Iteration 6216 / 14700) loss: 0.834705\n",
      "(Iteration 6217 / 14700) loss: 0.932563\n",
      "(Iteration 6218 / 14700) loss: 0.687346\n",
      "(Iteration 6219 / 14700) loss: 0.785299\n",
      "(Iteration 6220 / 14700) loss: 0.755714\n",
      "(Iteration 6221 / 14700) loss: 0.769253\n",
      "(Iteration 6222 / 14700) loss: 0.707232\n",
      "(Iteration 6223 / 14700) loss: 0.649340\n",
      "(Iteration 6224 / 14700) loss: 0.853852\n",
      "(Iteration 6225 / 14700) loss: 0.859667\n",
      "(Iteration 6226 / 14700) loss: 0.886011\n",
      "(Iteration 6227 / 14700) loss: 0.764373\n",
      "(Iteration 6228 / 14700) loss: 0.936751\n",
      "(Iteration 6229 / 14700) loss: 0.696458\n",
      "(Iteration 6230 / 14700) loss: 0.867375\n",
      "(Iteration 6231 / 14700) loss: 1.054040\n",
      "(Iteration 6232 / 14700) loss: 0.893244\n",
      "(Iteration 6233 / 14700) loss: 1.046551\n",
      "(Iteration 6234 / 14700) loss: 0.789686\n",
      "(Iteration 6235 / 14700) loss: 0.766228\n",
      "(Iteration 6236 / 14700) loss: 1.004265\n",
      "(Iteration 6237 / 14700) loss: 0.748639\n",
      "(Iteration 6238 / 14700) loss: 0.892717\n",
      "(Iteration 6239 / 14700) loss: 1.001725\n",
      "(Iteration 6240 / 14700) loss: 0.791826\n",
      "(Iteration 6241 / 14700) loss: 0.622249\n",
      "(Iteration 6242 / 14700) loss: 0.531828\n",
      "(Iteration 6243 / 14700) loss: 0.945555\n",
      "(Iteration 6244 / 14700) loss: 0.770223\n",
      "(Iteration 6245 / 14700) loss: 0.765060\n",
      "(Iteration 6246 / 14700) loss: 0.874368\n",
      "(Iteration 6247 / 14700) loss: 0.936784\n",
      "(Iteration 6248 / 14700) loss: 0.741675\n",
      "(Iteration 6249 / 14700) loss: 0.691015\n",
      "(Iteration 6250 / 14700) loss: 0.527290\n",
      "(Iteration 6251 / 14700) loss: 0.595263\n",
      "(Iteration 6252 / 14700) loss: 1.209017\n",
      "(Iteration 6253 / 14700) loss: 0.740890\n",
      "(Iteration 6254 / 14700) loss: 0.786780\n",
      "(Iteration 6255 / 14700) loss: 0.728736\n",
      "(Iteration 6256 / 14700) loss: 0.775927\n",
      "(Iteration 6257 / 14700) loss: 0.800428\n",
      "(Iteration 6258 / 14700) loss: 0.747866\n",
      "(Iteration 6259 / 14700) loss: 0.677048\n",
      "(Iteration 6260 / 14700) loss: 0.860638\n",
      "(Iteration 6261 / 14700) loss: 0.611361\n",
      "(Iteration 6262 / 14700) loss: 0.815786\n",
      "(Iteration 6263 / 14700) loss: 0.985648\n",
      "(Iteration 6264 / 14700) loss: 0.735673\n",
      "(Iteration 6265 / 14700) loss: 0.867891\n",
      "(Iteration 6266 / 14700) loss: 0.705672\n",
      "(Iteration 6267 / 14700) loss: 0.537879\n",
      "(Iteration 6268 / 14700) loss: 1.075173\n",
      "(Iteration 6269 / 14700) loss: 0.716885\n",
      "(Iteration 6270 / 14700) loss: 0.699087\n",
      "(Iteration 6271 / 14700) loss: 0.961934\n",
      "(Iteration 6272 / 14700) loss: 0.897049\n",
      "(Iteration 6273 / 14700) loss: 0.707338\n",
      "(Iteration 6274 / 14700) loss: 0.754689\n",
      "(Iteration 6275 / 14700) loss: 0.660395\n",
      "(Iteration 6276 / 14700) loss: 0.757239\n",
      "(Iteration 6277 / 14700) loss: 0.739360\n",
      "(Iteration 6278 / 14700) loss: 0.796521\n",
      "(Iteration 6279 / 14700) loss: 0.679206\n",
      "(Iteration 6280 / 14700) loss: 0.779859\n",
      "(Iteration 6281 / 14700) loss: 0.892072\n",
      "(Iteration 6282 / 14700) loss: 0.896675\n",
      "(Iteration 6283 / 14700) loss: 0.659000\n",
      "(Iteration 6284 / 14700) loss: 1.044193\n",
      "(Iteration 6285 / 14700) loss: 0.794686\n",
      "(Iteration 6286 / 14700) loss: 0.958242\n",
      "(Iteration 6287 / 14700) loss: 0.833196\n",
      "(Iteration 6288 / 14700) loss: 0.949587\n",
      "(Iteration 6289 / 14700) loss: 0.621266\n",
      "(Iteration 6290 / 14700) loss: 0.811474\n",
      "(Iteration 6291 / 14700) loss: 0.610644\n",
      "(Iteration 6292 / 14700) loss: 0.647656\n",
      "(Iteration 6293 / 14700) loss: 0.632949\n",
      "(Iteration 6294 / 14700) loss: 0.688928\n",
      "(Iteration 6295 / 14700) loss: 0.900167\n",
      "(Iteration 6296 / 14700) loss: 0.935988\n",
      "(Iteration 6297 / 14700) loss: 0.675119\n",
      "(Iteration 6298 / 14700) loss: 0.750354\n",
      "(Iteration 6299 / 14700) loss: 0.619084\n",
      "(Iteration 6300 / 14700) loss: 0.970568\n",
      "(Iteration 6301 / 14700) loss: 0.935505\n",
      "(Iteration 6302 / 14700) loss: 0.890984\n",
      "(Iteration 6303 / 14700) loss: 0.878044\n",
      "(Iteration 6304 / 14700) loss: 0.578546\n",
      "(Iteration 6305 / 14700) loss: 0.904619\n",
      "(Iteration 6306 / 14700) loss: 1.144208\n",
      "(Iteration 6307 / 14700) loss: 0.817022\n",
      "(Iteration 6308 / 14700) loss: 0.954899\n",
      "(Iteration 6309 / 14700) loss: 0.989109\n",
      "(Iteration 6310 / 14700) loss: 0.752440\n",
      "(Iteration 6311 / 14700) loss: 0.812388\n",
      "(Iteration 6312 / 14700) loss: 0.785136\n",
      "(Iteration 6313 / 14700) loss: 0.786420\n",
      "(Iteration 6314 / 14700) loss: 0.777960\n",
      "(Iteration 6315 / 14700) loss: 0.551432\n",
      "(Iteration 6316 / 14700) loss: 0.903129\n",
      "(Iteration 6317 / 14700) loss: 0.821915\n",
      "(Iteration 6318 / 14700) loss: 0.867772\n",
      "(Iteration 6319 / 14700) loss: 0.611984\n",
      "(Iteration 6320 / 14700) loss: 0.769936\n",
      "(Iteration 6321 / 14700) loss: 1.099677\n",
      "(Iteration 6322 / 14700) loss: 0.847288\n",
      "(Iteration 6323 / 14700) loss: 1.050970\n",
      "(Iteration 6324 / 14700) loss: 0.999056\n",
      "(Iteration 6325 / 14700) loss: 0.629113\n",
      "(Iteration 6326 / 14700) loss: 0.918898\n",
      "(Iteration 6327 / 14700) loss: 0.777735\n",
      "(Iteration 6328 / 14700) loss: 1.001512\n",
      "(Iteration 6329 / 14700) loss: 0.937571\n",
      "(Iteration 6330 / 14700) loss: 0.842636\n",
      "(Iteration 6331 / 14700) loss: 0.805529\n",
      "(Iteration 6332 / 14700) loss: 0.719006\n",
      "(Iteration 6333 / 14700) loss: 1.019464\n",
      "(Iteration 6334 / 14700) loss: 0.736250\n",
      "(Iteration 6335 / 14700) loss: 0.751569\n",
      "(Iteration 6336 / 14700) loss: 0.505800\n",
      "(Iteration 6337 / 14700) loss: 0.676158\n",
      "(Iteration 6338 / 14700) loss: 0.837657\n",
      "(Iteration 6339 / 14700) loss: 1.095885\n",
      "(Iteration 6340 / 14700) loss: 0.794679\n",
      "(Iteration 6341 / 14700) loss: 0.889911\n",
      "(Iteration 6342 / 14700) loss: 0.686136\n",
      "(Iteration 6343 / 14700) loss: 0.698923\n",
      "(Iteration 6344 / 14700) loss: 0.683352\n",
      "(Iteration 6345 / 14700) loss: 0.606896\n",
      "(Iteration 6346 / 14700) loss: 0.973642\n",
      "(Iteration 6347 / 14700) loss: 0.871742\n",
      "(Iteration 6348 / 14700) loss: 0.729325\n",
      "(Iteration 6349 / 14700) loss: 0.876317\n",
      "(Iteration 6350 / 14700) loss: 0.870554\n",
      "(Iteration 6351 / 14700) loss: 1.013613\n",
      "(Iteration 6352 / 14700) loss: 0.629457\n",
      "(Iteration 6353 / 14700) loss: 0.756055\n",
      "(Iteration 6354 / 14700) loss: 1.146055\n",
      "(Iteration 6355 / 14700) loss: 0.737277\n",
      "(Iteration 6356 / 14700) loss: 0.726724\n",
      "(Iteration 6357 / 14700) loss: 0.907901\n",
      "(Iteration 6358 / 14700) loss: 0.850327\n",
      "(Iteration 6359 / 14700) loss: 0.850927\n",
      "(Iteration 6360 / 14700) loss: 0.685408\n",
      "(Iteration 6361 / 14700) loss: 0.787262\n",
      "(Iteration 6362 / 14700) loss: 0.902192\n",
      "(Iteration 6363 / 14700) loss: 1.006306\n",
      "(Iteration 6364 / 14700) loss: 0.766563\n",
      "(Iteration 6365 / 14700) loss: 0.649742\n",
      "(Iteration 6366 / 14700) loss: 0.711239\n",
      "(Iteration 6367 / 14700) loss: 0.828259\n",
      "(Iteration 6368 / 14700) loss: 0.704753\n",
      "(Iteration 6369 / 14700) loss: 0.762511\n",
      "(Iteration 6370 / 14700) loss: 0.878269\n",
      "(Iteration 6371 / 14700) loss: 0.905763\n",
      "(Iteration 6372 / 14700) loss: 0.991815\n",
      "(Iteration 6373 / 14700) loss: 0.718703\n",
      "(Iteration 6374 / 14700) loss: 0.626303\n",
      "(Iteration 6375 / 14700) loss: 0.688576\n",
      "(Iteration 6376 / 14700) loss: 0.822317\n",
      "(Iteration 6377 / 14700) loss: 0.847921\n",
      "(Iteration 6378 / 14700) loss: 0.941966\n",
      "(Iteration 6379 / 14700) loss: 1.069867\n",
      "(Iteration 6380 / 14700) loss: 0.680772\n",
      "(Iteration 6381 / 14700) loss: 0.801263\n",
      "(Iteration 6382 / 14700) loss: 0.724209\n",
      "(Iteration 6383 / 14700) loss: 0.863884\n",
      "(Iteration 6384 / 14700) loss: 0.703572\n",
      "(Iteration 6385 / 14700) loss: 0.816455\n",
      "(Iteration 6386 / 14700) loss: 0.773120\n",
      "(Iteration 6387 / 14700) loss: 0.841656\n",
      "(Iteration 6388 / 14700) loss: 0.694226\n",
      "(Iteration 6389 / 14700) loss: 0.694271\n",
      "(Iteration 6390 / 14700) loss: 0.919175\n",
      "(Iteration 6391 / 14700) loss: 0.981150\n",
      "(Iteration 6392 / 14700) loss: 0.827333\n",
      "(Iteration 6393 / 14700) loss: 0.722970\n",
      "(Iteration 6394 / 14700) loss: 0.880976\n",
      "(Iteration 6395 / 14700) loss: 0.852628\n",
      "(Iteration 6396 / 14700) loss: 0.672090\n",
      "(Iteration 6397 / 14700) loss: 0.588264\n",
      "(Iteration 6398 / 14700) loss: 0.845810\n",
      "(Iteration 6399 / 14700) loss: 0.980568\n",
      "(Iteration 6400 / 14700) loss: 0.690983\n",
      "(Iteration 6401 / 14700) loss: 0.795918\n",
      "(Iteration 6402 / 14700) loss: 1.016435\n",
      "(Iteration 6403 / 14700) loss: 0.983814\n",
      "(Iteration 6404 / 14700) loss: 0.762118\n",
      "(Iteration 6405 / 14700) loss: 0.715298\n",
      "(Iteration 6406 / 14700) loss: 0.524387\n",
      "(Iteration 6407 / 14700) loss: 0.879536\n",
      "(Iteration 6408 / 14700) loss: 0.813757\n",
      "(Iteration 6409 / 14700) loss: 0.679813\n",
      "(Iteration 6410 / 14700) loss: 0.851995\n",
      "(Iteration 6411 / 14700) loss: 0.904518\n",
      "(Iteration 6412 / 14700) loss: 0.951639\n",
      "(Iteration 6413 / 14700) loss: 0.876719\n",
      "(Iteration 6414 / 14700) loss: 0.830784\n",
      "(Iteration 6415 / 14700) loss: 0.804388\n",
      "(Iteration 6416 / 14700) loss: 0.799746\n",
      "(Iteration 6417 / 14700) loss: 0.598072\n",
      "(Iteration 6418 / 14700) loss: 0.886431\n",
      "(Iteration 6419 / 14700) loss: 0.855322\n",
      "(Iteration 6420 / 14700) loss: 0.610246\n",
      "(Iteration 6421 / 14700) loss: 0.740315\n",
      "(Iteration 6422 / 14700) loss: 0.782609\n",
      "(Iteration 6423 / 14700) loss: 0.708267\n",
      "(Iteration 6424 / 14700) loss: 1.005280\n",
      "(Iteration 6425 / 14700) loss: 0.844301\n",
      "(Iteration 6426 / 14700) loss: 0.932738\n",
      "(Iteration 6427 / 14700) loss: 0.841695\n",
      "(Iteration 6428 / 14700) loss: 0.697309\n",
      "(Iteration 6429 / 14700) loss: 0.973525\n",
      "(Iteration 6430 / 14700) loss: 0.849605\n",
      "(Iteration 6431 / 14700) loss: 0.843657\n",
      "(Iteration 6432 / 14700) loss: 0.730983\n",
      "(Iteration 6433 / 14700) loss: 0.856856\n",
      "(Iteration 6434 / 14700) loss: 0.932819\n",
      "(Iteration 6435 / 14700) loss: 0.896537\n",
      "(Iteration 6436 / 14700) loss: 0.783029\n",
      "(Iteration 6437 / 14700) loss: 0.786007\n",
      "(Iteration 6438 / 14700) loss: 0.840388\n",
      "(Iteration 6439 / 14700) loss: 0.840490\n",
      "(Iteration 6440 / 14700) loss: 0.903773\n",
      "(Iteration 6441 / 14700) loss: 0.871117\n",
      "(Iteration 6442 / 14700) loss: 0.671507\n",
      "(Iteration 6443 / 14700) loss: 0.793894\n",
      "(Iteration 6444 / 14700) loss: 0.780241\n",
      "(Iteration 6445 / 14700) loss: 0.663845\n",
      "(Iteration 6446 / 14700) loss: 0.829396\n",
      "(Iteration 6447 / 14700) loss: 0.929769\n",
      "(Iteration 6448 / 14700) loss: 0.839248\n",
      "(Iteration 6449 / 14700) loss: 0.925059\n",
      "(Iteration 6450 / 14700) loss: 0.809379\n",
      "(Iteration 6451 / 14700) loss: 0.783222\n",
      "(Iteration 6452 / 14700) loss: 0.749418\n",
      "(Iteration 6453 / 14700) loss: 0.789928\n",
      "(Iteration 6454 / 14700) loss: 0.892980\n",
      "(Iteration 6455 / 14700) loss: 0.988261\n",
      "(Iteration 6456 / 14700) loss: 0.962274\n",
      "(Iteration 6457 / 14700) loss: 0.757165\n",
      "(Iteration 6458 / 14700) loss: 0.671362\n",
      "(Iteration 6459 / 14700) loss: 0.594005\n",
      "(Iteration 6460 / 14700) loss: 0.764837\n",
      "(Iteration 6461 / 14700) loss: 0.846635\n",
      "(Iteration 6462 / 14700) loss: 0.965603\n",
      "(Iteration 6463 / 14700) loss: 0.875352\n",
      "(Iteration 6464 / 14700) loss: 0.671362\n",
      "(Iteration 6465 / 14700) loss: 0.697026\n",
      "(Iteration 6466 / 14700) loss: 0.980327\n",
      "(Iteration 6467 / 14700) loss: 0.813883\n",
      "(Iteration 6468 / 14700) loss: 0.678634\n",
      "(Iteration 6469 / 14700) loss: 0.709424\n",
      "(Iteration 6470 / 14700) loss: 0.821351\n",
      "(Iteration 6471 / 14700) loss: 0.812214\n",
      "(Iteration 6472 / 14700) loss: 1.118722\n",
      "(Iteration 6473 / 14700) loss: 1.073780\n",
      "(Iteration 6474 / 14700) loss: 1.033293\n",
      "(Iteration 6475 / 14700) loss: 0.910036\n",
      "(Iteration 6476 / 14700) loss: 0.822367\n",
      "(Iteration 6477 / 14700) loss: 0.799204\n",
      "(Iteration 6478 / 14700) loss: 0.716857\n",
      "(Iteration 6479 / 14700) loss: 0.970065\n",
      "(Iteration 6480 / 14700) loss: 0.701856\n",
      "(Iteration 6481 / 14700) loss: 0.900191\n",
      "(Iteration 6482 / 14700) loss: 0.839230\n",
      "(Iteration 6483 / 14700) loss: 0.896227\n",
      "(Iteration 6484 / 14700) loss: 0.722147\n",
      "(Iteration 6485 / 14700) loss: 0.904964\n",
      "(Iteration 6486 / 14700) loss: 0.634647\n",
      "(Iteration 6487 / 14700) loss: 0.782529\n",
      "(Iteration 6488 / 14700) loss: 0.917705\n",
      "(Iteration 6489 / 14700) loss: 0.781394\n",
      "(Iteration 6490 / 14700) loss: 0.687298\n",
      "(Iteration 6491 / 14700) loss: 0.689257\n",
      "(Iteration 6492 / 14700) loss: 0.864472\n",
      "(Iteration 6493 / 14700) loss: 1.010037\n",
      "(Iteration 6494 / 14700) loss: 0.866658\n",
      "(Iteration 6495 / 14700) loss: 0.637901\n",
      "(Iteration 6496 / 14700) loss: 0.743274\n",
      "(Iteration 6497 / 14700) loss: 0.866414\n",
      "(Iteration 6498 / 14700) loss: 0.908983\n",
      "(Iteration 6499 / 14700) loss: 0.636783\n",
      "(Iteration 6500 / 14700) loss: 0.866236\n",
      "(Iteration 6501 / 14700) loss: 0.651221\n",
      "(Iteration 6502 / 14700) loss: 0.575264\n",
      "(Iteration 6503 / 14700) loss: 0.922279\n",
      "(Iteration 6504 / 14700) loss: 0.623934\n",
      "(Iteration 6505 / 14700) loss: 0.792507\n",
      "(Iteration 6506 / 14700) loss: 0.772959\n",
      "(Iteration 6507 / 14700) loss: 0.930293\n",
      "(Iteration 6508 / 14700) loss: 0.806074\n",
      "(Iteration 6509 / 14700) loss: 0.737981\n",
      "(Iteration 6510 / 14700) loss: 0.897799\n",
      "(Iteration 6511 / 14700) loss: 0.875546\n",
      "(Iteration 6512 / 14700) loss: 0.731238\n",
      "(Iteration 6513 / 14700) loss: 0.742644\n",
      "(Iteration 6514 / 14700) loss: 0.518528\n",
      "(Iteration 6515 / 14700) loss: 0.577720\n",
      "(Iteration 6516 / 14700) loss: 0.818601\n",
      "(Iteration 6517 / 14700) loss: 0.884318\n",
      "(Iteration 6518 / 14700) loss: 0.754897\n",
      "(Iteration 6519 / 14700) loss: 0.730081\n",
      "(Iteration 6520 / 14700) loss: 0.751125\n",
      "(Iteration 6521 / 14700) loss: 0.779435\n",
      "(Iteration 6522 / 14700) loss: 0.763827\n",
      "(Iteration 6523 / 14700) loss: 0.854285\n",
      "(Iteration 6524 / 14700) loss: 0.969454\n",
      "(Iteration 6525 / 14700) loss: 0.878784\n",
      "(Iteration 6526 / 14700) loss: 0.732737\n",
      "(Iteration 6527 / 14700) loss: 0.967269\n",
      "(Iteration 6528 / 14700) loss: 0.799942\n",
      "(Iteration 6529 / 14700) loss: 1.283098\n",
      "(Iteration 6530 / 14700) loss: 1.047019\n",
      "(Iteration 6531 / 14700) loss: 0.820676\n",
      "(Iteration 6532 / 14700) loss: 0.824484\n",
      "(Iteration 6533 / 14700) loss: 0.646729\n",
      "(Iteration 6534 / 14700) loss: 0.784697\n",
      "(Iteration 6535 / 14700) loss: 0.823404\n",
      "(Iteration 6536 / 14700) loss: 1.030992\n",
      "(Iteration 6537 / 14700) loss: 0.811825\n",
      "(Iteration 6538 / 14700) loss: 1.005902\n",
      "(Iteration 6539 / 14700) loss: 1.002267\n",
      "(Iteration 6540 / 14700) loss: 0.923438\n",
      "(Iteration 6541 / 14700) loss: 0.679719\n",
      "(Iteration 6542 / 14700) loss: 0.779242\n",
      "(Iteration 6543 / 14700) loss: 0.922443\n",
      "(Iteration 6544 / 14700) loss: 0.933071\n",
      "(Iteration 6545 / 14700) loss: 0.845711\n",
      "(Iteration 6546 / 14700) loss: 0.720784\n",
      "(Iteration 6547 / 14700) loss: 0.554025\n",
      "(Iteration 6548 / 14700) loss: 0.931536\n",
      "(Iteration 6549 / 14700) loss: 0.845913\n",
      "(Iteration 6550 / 14700) loss: 0.727802\n",
      "(Iteration 6551 / 14700) loss: 0.976984\n",
      "(Iteration 6552 / 14700) loss: 0.814788\n",
      "(Iteration 6553 / 14700) loss: 0.607032\n",
      "(Iteration 6554 / 14700) loss: 0.841695\n",
      "(Iteration 6555 / 14700) loss: 0.770513\n",
      "(Iteration 6556 / 14700) loss: 0.632659\n",
      "(Iteration 6557 / 14700) loss: 0.867445\n",
      "(Iteration 6558 / 14700) loss: 0.783849\n",
      "(Iteration 6559 / 14700) loss: 0.913910\n",
      "(Iteration 6560 / 14700) loss: 0.638960\n",
      "(Iteration 6561 / 14700) loss: 0.656777\n",
      "(Iteration 6562 / 14700) loss: 0.786597\n",
      "(Iteration 6563 / 14700) loss: 0.830098\n",
      "(Iteration 6564 / 14700) loss: 0.764972\n",
      "(Iteration 6565 / 14700) loss: 0.754482\n",
      "(Iteration 6566 / 14700) loss: 0.698345\n",
      "(Iteration 6567 / 14700) loss: 0.839110\n",
      "(Iteration 6568 / 14700) loss: 0.756560\n",
      "(Iteration 6569 / 14700) loss: 0.685065\n",
      "(Iteration 6570 / 14700) loss: 0.607800\n",
      "(Iteration 6571 / 14700) loss: 0.727952\n",
      "(Iteration 6572 / 14700) loss: 0.743853\n",
      "(Iteration 6573 / 14700) loss: 0.763227\n",
      "(Iteration 6574 / 14700) loss: 0.628508\n",
      "(Iteration 6575 / 14700) loss: 1.014075\n",
      "(Iteration 6576 / 14700) loss: 1.237597\n",
      "(Iteration 6577 / 14700) loss: 0.920731\n",
      "(Iteration 6578 / 14700) loss: 0.658100\n",
      "(Iteration 6579 / 14700) loss: 0.730544\n",
      "(Iteration 6580 / 14700) loss: 0.787795\n",
      "(Iteration 6581 / 14700) loss: 0.932966\n",
      "(Iteration 6582 / 14700) loss: 0.665654\n",
      "(Iteration 6583 / 14700) loss: 0.696521\n",
      "(Iteration 6584 / 14700) loss: 1.026252\n",
      "(Iteration 6585 / 14700) loss: 0.769230\n",
      "(Iteration 6586 / 14700) loss: 0.737553\n",
      "(Iteration 6587 / 14700) loss: 0.703595\n",
      "(Iteration 6588 / 14700) loss: 0.688681\n",
      "(Iteration 6589 / 14700) loss: 0.671057\n",
      "(Iteration 6590 / 14700) loss: 0.706313\n",
      "(Iteration 6591 / 14700) loss: 0.742284\n",
      "(Iteration 6592 / 14700) loss: 0.638974\n",
      "(Iteration 6593 / 14700) loss: 1.039357\n",
      "(Iteration 6594 / 14700) loss: 0.809316\n",
      "(Iteration 6595 / 14700) loss: 0.769099\n",
      "(Iteration 6596 / 14700) loss: 1.011571\n",
      "(Iteration 6597 / 14700) loss: 0.720278\n",
      "(Iteration 6598 / 14700) loss: 0.852649\n",
      "(Iteration 6599 / 14700) loss: 0.920743\n",
      "(Iteration 6600 / 14700) loss: 0.645813\n",
      "(Iteration 6601 / 14700) loss: 0.782330\n",
      "(Iteration 6602 / 14700) loss: 0.654408\n",
      "(Iteration 6603 / 14700) loss: 0.705666\n",
      "(Iteration 6604 / 14700) loss: 0.764599\n",
      "(Iteration 6605 / 14700) loss: 1.044566\n",
      "(Iteration 6606 / 14700) loss: 0.862402\n",
      "(Iteration 6607 / 14700) loss: 0.887855\n",
      "(Iteration 6608 / 14700) loss: 1.045361\n",
      "(Iteration 6609 / 14700) loss: 0.976818\n",
      "(Iteration 6610 / 14700) loss: 0.775522\n",
      "(Iteration 6611 / 14700) loss: 0.928364\n",
      "(Iteration 6612 / 14700) loss: 0.965932\n",
      "(Iteration 6613 / 14700) loss: 0.792145\n",
      "(Iteration 6614 / 14700) loss: 0.702207\n",
      "(Iteration 6615 / 14700) loss: 0.878074\n",
      "(Iteration 6616 / 14700) loss: 0.915800\n",
      "(Iteration 6617 / 14700) loss: 0.958054\n",
      "(Iteration 6618 / 14700) loss: 0.727380\n",
      "(Iteration 6619 / 14700) loss: 1.056569\n",
      "(Iteration 6620 / 14700) loss: 0.773959\n",
      "(Iteration 6621 / 14700) loss: 0.908701\n",
      "(Iteration 6622 / 14700) loss: 0.949899\n",
      "(Iteration 6623 / 14700) loss: 0.785917\n",
      "(Iteration 6624 / 14700) loss: 0.918721\n",
      "(Iteration 6625 / 14700) loss: 0.851380\n",
      "(Iteration 6626 / 14700) loss: 0.933294\n",
      "(Iteration 6627 / 14700) loss: 0.778097\n",
      "(Iteration 6628 / 14700) loss: 0.874122\n",
      "(Iteration 6629 / 14700) loss: 0.762554\n",
      "(Iteration 6630 / 14700) loss: 0.775120\n",
      "(Iteration 6631 / 14700) loss: 0.886154\n",
      "(Iteration 6632 / 14700) loss: 0.794028\n",
      "(Iteration 6633 / 14700) loss: 0.894403\n",
      "(Iteration 6634 / 14700) loss: 0.903210\n",
      "(Iteration 6635 / 14700) loss: 0.976086\n",
      "(Iteration 6636 / 14700) loss: 0.546589\n",
      "(Iteration 6637 / 14700) loss: 0.778153\n",
      "(Iteration 6638 / 14700) loss: 0.730698\n",
      "(Iteration 6639 / 14700) loss: 0.617627\n",
      "(Iteration 6640 / 14700) loss: 0.866184\n",
      "(Iteration 6641 / 14700) loss: 1.161251\n",
      "(Iteration 6642 / 14700) loss: 0.778691\n",
      "(Iteration 6643 / 14700) loss: 0.724670\n",
      "(Iteration 6644 / 14700) loss: 0.950332\n",
      "(Iteration 6645 / 14700) loss: 0.935387\n",
      "(Iteration 6646 / 14700) loss: 0.739077\n",
      "(Iteration 6647 / 14700) loss: 0.749176\n",
      "(Iteration 6648 / 14700) loss: 1.084742\n",
      "(Iteration 6649 / 14700) loss: 0.616693\n",
      "(Iteration 6650 / 14700) loss: 0.814441\n",
      "(Iteration 6651 / 14700) loss: 0.920205\n",
      "(Iteration 6652 / 14700) loss: 0.753163\n",
      "(Iteration 6653 / 14700) loss: 0.943529\n",
      "(Iteration 6654 / 14700) loss: 0.773697\n",
      "(Iteration 6655 / 14700) loss: 0.986531\n",
      "(Iteration 6656 / 14700) loss: 0.756505\n",
      "(Iteration 6657 / 14700) loss: 0.712126\n",
      "(Iteration 6658 / 14700) loss: 0.915638\n",
      "(Iteration 6659 / 14700) loss: 0.808764\n",
      "(Iteration 6660 / 14700) loss: 0.814694\n",
      "(Iteration 6661 / 14700) loss: 0.835731\n",
      "(Iteration 6662 / 14700) loss: 0.763259\n",
      "(Iteration 6663 / 14700) loss: 0.825386\n",
      "(Iteration 6664 / 14700) loss: 0.647568\n",
      "(Iteration 6665 / 14700) loss: 0.844660\n",
      "(Iteration 6666 / 14700) loss: 0.911129\n",
      "(Iteration 6667 / 14700) loss: 0.638469\n",
      "(Iteration 6668 / 14700) loss: 0.921720\n",
      "(Iteration 6669 / 14700) loss: 0.878373\n",
      "(Iteration 6670 / 14700) loss: 0.652415\n",
      "(Iteration 6671 / 14700) loss: 0.701319\n",
      "(Iteration 6672 / 14700) loss: 0.819732\n",
      "(Iteration 6673 / 14700) loss: 1.085781\n",
      "(Iteration 6674 / 14700) loss: 0.589364\n",
      "(Iteration 6675 / 14700) loss: 0.828067\n",
      "(Iteration 6676 / 14700) loss: 0.682652\n",
      "(Iteration 6677 / 14700) loss: 0.958683\n",
      "(Iteration 6678 / 14700) loss: 0.804760\n",
      "(Iteration 6679 / 14700) loss: 0.668684\n",
      "(Iteration 6680 / 14700) loss: 0.794939\n",
      "(Iteration 6681 / 14700) loss: 0.627675\n",
      "(Iteration 6682 / 14700) loss: 0.712158\n",
      "(Iteration 6683 / 14700) loss: 0.873274\n",
      "(Iteration 6684 / 14700) loss: 0.686177\n",
      "(Iteration 6685 / 14700) loss: 0.902885\n",
      "(Iteration 6686 / 14700) loss: 0.788436\n",
      "(Iteration 6687 / 14700) loss: 1.209358\n",
      "(Iteration 6688 / 14700) loss: 0.773233\n",
      "(Iteration 6689 / 14700) loss: 0.702564\n",
      "(Iteration 6690 / 14700) loss: 0.830797\n",
      "(Iteration 6691 / 14700) loss: 0.908470\n",
      "(Iteration 6692 / 14700) loss: 0.665363\n",
      "(Iteration 6693 / 14700) loss: 0.772521\n",
      "(Iteration 6694 / 14700) loss: 0.755446\n",
      "(Iteration 6695 / 14700) loss: 0.603061\n",
      "(Iteration 6696 / 14700) loss: 0.800281\n",
      "(Iteration 6697 / 14700) loss: 0.788536\n",
      "(Iteration 6698 / 14700) loss: 0.712910\n",
      "(Iteration 6699 / 14700) loss: 0.906122\n",
      "(Iteration 6700 / 14700) loss: 0.749975\n",
      "(Iteration 6701 / 14700) loss: 0.701036\n",
      "(Iteration 6702 / 14700) loss: 0.934972\n",
      "(Iteration 6703 / 14700) loss: 1.149527\n",
      "(Iteration 6704 / 14700) loss: 0.859079\n",
      "(Iteration 6705 / 14700) loss: 0.755734\n",
      "(Iteration 6706 / 14700) loss: 1.050033\n",
      "(Iteration 6707 / 14700) loss: 1.053484\n",
      "(Iteration 6708 / 14700) loss: 0.797593\n",
      "(Iteration 6709 / 14700) loss: 0.706595\n",
      "(Iteration 6710 / 14700) loss: 0.721422\n",
      "(Iteration 6711 / 14700) loss: 0.972622\n",
      "(Iteration 6712 / 14700) loss: 0.774341\n",
      "(Iteration 6713 / 14700) loss: 0.857692\n",
      "(Iteration 6714 / 14700) loss: 0.903994\n",
      "(Iteration 6715 / 14700) loss: 0.717949\n",
      "(Iteration 6716 / 14700) loss: 0.807078\n",
      "(Iteration 6717 / 14700) loss: 0.692327\n",
      "(Iteration 6718 / 14700) loss: 0.999614\n",
      "(Iteration 6719 / 14700) loss: 0.890927\n",
      "(Iteration 6720 / 14700) loss: 0.796729\n",
      "(Iteration 6721 / 14700) loss: 0.713705\n",
      "(Iteration 6722 / 14700) loss: 0.878009\n",
      "(Iteration 6723 / 14700) loss: 0.838185\n",
      "(Iteration 6724 / 14700) loss: 0.771095\n",
      "(Iteration 6725 / 14700) loss: 0.656328\n",
      "(Iteration 6726 / 14700) loss: 0.823533\n",
      "(Iteration 6727 / 14700) loss: 0.738773\n",
      "(Iteration 6728 / 14700) loss: 0.960583\n",
      "(Iteration 6729 / 14700) loss: 0.891058\n",
      "(Iteration 6730 / 14700) loss: 0.620652\n",
      "(Iteration 6731 / 14700) loss: 0.572210\n",
      "(Iteration 6732 / 14700) loss: 0.690200\n",
      "(Iteration 6733 / 14700) loss: 0.875004\n",
      "(Iteration 6734 / 14700) loss: 0.785526\n",
      "(Iteration 6735 / 14700) loss: 0.994327\n",
      "(Iteration 6736 / 14700) loss: 0.639905\n",
      "(Iteration 6737 / 14700) loss: 0.628066\n",
      "(Iteration 6738 / 14700) loss: 0.745502\n",
      "(Iteration 6739 / 14700) loss: 0.823195\n",
      "(Iteration 6740 / 14700) loss: 0.973796\n",
      "(Iteration 6741 / 14700) loss: 0.785493\n",
      "(Iteration 6742 / 14700) loss: 0.841431\n",
      "(Iteration 6743 / 14700) loss: 0.659947\n",
      "(Iteration 6744 / 14700) loss: 0.790626\n",
      "(Iteration 6745 / 14700) loss: 0.703582\n",
      "(Iteration 6746 / 14700) loss: 0.521783\n",
      "(Iteration 6747 / 14700) loss: 0.665993\n",
      "(Iteration 6748 / 14700) loss: 0.869564\n",
      "(Iteration 6749 / 14700) loss: 0.602836\n",
      "(Iteration 6750 / 14700) loss: 0.834372\n",
      "(Iteration 6751 / 14700) loss: 0.715912\n",
      "(Iteration 6752 / 14700) loss: 0.544201\n",
      "(Iteration 6753 / 14700) loss: 0.800394\n",
      "(Iteration 6754 / 14700) loss: 0.615556\n",
      "(Iteration 6755 / 14700) loss: 0.787005\n",
      "(Iteration 6756 / 14700) loss: 0.905069\n",
      "(Iteration 6757 / 14700) loss: 0.720173\n",
      "(Iteration 6758 / 14700) loss: 0.680314\n",
      "(Iteration 6759 / 14700) loss: 0.665393\n",
      "(Iteration 6760 / 14700) loss: 0.779488\n",
      "(Iteration 6761 / 14700) loss: 0.745288\n",
      "(Iteration 6762 / 14700) loss: 0.588917\n",
      "(Iteration 6763 / 14700) loss: 0.914305\n",
      "(Iteration 6764 / 14700) loss: 1.007851\n",
      "(Iteration 6765 / 14700) loss: 0.917327\n",
      "(Iteration 6766 / 14700) loss: 1.041124\n",
      "(Iteration 6767 / 14700) loss: 0.725422\n",
      "(Iteration 6768 / 14700) loss: 0.973631\n",
      "(Iteration 6769 / 14700) loss: 0.666220\n",
      "(Iteration 6770 / 14700) loss: 0.677311\n",
      "(Iteration 6771 / 14700) loss: 0.560896\n",
      "(Iteration 6772 / 14700) loss: 1.087656\n",
      "(Iteration 6773 / 14700) loss: 0.676557\n",
      "(Iteration 6774 / 14700) loss: 0.707576\n",
      "(Iteration 6775 / 14700) loss: 0.780645\n",
      "(Iteration 6776 / 14700) loss: 0.835838\n",
      "(Iteration 6777 / 14700) loss: 0.731466\n",
      "(Iteration 6778 / 14700) loss: 0.798617\n",
      "(Iteration 6779 / 14700) loss: 1.023984\n",
      "(Iteration 6780 / 14700) loss: 0.701007\n",
      "(Iteration 6781 / 14700) loss: 0.641134\n",
      "(Iteration 6782 / 14700) loss: 0.781929\n",
      "(Iteration 6783 / 14700) loss: 1.003811\n",
      "(Iteration 6784 / 14700) loss: 0.637530\n",
      "(Iteration 6785 / 14700) loss: 0.822747\n",
      "(Iteration 6786 / 14700) loss: 0.804628\n",
      "(Iteration 6787 / 14700) loss: 0.751560\n",
      "(Iteration 6788 / 14700) loss: 0.803503\n",
      "(Iteration 6789 / 14700) loss: 0.623249\n",
      "(Iteration 6790 / 14700) loss: 0.700236\n",
      "(Iteration 6791 / 14700) loss: 0.671282\n",
      "(Iteration 6792 / 14700) loss: 0.764310\n",
      "(Iteration 6793 / 14700) loss: 0.968463\n",
      "(Iteration 6794 / 14700) loss: 0.846032\n",
      "(Iteration 6795 / 14700) loss: 0.690687\n",
      "(Iteration 6796 / 14700) loss: 0.561526\n",
      "(Iteration 6797 / 14700) loss: 1.301525\n",
      "(Iteration 6798 / 14700) loss: 0.553526\n",
      "(Iteration 6799 / 14700) loss: 0.910560\n",
      "(Iteration 6800 / 14700) loss: 0.526571\n",
      "(Iteration 6801 / 14700) loss: 0.809576\n",
      "(Iteration 6802 / 14700) loss: 0.789172\n",
      "(Iteration 6803 / 14700) loss: 0.886414\n",
      "(Iteration 6804 / 14700) loss: 1.131472\n",
      "(Iteration 6805 / 14700) loss: 0.852923\n",
      "(Iteration 6806 / 14700) loss: 0.676409\n",
      "(Iteration 6807 / 14700) loss: 0.662389\n",
      "(Iteration 6808 / 14700) loss: 0.988883\n",
      "(Iteration 6809 / 14700) loss: 0.883978\n",
      "(Iteration 6810 / 14700) loss: 0.709996\n",
      "(Iteration 6811 / 14700) loss: 0.688881\n",
      "(Iteration 6812 / 14700) loss: 0.737041\n",
      "(Iteration 6813 / 14700) loss: 0.755051\n",
      "(Iteration 6814 / 14700) loss: 0.769631\n",
      "(Iteration 6815 / 14700) loss: 0.613458\n",
      "(Iteration 6816 / 14700) loss: 0.874810\n",
      "(Iteration 6817 / 14700) loss: 0.878188\n",
      "(Iteration 6818 / 14700) loss: 0.728470\n",
      "(Iteration 6819 / 14700) loss: 0.670308\n",
      "(Iteration 6820 / 14700) loss: 0.773986\n",
      "(Iteration 6821 / 14700) loss: 0.673279\n",
      "(Iteration 6822 / 14700) loss: 1.008575\n",
      "(Iteration 6823 / 14700) loss: 0.512905\n",
      "(Iteration 6824 / 14700) loss: 0.795527\n",
      "(Iteration 6825 / 14700) loss: 0.728263\n",
      "(Iteration 6826 / 14700) loss: 0.696314\n",
      "(Iteration 6827 / 14700) loss: 0.860552\n",
      "(Iteration 6828 / 14700) loss: 0.854207\n",
      "(Iteration 6829 / 14700) loss: 0.980707\n",
      "(Iteration 6830 / 14700) loss: 0.725370\n",
      "(Iteration 6831 / 14700) loss: 0.881210\n",
      "(Iteration 6832 / 14700) loss: 0.846679\n",
      "(Iteration 6833 / 14700) loss: 0.771830\n",
      "(Iteration 6834 / 14700) loss: 0.724291\n",
      "(Iteration 6835 / 14700) loss: 0.672820\n",
      "(Iteration 6836 / 14700) loss: 0.700335\n",
      "(Iteration 6837 / 14700) loss: 0.873289\n",
      "(Iteration 6838 / 14700) loss: 0.778374\n",
      "(Iteration 6839 / 14700) loss: 1.135675\n",
      "(Iteration 6840 / 14700) loss: 0.938280\n",
      "(Iteration 6841 / 14700) loss: 0.767641\n",
      "(Iteration 6842 / 14700) loss: 0.924888\n",
      "(Iteration 6843 / 14700) loss: 0.649764\n",
      "(Iteration 6844 / 14700) loss: 1.037655\n",
      "(Iteration 6845 / 14700) loss: 0.706331\n",
      "(Iteration 6846 / 14700) loss: 0.775778\n",
      "(Iteration 6847 / 14700) loss: 0.979863\n",
      "(Iteration 6848 / 14700) loss: 0.759370\n",
      "(Iteration 6849 / 14700) loss: 0.979253\n",
      "(Iteration 6850 / 14700) loss: 1.016985\n",
      "(Iteration 6851 / 14700) loss: 0.807093\n",
      "(Iteration 6852 / 14700) loss: 0.838714\n",
      "(Iteration 6853 / 14700) loss: 0.913513\n",
      "(Iteration 6854 / 14700) loss: 0.842681\n",
      "(Iteration 6855 / 14700) loss: 0.801401\n",
      "(Iteration 6856 / 14700) loss: 1.031065\n",
      "(Iteration 6857 / 14700) loss: 0.848101\n",
      "(Iteration 6858 / 14700) loss: 0.746719\n",
      "(Iteration 6859 / 14700) loss: 0.921046\n",
      "(Iteration 6860 / 14700) loss: 0.704324\n",
      "(Epoch 7 / 15) train acc: 0.808000; val_acc: 0.735000\n",
      "(Iteration 6861 / 14700) loss: 0.910537\n",
      "(Iteration 6862 / 14700) loss: 0.866805\n",
      "(Iteration 6863 / 14700) loss: 0.699062\n",
      "(Iteration 6864 / 14700) loss: 0.823609\n",
      "(Iteration 6865 / 14700) loss: 1.027723\n",
      "(Iteration 6866 / 14700) loss: 0.838744\n",
      "(Iteration 6867 / 14700) loss: 0.793353\n",
      "(Iteration 6868 / 14700) loss: 0.781808\n",
      "(Iteration 6869 / 14700) loss: 0.796508\n",
      "(Iteration 6870 / 14700) loss: 0.834809\n",
      "(Iteration 6871 / 14700) loss: 0.695038\n",
      "(Iteration 6872 / 14700) loss: 0.972168\n",
      "(Iteration 6873 / 14700) loss: 0.779842\n",
      "(Iteration 6874 / 14700) loss: 1.047157\n",
      "(Iteration 6875 / 14700) loss: 0.987314\n",
      "(Iteration 6876 / 14700) loss: 0.790225\n",
      "(Iteration 6877 / 14700) loss: 0.761818\n",
      "(Iteration 6878 / 14700) loss: 0.854189\n",
      "(Iteration 6879 / 14700) loss: 1.078140\n",
      "(Iteration 6880 / 14700) loss: 0.832018\n",
      "(Iteration 6881 / 14700) loss: 0.676998\n",
      "(Iteration 6882 / 14700) loss: 0.778080\n",
      "(Iteration 6883 / 14700) loss: 0.818548\n",
      "(Iteration 6884 / 14700) loss: 1.032228\n",
      "(Iteration 6885 / 14700) loss: 0.570974\n",
      "(Iteration 6886 / 14700) loss: 0.890063\n",
      "(Iteration 6887 / 14700) loss: 1.026323\n",
      "(Iteration 6888 / 14700) loss: 0.774624\n",
      "(Iteration 6889 / 14700) loss: 0.985921\n",
      "(Iteration 6890 / 14700) loss: 0.831099\n",
      "(Iteration 6891 / 14700) loss: 0.823099\n",
      "(Iteration 6892 / 14700) loss: 0.619328\n",
      "(Iteration 6893 / 14700) loss: 1.071894\n",
      "(Iteration 6894 / 14700) loss: 0.794607\n",
      "(Iteration 6895 / 14700) loss: 0.773598\n",
      "(Iteration 6896 / 14700) loss: 0.780795\n",
      "(Iteration 6897 / 14700) loss: 0.522588\n",
      "(Iteration 6898 / 14700) loss: 0.744384\n",
      "(Iteration 6899 / 14700) loss: 0.791676\n",
      "(Iteration 6900 / 14700) loss: 0.918397\n",
      "(Iteration 6901 / 14700) loss: 0.903763\n",
      "(Iteration 6902 / 14700) loss: 0.954819\n",
      "(Iteration 6903 / 14700) loss: 0.620486\n",
      "(Iteration 6904 / 14700) loss: 0.742669\n",
      "(Iteration 6905 / 14700) loss: 0.911197\n",
      "(Iteration 6906 / 14700) loss: 0.675722\n",
      "(Iteration 6907 / 14700) loss: 0.679348\n",
      "(Iteration 6908 / 14700) loss: 0.750438\n",
      "(Iteration 6909 / 14700) loss: 1.134341\n",
      "(Iteration 6910 / 14700) loss: 0.927300\n",
      "(Iteration 6911 / 14700) loss: 0.639058\n",
      "(Iteration 6912 / 14700) loss: 0.752253\n",
      "(Iteration 6913 / 14700) loss: 0.580205\n",
      "(Iteration 6914 / 14700) loss: 0.827549\n",
      "(Iteration 6915 / 14700) loss: 0.959233\n",
      "(Iteration 6916 / 14700) loss: 1.063602\n",
      "(Iteration 6917 / 14700) loss: 0.781525\n",
      "(Iteration 6918 / 14700) loss: 0.721105\n",
      "(Iteration 6919 / 14700) loss: 0.801608\n",
      "(Iteration 6920 / 14700) loss: 0.686319\n",
      "(Iteration 6921 / 14700) loss: 0.594332\n",
      "(Iteration 6922 / 14700) loss: 0.768954\n",
      "(Iteration 6923 / 14700) loss: 0.850499\n",
      "(Iteration 6924 / 14700) loss: 0.587193\n",
      "(Iteration 6925 / 14700) loss: 0.831554\n",
      "(Iteration 6926 / 14700) loss: 0.922261\n",
      "(Iteration 6927 / 14700) loss: 0.626953\n",
      "(Iteration 6928 / 14700) loss: 0.991231\n",
      "(Iteration 6929 / 14700) loss: 0.879418\n",
      "(Iteration 6930 / 14700) loss: 0.715815\n",
      "(Iteration 6931 / 14700) loss: 0.705814\n",
      "(Iteration 6932 / 14700) loss: 0.886898\n",
      "(Iteration 6933 / 14700) loss: 0.687511\n",
      "(Iteration 6934 / 14700) loss: 0.874515\n",
      "(Iteration 6935 / 14700) loss: 0.681090\n",
      "(Iteration 6936 / 14700) loss: 0.723933\n",
      "(Iteration 6937 / 14700) loss: 0.726997\n",
      "(Iteration 6938 / 14700) loss: 1.148283\n",
      "(Iteration 6939 / 14700) loss: 0.629556\n",
      "(Iteration 6940 / 14700) loss: 0.768868\n",
      "(Iteration 6941 / 14700) loss: 0.943118\n",
      "(Iteration 6942 / 14700) loss: 0.825436\n",
      "(Iteration 6943 / 14700) loss: 0.648311\n",
      "(Iteration 6944 / 14700) loss: 0.655680\n",
      "(Iteration 6945 / 14700) loss: 0.859469\n",
      "(Iteration 6946 / 14700) loss: 0.663003\n",
      "(Iteration 6947 / 14700) loss: 0.985895\n",
      "(Iteration 6948 / 14700) loss: 0.894663\n",
      "(Iteration 6949 / 14700) loss: 0.856729\n",
      "(Iteration 6950 / 14700) loss: 0.746871\n",
      "(Iteration 6951 / 14700) loss: 0.589447\n",
      "(Iteration 6952 / 14700) loss: 0.752032\n",
      "(Iteration 6953 / 14700) loss: 0.648700\n",
      "(Iteration 6954 / 14700) loss: 0.632209\n",
      "(Iteration 6955 / 14700) loss: 0.638276\n",
      "(Iteration 6956 / 14700) loss: 0.708172\n",
      "(Iteration 6957 / 14700) loss: 0.667545\n",
      "(Iteration 6958 / 14700) loss: 0.705788\n",
      "(Iteration 6959 / 14700) loss: 0.764433\n",
      "(Iteration 6960 / 14700) loss: 0.904409\n",
      "(Iteration 6961 / 14700) loss: 0.722956\n",
      "(Iteration 6962 / 14700) loss: 1.181186\n",
      "(Iteration 6963 / 14700) loss: 0.694206\n",
      "(Iteration 6964 / 14700) loss: 0.511565\n",
      "(Iteration 6965 / 14700) loss: 0.732595\n",
      "(Iteration 6966 / 14700) loss: 0.828974\n",
      "(Iteration 6967 / 14700) loss: 0.586202\n",
      "(Iteration 6968 / 14700) loss: 0.672946\n",
      "(Iteration 6969 / 14700) loss: 0.691067\n",
      "(Iteration 6970 / 14700) loss: 0.693943\n",
      "(Iteration 6971 / 14700) loss: 1.109567\n",
      "(Iteration 6972 / 14700) loss: 0.779535\n",
      "(Iteration 6973 / 14700) loss: 0.934546\n",
      "(Iteration 6974 / 14700) loss: 0.722091\n",
      "(Iteration 6975 / 14700) loss: 0.638066\n",
      "(Iteration 6976 / 14700) loss: 0.772204\n",
      "(Iteration 6977 / 14700) loss: 1.012777\n",
      "(Iteration 6978 / 14700) loss: 0.764173\n",
      "(Iteration 6979 / 14700) loss: 0.585433\n",
      "(Iteration 6980 / 14700) loss: 0.806431\n",
      "(Iteration 6981 / 14700) loss: 1.070440\n",
      "(Iteration 6982 / 14700) loss: 0.819525\n",
      "(Iteration 6983 / 14700) loss: 0.864506\n",
      "(Iteration 6984 / 14700) loss: 0.874996\n",
      "(Iteration 6985 / 14700) loss: 0.876977\n",
      "(Iteration 6986 / 14700) loss: 0.776976\n",
      "(Iteration 6987 / 14700) loss: 0.700741\n",
      "(Iteration 6988 / 14700) loss: 0.863704\n",
      "(Iteration 6989 / 14700) loss: 0.788336\n",
      "(Iteration 6990 / 14700) loss: 0.785527\n",
      "(Iteration 6991 / 14700) loss: 0.811031\n",
      "(Iteration 6992 / 14700) loss: 0.847263\n",
      "(Iteration 6993 / 14700) loss: 0.857965\n",
      "(Iteration 6994 / 14700) loss: 0.762044\n",
      "(Iteration 6995 / 14700) loss: 0.811300\n",
      "(Iteration 6996 / 14700) loss: 0.891289\n",
      "(Iteration 6997 / 14700) loss: 0.818473\n",
      "(Iteration 6998 / 14700) loss: 0.906066\n",
      "(Iteration 6999 / 14700) loss: 0.864868\n",
      "(Iteration 7000 / 14700) loss: 0.703714\n",
      "(Iteration 7001 / 14700) loss: 0.849105\n",
      "(Iteration 7002 / 14700) loss: 0.787454\n",
      "(Iteration 7003 / 14700) loss: 0.875965\n",
      "(Iteration 7004 / 14700) loss: 0.907380\n",
      "(Iteration 7005 / 14700) loss: 0.897000\n",
      "(Iteration 7006 / 14700) loss: 0.640864\n",
      "(Iteration 7007 / 14700) loss: 0.969539\n",
      "(Iteration 7008 / 14700) loss: 0.719818\n",
      "(Iteration 7009 / 14700) loss: 0.782818\n",
      "(Iteration 7010 / 14700) loss: 0.829389\n",
      "(Iteration 7011 / 14700) loss: 0.681221\n",
      "(Iteration 7012 / 14700) loss: 0.672660\n",
      "(Iteration 7013 / 14700) loss: 0.599517\n",
      "(Iteration 7014 / 14700) loss: 0.771644\n",
      "(Iteration 7015 / 14700) loss: 0.925022\n",
      "(Iteration 7016 / 14700) loss: 0.680427\n",
      "(Iteration 7017 / 14700) loss: 0.787860\n",
      "(Iteration 7018 / 14700) loss: 0.880398\n",
      "(Iteration 7019 / 14700) loss: 0.658997\n",
      "(Iteration 7020 / 14700) loss: 1.006835\n",
      "(Iteration 7021 / 14700) loss: 0.785677\n",
      "(Iteration 7022 / 14700) loss: 0.850553\n",
      "(Iteration 7023 / 14700) loss: 0.779295\n",
      "(Iteration 7024 / 14700) loss: 0.858075\n",
      "(Iteration 7025 / 14700) loss: 0.825775\n",
      "(Iteration 7026 / 14700) loss: 0.811177\n",
      "(Iteration 7027 / 14700) loss: 0.716056\n",
      "(Iteration 7028 / 14700) loss: 0.913036\n",
      "(Iteration 7029 / 14700) loss: 0.528854\n",
      "(Iteration 7030 / 14700) loss: 0.766469\n",
      "(Iteration 7031 / 14700) loss: 0.892974\n",
      "(Iteration 7032 / 14700) loss: 0.683793\n",
      "(Iteration 7033 / 14700) loss: 0.813706\n",
      "(Iteration 7034 / 14700) loss: 0.817044\n",
      "(Iteration 7035 / 14700) loss: 0.859005\n",
      "(Iteration 7036 / 14700) loss: 0.584718\n",
      "(Iteration 7037 / 14700) loss: 0.709232\n",
      "(Iteration 7038 / 14700) loss: 0.719810\n",
      "(Iteration 7039 / 14700) loss: 0.628878\n",
      "(Iteration 7040 / 14700) loss: 0.907099\n",
      "(Iteration 7041 / 14700) loss: 0.764205\n",
      "(Iteration 7042 / 14700) loss: 0.633870\n",
      "(Iteration 7043 / 14700) loss: 0.817025\n",
      "(Iteration 7044 / 14700) loss: 0.698748\n",
      "(Iteration 7045 / 14700) loss: 0.499977\n",
      "(Iteration 7046 / 14700) loss: 1.080000\n",
      "(Iteration 7047 / 14700) loss: 0.632773\n",
      "(Iteration 7048 / 14700) loss: 0.765498\n",
      "(Iteration 7049 / 14700) loss: 0.850145\n",
      "(Iteration 7050 / 14700) loss: 0.658376\n",
      "(Iteration 7051 / 14700) loss: 0.778394\n",
      "(Iteration 7052 / 14700) loss: 0.905734\n",
      "(Iteration 7053 / 14700) loss: 0.881675\n",
      "(Iteration 7054 / 14700) loss: 0.713933\n",
      "(Iteration 7055 / 14700) loss: 0.900015\n",
      "(Iteration 7056 / 14700) loss: 0.919772\n",
      "(Iteration 7057 / 14700) loss: 0.749948\n",
      "(Iteration 7058 / 14700) loss: 0.811346\n",
      "(Iteration 7059 / 14700) loss: 0.827234\n",
      "(Iteration 7060 / 14700) loss: 0.836767\n",
      "(Iteration 7061 / 14700) loss: 0.703468\n",
      "(Iteration 7062 / 14700) loss: 0.694989\n",
      "(Iteration 7063 / 14700) loss: 0.641898\n",
      "(Iteration 7064 / 14700) loss: 0.557458\n",
      "(Iteration 7065 / 14700) loss: 0.648922\n",
      "(Iteration 7066 / 14700) loss: 0.597889\n",
      "(Iteration 7067 / 14700) loss: 1.184609\n",
      "(Iteration 7068 / 14700) loss: 0.864363\n",
      "(Iteration 7069 / 14700) loss: 0.693064\n",
      "(Iteration 7070 / 14700) loss: 0.843379\n",
      "(Iteration 7071 / 14700) loss: 0.566399\n",
      "(Iteration 7072 / 14700) loss: 0.829544\n",
      "(Iteration 7073 / 14700) loss: 0.634486\n",
      "(Iteration 7074 / 14700) loss: 0.522974\n",
      "(Iteration 7075 / 14700) loss: 0.863238\n",
      "(Iteration 7076 / 14700) loss: 0.780020\n",
      "(Iteration 7077 / 14700) loss: 0.884716\n",
      "(Iteration 7078 / 14700) loss: 0.909015\n",
      "(Iteration 7079 / 14700) loss: 0.559808\n",
      "(Iteration 7080 / 14700) loss: 0.761829\n",
      "(Iteration 7081 / 14700) loss: 0.663426\n",
      "(Iteration 7082 / 14700) loss: 0.740203\n",
      "(Iteration 7083 / 14700) loss: 0.638400\n",
      "(Iteration 7084 / 14700) loss: 0.627868\n",
      "(Iteration 7085 / 14700) loss: 0.655724\n",
      "(Iteration 7086 / 14700) loss: 0.832743\n",
      "(Iteration 7087 / 14700) loss: 0.705604\n",
      "(Iteration 7088 / 14700) loss: 0.693732\n",
      "(Iteration 7089 / 14700) loss: 0.819097\n",
      "(Iteration 7090 / 14700) loss: 0.684258\n",
      "(Iteration 7091 / 14700) loss: 0.818398\n",
      "(Iteration 7092 / 14700) loss: 0.561525\n",
      "(Iteration 7093 / 14700) loss: 1.011069\n",
      "(Iteration 7094 / 14700) loss: 0.817856\n",
      "(Iteration 7095 / 14700) loss: 0.595939\n",
      "(Iteration 7096 / 14700) loss: 0.844086\n",
      "(Iteration 7097 / 14700) loss: 0.819159\n",
      "(Iteration 7098 / 14700) loss: 0.686302\n",
      "(Iteration 7099 / 14700) loss: 0.646862\n",
      "(Iteration 7100 / 14700) loss: 0.752809\n",
      "(Iteration 7101 / 14700) loss: 0.548592\n",
      "(Iteration 7102 / 14700) loss: 0.675298\n",
      "(Iteration 7103 / 14700) loss: 0.685873\n",
      "(Iteration 7104 / 14700) loss: 0.610948\n",
      "(Iteration 7105 / 14700) loss: 0.608999\n",
      "(Iteration 7106 / 14700) loss: 0.747240\n",
      "(Iteration 7107 / 14700) loss: 0.787146\n",
      "(Iteration 7108 / 14700) loss: 0.960782\n",
      "(Iteration 7109 / 14700) loss: 0.784184\n",
      "(Iteration 7110 / 14700) loss: 0.935625\n",
      "(Iteration 7111 / 14700) loss: 0.636022\n",
      "(Iteration 7112 / 14700) loss: 0.630154\n",
      "(Iteration 7113 / 14700) loss: 0.552480\n",
      "(Iteration 7114 / 14700) loss: 0.739932\n",
      "(Iteration 7115 / 14700) loss: 0.864381\n",
      "(Iteration 7116 / 14700) loss: 0.829392\n",
      "(Iteration 7117 / 14700) loss: 0.836975\n",
      "(Iteration 7118 / 14700) loss: 0.560704\n",
      "(Iteration 7119 / 14700) loss: 0.761263\n",
      "(Iteration 7120 / 14700) loss: 0.726750\n",
      "(Iteration 7121 / 14700) loss: 0.662759\n",
      "(Iteration 7122 / 14700) loss: 0.839044\n",
      "(Iteration 7123 / 14700) loss: 0.704335\n",
      "(Iteration 7124 / 14700) loss: 1.220962\n",
      "(Iteration 7125 / 14700) loss: 0.888885\n",
      "(Iteration 7126 / 14700) loss: 0.874439\n",
      "(Iteration 7127 / 14700) loss: 0.971683\n",
      "(Iteration 7128 / 14700) loss: 0.787554\n",
      "(Iteration 7129 / 14700) loss: 0.619829\n",
      "(Iteration 7130 / 14700) loss: 0.684155\n",
      "(Iteration 7131 / 14700) loss: 0.601590\n",
      "(Iteration 7132 / 14700) loss: 0.592706\n",
      "(Iteration 7133 / 14700) loss: 1.065600\n",
      "(Iteration 7134 / 14700) loss: 0.751310\n",
      "(Iteration 7135 / 14700) loss: 0.774516\n",
      "(Iteration 7136 / 14700) loss: 0.733810\n",
      "(Iteration 7137 / 14700) loss: 0.631697\n",
      "(Iteration 7138 / 14700) loss: 1.056481\n",
      "(Iteration 7139 / 14700) loss: 0.920179\n",
      "(Iteration 7140 / 14700) loss: 0.786202\n",
      "(Iteration 7141 / 14700) loss: 1.028275\n",
      "(Iteration 7142 / 14700) loss: 0.666132\n",
      "(Iteration 7143 / 14700) loss: 0.804291\n",
      "(Iteration 7144 / 14700) loss: 0.654908\n",
      "(Iteration 7145 / 14700) loss: 0.517208\n",
      "(Iteration 7146 / 14700) loss: 0.923670\n",
      "(Iteration 7147 / 14700) loss: 0.754423\n",
      "(Iteration 7148 / 14700) loss: 0.794407\n",
      "(Iteration 7149 / 14700) loss: 0.877798\n",
      "(Iteration 7150 / 14700) loss: 0.820670\n",
      "(Iteration 7151 / 14700) loss: 0.939222\n",
      "(Iteration 7152 / 14700) loss: 0.757262\n",
      "(Iteration 7153 / 14700) loss: 0.662244\n",
      "(Iteration 7154 / 14700) loss: 0.934055\n",
      "(Iteration 7155 / 14700) loss: 0.633777\n",
      "(Iteration 7156 / 14700) loss: 0.787863\n",
      "(Iteration 7157 / 14700) loss: 0.627114\n",
      "(Iteration 7158 / 14700) loss: 0.769755\n",
      "(Iteration 7159 / 14700) loss: 0.786578\n",
      "(Iteration 7160 / 14700) loss: 0.633418\n",
      "(Iteration 7161 / 14700) loss: 0.903644\n",
      "(Iteration 7162 / 14700) loss: 0.815411\n",
      "(Iteration 7163 / 14700) loss: 0.722785\n",
      "(Iteration 7164 / 14700) loss: 0.850169\n",
      "(Iteration 7165 / 14700) loss: 0.990448\n",
      "(Iteration 7166 / 14700) loss: 0.804368\n",
      "(Iteration 7167 / 14700) loss: 0.636672\n",
      "(Iteration 7168 / 14700) loss: 0.908566\n",
      "(Iteration 7169 / 14700) loss: 0.914290\n",
      "(Iteration 7170 / 14700) loss: 0.653480\n",
      "(Iteration 7171 / 14700) loss: 0.831569\n",
      "(Iteration 7172 / 14700) loss: 0.720179\n",
      "(Iteration 7173 / 14700) loss: 0.608020\n",
      "(Iteration 7174 / 14700) loss: 0.735562\n",
      "(Iteration 7175 / 14700) loss: 0.900082\n",
      "(Iteration 7176 / 14700) loss: 0.860341\n",
      "(Iteration 7177 / 14700) loss: 0.796041\n",
      "(Iteration 7178 / 14700) loss: 0.873530\n",
      "(Iteration 7179 / 14700) loss: 0.790615\n",
      "(Iteration 7180 / 14700) loss: 0.667142\n",
      "(Iteration 7181 / 14700) loss: 0.733286\n",
      "(Iteration 7182 / 14700) loss: 0.635979\n",
      "(Iteration 7183 / 14700) loss: 0.890792\n",
      "(Iteration 7184 / 14700) loss: 1.048196\n",
      "(Iteration 7185 / 14700) loss: 0.533437\n",
      "(Iteration 7186 / 14700) loss: 0.665642\n",
      "(Iteration 7187 / 14700) loss: 0.737691\n",
      "(Iteration 7188 / 14700) loss: 0.928221\n",
      "(Iteration 7189 / 14700) loss: 0.970618\n",
      "(Iteration 7190 / 14700) loss: 0.699710\n",
      "(Iteration 7191 / 14700) loss: 0.601554\n",
      "(Iteration 7192 / 14700) loss: 0.558551\n",
      "(Iteration 7193 / 14700) loss: 0.858166\n",
      "(Iteration 7194 / 14700) loss: 0.686090\n",
      "(Iteration 7195 / 14700) loss: 0.631453\n",
      "(Iteration 7196 / 14700) loss: 0.651304\n",
      "(Iteration 7197 / 14700) loss: 0.679647\n",
      "(Iteration 7198 / 14700) loss: 1.008237\n",
      "(Iteration 7199 / 14700) loss: 0.533445\n",
      "(Iteration 7200 / 14700) loss: 0.749535\n",
      "(Iteration 7201 / 14700) loss: 0.887254\n",
      "(Iteration 7202 / 14700) loss: 0.643862\n",
      "(Iteration 7203 / 14700) loss: 1.129324\n",
      "(Iteration 7204 / 14700) loss: 0.634751\n",
      "(Iteration 7205 / 14700) loss: 0.950327\n",
      "(Iteration 7206 / 14700) loss: 0.920356\n",
      "(Iteration 7207 / 14700) loss: 0.841799\n",
      "(Iteration 7208 / 14700) loss: 0.887223\n",
      "(Iteration 7209 / 14700) loss: 0.805815\n",
      "(Iteration 7210 / 14700) loss: 0.955962\n",
      "(Iteration 7211 / 14700) loss: 0.869263\n",
      "(Iteration 7212 / 14700) loss: 0.938145\n",
      "(Iteration 7213 / 14700) loss: 0.698504\n",
      "(Iteration 7214 / 14700) loss: 1.046503\n",
      "(Iteration 7215 / 14700) loss: 0.933842\n",
      "(Iteration 7216 / 14700) loss: 0.688309\n",
      "(Iteration 7217 / 14700) loss: 0.526126\n",
      "(Iteration 7218 / 14700) loss: 0.875505\n",
      "(Iteration 7219 / 14700) loss: 0.691966\n",
      "(Iteration 7220 / 14700) loss: 0.746290\n",
      "(Iteration 7221 / 14700) loss: 0.770505\n",
      "(Iteration 7222 / 14700) loss: 0.858822\n",
      "(Iteration 7223 / 14700) loss: 0.873585\n",
      "(Iteration 7224 / 14700) loss: 0.868568\n",
      "(Iteration 7225 / 14700) loss: 0.680983\n",
      "(Iteration 7226 / 14700) loss: 0.632589\n",
      "(Iteration 7227 / 14700) loss: 0.589256\n",
      "(Iteration 7228 / 14700) loss: 0.769195\n",
      "(Iteration 7229 / 14700) loss: 0.659607\n",
      "(Iteration 7230 / 14700) loss: 0.654766\n",
      "(Iteration 7231 / 14700) loss: 0.816925\n",
      "(Iteration 7232 / 14700) loss: 0.755413\n",
      "(Iteration 7233 / 14700) loss: 0.801314\n",
      "(Iteration 7234 / 14700) loss: 0.774808\n",
      "(Iteration 7235 / 14700) loss: 1.176722\n",
      "(Iteration 7236 / 14700) loss: 0.607360\n",
      "(Iteration 7237 / 14700) loss: 0.739984\n",
      "(Iteration 7238 / 14700) loss: 0.788174\n",
      "(Iteration 7239 / 14700) loss: 0.752188\n",
      "(Iteration 7240 / 14700) loss: 0.574565\n",
      "(Iteration 7241 / 14700) loss: 0.736180\n",
      "(Iteration 7242 / 14700) loss: 0.580316\n",
      "(Iteration 7243 / 14700) loss: 0.982673\n",
      "(Iteration 7244 / 14700) loss: 0.932380\n",
      "(Iteration 7245 / 14700) loss: 0.739580\n",
      "(Iteration 7246 / 14700) loss: 0.727161\n",
      "(Iteration 7247 / 14700) loss: 0.621010\n",
      "(Iteration 7248 / 14700) loss: 0.733650\n",
      "(Iteration 7249 / 14700) loss: 0.710715\n",
      "(Iteration 7250 / 14700) loss: 0.738345\n",
      "(Iteration 7251 / 14700) loss: 0.914676\n",
      "(Iteration 7252 / 14700) loss: 0.631181\n",
      "(Iteration 7253 / 14700) loss: 0.840102\n",
      "(Iteration 7254 / 14700) loss: 0.584096\n",
      "(Iteration 7255 / 14700) loss: 0.842144\n",
      "(Iteration 7256 / 14700) loss: 0.881800\n",
      "(Iteration 7257 / 14700) loss: 0.800833\n",
      "(Iteration 7258 / 14700) loss: 0.617588\n",
      "(Iteration 7259 / 14700) loss: 0.812893\n",
      "(Iteration 7260 / 14700) loss: 0.893603\n",
      "(Iteration 7261 / 14700) loss: 0.821726\n",
      "(Iteration 7262 / 14700) loss: 0.704112\n",
      "(Iteration 7263 / 14700) loss: 1.031352\n",
      "(Iteration 7264 / 14700) loss: 1.078743\n",
      "(Iteration 7265 / 14700) loss: 0.703974\n",
      "(Iteration 7266 / 14700) loss: 0.639483\n",
      "(Iteration 7267 / 14700) loss: 1.248482\n",
      "(Iteration 7268 / 14700) loss: 0.774839\n",
      "(Iteration 7269 / 14700) loss: 0.630121\n",
      "(Iteration 7270 / 14700) loss: 0.839956\n",
      "(Iteration 7271 / 14700) loss: 0.548237\n",
      "(Iteration 7272 / 14700) loss: 0.948171\n",
      "(Iteration 7273 / 14700) loss: 0.800424\n",
      "(Iteration 7274 / 14700) loss: 0.653584\n",
      "(Iteration 7275 / 14700) loss: 0.686277\n",
      "(Iteration 7276 / 14700) loss: 0.862070\n",
      "(Iteration 7277 / 14700) loss: 0.764547\n",
      "(Iteration 7278 / 14700) loss: 0.720085\n",
      "(Iteration 7279 / 14700) loss: 0.607249\n",
      "(Iteration 7280 / 14700) loss: 0.722123\n",
      "(Iteration 7281 / 14700) loss: 0.793857\n",
      "(Iteration 7282 / 14700) loss: 0.731762\n",
      "(Iteration 7283 / 14700) loss: 0.647846\n",
      "(Iteration 7284 / 14700) loss: 0.730756\n",
      "(Iteration 7285 / 14700) loss: 0.834263\n",
      "(Iteration 7286 / 14700) loss: 0.705474\n",
      "(Iteration 7287 / 14700) loss: 0.665802\n",
      "(Iteration 7288 / 14700) loss: 0.889075\n",
      "(Iteration 7289 / 14700) loss: 0.802856\n",
      "(Iteration 7290 / 14700) loss: 1.018984\n",
      "(Iteration 7291 / 14700) loss: 0.762675\n",
      "(Iteration 7292 / 14700) loss: 0.494644\n",
      "(Iteration 7293 / 14700) loss: 0.879255\n",
      "(Iteration 7294 / 14700) loss: 0.715346\n",
      "(Iteration 7295 / 14700) loss: 0.679169\n",
      "(Iteration 7296 / 14700) loss: 0.784937\n",
      "(Iteration 7297 / 14700) loss: 0.548861\n",
      "(Iteration 7298 / 14700) loss: 0.724609\n",
      "(Iteration 7299 / 14700) loss: 0.829792\n",
      "(Iteration 7300 / 14700) loss: 0.574944\n",
      "(Iteration 7301 / 14700) loss: 0.635355\n",
      "(Iteration 7302 / 14700) loss: 0.813316\n",
      "(Iteration 7303 / 14700) loss: 0.659163\n",
      "(Iteration 7304 / 14700) loss: 0.999252\n",
      "(Iteration 7305 / 14700) loss: 0.661947\n",
      "(Iteration 7306 / 14700) loss: 0.814266\n",
      "(Iteration 7307 / 14700) loss: 0.549967\n",
      "(Iteration 7308 / 14700) loss: 1.038918\n",
      "(Iteration 7309 / 14700) loss: 1.026340\n",
      "(Iteration 7310 / 14700) loss: 0.927312\n",
      "(Iteration 7311 / 14700) loss: 0.755754\n",
      "(Iteration 7312 / 14700) loss: 0.571595\n",
      "(Iteration 7313 / 14700) loss: 0.479486\n",
      "(Iteration 7314 / 14700) loss: 0.794753\n",
      "(Iteration 7315 / 14700) loss: 0.504478\n",
      "(Iteration 7316 / 14700) loss: 1.145599\n",
      "(Iteration 7317 / 14700) loss: 0.875080\n",
      "(Iteration 7318 / 14700) loss: 0.662789\n",
      "(Iteration 7319 / 14700) loss: 0.712877\n",
      "(Iteration 7320 / 14700) loss: 0.565416\n",
      "(Iteration 7321 / 14700) loss: 0.680681\n",
      "(Iteration 7322 / 14700) loss: 0.643174\n",
      "(Iteration 7323 / 14700) loss: 0.838988\n",
      "(Iteration 7324 / 14700) loss: 0.595207\n",
      "(Iteration 7325 / 14700) loss: 0.589913\n",
      "(Iteration 7326 / 14700) loss: 0.738562\n",
      "(Iteration 7327 / 14700) loss: 0.895195\n",
      "(Iteration 7328 / 14700) loss: 0.920265\n",
      "(Iteration 7329 / 14700) loss: 0.609426\n",
      "(Iteration 7330 / 14700) loss: 0.685097\n",
      "(Iteration 7331 / 14700) loss: 0.873691\n",
      "(Iteration 7332 / 14700) loss: 0.838398\n",
      "(Iteration 7333 / 14700) loss: 0.826891\n",
      "(Iteration 7334 / 14700) loss: 0.652760\n",
      "(Iteration 7335 / 14700) loss: 0.776372\n",
      "(Iteration 7336 / 14700) loss: 0.916366\n",
      "(Iteration 7337 / 14700) loss: 0.521255\n",
      "(Iteration 7338 / 14700) loss: 1.064212\n",
      "(Iteration 7339 / 14700) loss: 0.872817\n",
      "(Iteration 7340 / 14700) loss: 1.135335\n",
      "(Iteration 7341 / 14700) loss: 0.805210\n",
      "(Iteration 7342 / 14700) loss: 0.632574\n",
      "(Iteration 7343 / 14700) loss: 1.019943\n",
      "(Iteration 7344 / 14700) loss: 0.857452\n",
      "(Iteration 7345 / 14700) loss: 0.860962\n",
      "(Iteration 7346 / 14700) loss: 0.616538\n",
      "(Iteration 7347 / 14700) loss: 0.775042\n",
      "(Iteration 7348 / 14700) loss: 1.100692\n",
      "(Iteration 7349 / 14700) loss: 0.808633\n",
      "(Iteration 7350 / 14700) loss: 0.800717\n",
      "(Iteration 7351 / 14700) loss: 0.788019\n",
      "(Iteration 7352 / 14700) loss: 0.813774\n",
      "(Iteration 7353 / 14700) loss: 0.754367\n",
      "(Iteration 7354 / 14700) loss: 0.935499\n",
      "(Iteration 7355 / 14700) loss: 0.736732\n",
      "(Iteration 7356 / 14700) loss: 0.821912\n",
      "(Iteration 7357 / 14700) loss: 0.595809\n",
      "(Iteration 7358 / 14700) loss: 0.897352\n",
      "(Iteration 7359 / 14700) loss: 1.013789\n",
      "(Iteration 7360 / 14700) loss: 0.725378\n",
      "(Iteration 7361 / 14700) loss: 0.744208\n",
      "(Iteration 7362 / 14700) loss: 0.762953\n",
      "(Iteration 7363 / 14700) loss: 0.932581\n",
      "(Iteration 7364 / 14700) loss: 0.820975\n",
      "(Iteration 7365 / 14700) loss: 0.813490\n",
      "(Iteration 7366 / 14700) loss: 1.137555\n",
      "(Iteration 7367 / 14700) loss: 0.824740\n",
      "(Iteration 7368 / 14700) loss: 0.743270\n",
      "(Iteration 7369 / 14700) loss: 0.618867\n",
      "(Iteration 7370 / 14700) loss: 0.726870\n",
      "(Iteration 7371 / 14700) loss: 0.651780\n",
      "(Iteration 7372 / 14700) loss: 0.602806\n",
      "(Iteration 7373 / 14700) loss: 1.129205\n",
      "(Iteration 7374 / 14700) loss: 0.672565\n",
      "(Iteration 7375 / 14700) loss: 0.642328\n",
      "(Iteration 7376 / 14700) loss: 0.849568\n",
      "(Iteration 7377 / 14700) loss: 0.907844\n",
      "(Iteration 7378 / 14700) loss: 0.984322\n",
      "(Iteration 7379 / 14700) loss: 0.607901\n",
      "(Iteration 7380 / 14700) loss: 0.721855\n",
      "(Iteration 7381 / 14700) loss: 0.906942\n",
      "(Iteration 7382 / 14700) loss: 0.813396\n",
      "(Iteration 7383 / 14700) loss: 1.044173\n",
      "(Iteration 7384 / 14700) loss: 0.919267\n",
      "(Iteration 7385 / 14700) loss: 0.719226\n",
      "(Iteration 7386 / 14700) loss: 0.621193\n",
      "(Iteration 7387 / 14700) loss: 0.968606\n",
      "(Iteration 7388 / 14700) loss: 0.911886\n",
      "(Iteration 7389 / 14700) loss: 0.649353\n",
      "(Iteration 7390 / 14700) loss: 0.757027\n",
      "(Iteration 7391 / 14700) loss: 0.885144\n",
      "(Iteration 7392 / 14700) loss: 0.669675\n",
      "(Iteration 7393 / 14700) loss: 1.214189\n",
      "(Iteration 7394 / 14700) loss: 1.056034\n",
      "(Iteration 7395 / 14700) loss: 0.758565\n",
      "(Iteration 7396 / 14700) loss: 0.492417\n",
      "(Iteration 7397 / 14700) loss: 0.699839\n",
      "(Iteration 7398 / 14700) loss: 0.672467\n",
      "(Iteration 7399 / 14700) loss: 0.631681\n",
      "(Iteration 7400 / 14700) loss: 0.961477\n",
      "(Iteration 7401 / 14700) loss: 0.716942\n",
      "(Iteration 7402 / 14700) loss: 1.145920\n",
      "(Iteration 7403 / 14700) loss: 0.964678\n",
      "(Iteration 7404 / 14700) loss: 0.955192\n",
      "(Iteration 7405 / 14700) loss: 0.777261\n",
      "(Iteration 7406 / 14700) loss: 0.743768\n",
      "(Iteration 7407 / 14700) loss: 0.831627\n",
      "(Iteration 7408 / 14700) loss: 0.776739\n",
      "(Iteration 7409 / 14700) loss: 0.754803\n",
      "(Iteration 7410 / 14700) loss: 0.988350\n",
      "(Iteration 7411 / 14700) loss: 0.650481\n",
      "(Iteration 7412 / 14700) loss: 0.666992\n",
      "(Iteration 7413 / 14700) loss: 0.680442\n",
      "(Iteration 7414 / 14700) loss: 0.776461\n",
      "(Iteration 7415 / 14700) loss: 0.757668\n",
      "(Iteration 7416 / 14700) loss: 0.674095\n",
      "(Iteration 7417 / 14700) loss: 0.758425\n",
      "(Iteration 7418 / 14700) loss: 0.857153\n",
      "(Iteration 7419 / 14700) loss: 1.022380\n",
      "(Iteration 7420 / 14700) loss: 0.892651\n",
      "(Iteration 7421 / 14700) loss: 0.809722\n",
      "(Iteration 7422 / 14700) loss: 0.631552\n",
      "(Iteration 7423 / 14700) loss: 0.735935\n",
      "(Iteration 7424 / 14700) loss: 0.849833\n",
      "(Iteration 7425 / 14700) loss: 0.817314\n",
      "(Iteration 7426 / 14700) loss: 1.043204\n",
      "(Iteration 7427 / 14700) loss: 0.741808\n",
      "(Iteration 7428 / 14700) loss: 0.848013\n",
      "(Iteration 7429 / 14700) loss: 0.582358\n",
      "(Iteration 7430 / 14700) loss: 0.638950\n",
      "(Iteration 7431 / 14700) loss: 1.132145\n",
      "(Iteration 7432 / 14700) loss: 0.934221\n",
      "(Iteration 7433 / 14700) loss: 0.836241\n",
      "(Iteration 7434 / 14700) loss: 0.644286\n",
      "(Iteration 7435 / 14700) loss: 0.823844\n",
      "(Iteration 7436 / 14700) loss: 0.547972\n",
      "(Iteration 7437 / 14700) loss: 0.719117\n",
      "(Iteration 7438 / 14700) loss: 0.988486\n",
      "(Iteration 7439 / 14700) loss: 0.800663\n",
      "(Iteration 7440 / 14700) loss: 0.784828\n",
      "(Iteration 7441 / 14700) loss: 0.824120\n",
      "(Iteration 7442 / 14700) loss: 0.783414\n",
      "(Iteration 7443 / 14700) loss: 0.817719\n",
      "(Iteration 7444 / 14700) loss: 0.908616\n",
      "(Iteration 7445 / 14700) loss: 0.815586\n",
      "(Iteration 7446 / 14700) loss: 0.613178\n",
      "(Iteration 7447 / 14700) loss: 0.978760\n",
      "(Iteration 7448 / 14700) loss: 0.759227\n",
      "(Iteration 7449 / 14700) loss: 0.739884\n",
      "(Iteration 7450 / 14700) loss: 0.977338\n",
      "(Iteration 7451 / 14700) loss: 0.747099\n",
      "(Iteration 7452 / 14700) loss: 0.739712\n",
      "(Iteration 7453 / 14700) loss: 0.677509\n",
      "(Iteration 7454 / 14700) loss: 0.584155\n",
      "(Iteration 7455 / 14700) loss: 0.580941\n",
      "(Iteration 7456 / 14700) loss: 0.936360\n",
      "(Iteration 7457 / 14700) loss: 0.869405\n",
      "(Iteration 7458 / 14700) loss: 0.684623\n",
      "(Iteration 7459 / 14700) loss: 0.739723\n",
      "(Iteration 7460 / 14700) loss: 1.103722\n",
      "(Iteration 7461 / 14700) loss: 0.915902\n",
      "(Iteration 7462 / 14700) loss: 1.076307\n",
      "(Iteration 7463 / 14700) loss: 0.655742\n",
      "(Iteration 7464 / 14700) loss: 0.642891\n",
      "(Iteration 7465 / 14700) loss: 0.716559\n",
      "(Iteration 7466 / 14700) loss: 0.797113\n",
      "(Iteration 7467 / 14700) loss: 0.818385\n",
      "(Iteration 7468 / 14700) loss: 0.793674\n",
      "(Iteration 7469 / 14700) loss: 0.940038\n",
      "(Iteration 7470 / 14700) loss: 0.863572\n",
      "(Iteration 7471 / 14700) loss: 0.752403\n",
      "(Iteration 7472 / 14700) loss: 0.694987\n",
      "(Iteration 7473 / 14700) loss: 0.883911\n",
      "(Iteration 7474 / 14700) loss: 0.808098\n",
      "(Iteration 7475 / 14700) loss: 0.831383\n",
      "(Iteration 7476 / 14700) loss: 0.887344\n",
      "(Iteration 7477 / 14700) loss: 0.946213\n",
      "(Iteration 7478 / 14700) loss: 0.829667\n",
      "(Iteration 7479 / 14700) loss: 0.820346\n",
      "(Iteration 7480 / 14700) loss: 0.639734\n",
      "(Iteration 7481 / 14700) loss: 0.692566\n",
      "(Iteration 7482 / 14700) loss: 0.805699\n",
      "(Iteration 7483 / 14700) loss: 0.954409\n",
      "(Iteration 7484 / 14700) loss: 0.697313\n",
      "(Iteration 7485 / 14700) loss: 0.778614\n",
      "(Iteration 7486 / 14700) loss: 0.858120\n",
      "(Iteration 7487 / 14700) loss: 0.991731\n",
      "(Iteration 7488 / 14700) loss: 0.971554\n",
      "(Iteration 7489 / 14700) loss: 0.770951\n",
      "(Iteration 7490 / 14700) loss: 0.816376\n",
      "(Iteration 7491 / 14700) loss: 0.644972\n",
      "(Iteration 7492 / 14700) loss: 0.707553\n",
      "(Iteration 7493 / 14700) loss: 0.824933\n",
      "(Iteration 7494 / 14700) loss: 0.895758\n",
      "(Iteration 7495 / 14700) loss: 0.632266\n",
      "(Iteration 7496 / 14700) loss: 0.839574\n",
      "(Iteration 7497 / 14700) loss: 0.881635\n",
      "(Iteration 7498 / 14700) loss: 0.834881\n",
      "(Iteration 7499 / 14700) loss: 0.742227\n",
      "(Iteration 7500 / 14700) loss: 0.686634\n",
      "(Iteration 7501 / 14700) loss: 0.655426\n",
      "(Iteration 7502 / 14700) loss: 0.924869\n",
      "(Iteration 7503 / 14700) loss: 0.784400\n",
      "(Iteration 7504 / 14700) loss: 0.501457\n",
      "(Iteration 7505 / 14700) loss: 0.912577\n",
      "(Iteration 7506 / 14700) loss: 0.607127\n",
      "(Iteration 7507 / 14700) loss: 0.959958\n",
      "(Iteration 7508 / 14700) loss: 0.660119\n",
      "(Iteration 7509 / 14700) loss: 0.792190\n",
      "(Iteration 7510 / 14700) loss: 0.839609\n",
      "(Iteration 7511 / 14700) loss: 0.753700\n",
      "(Iteration 7512 / 14700) loss: 0.802895\n",
      "(Iteration 7513 / 14700) loss: 0.861464\n",
      "(Iteration 7514 / 14700) loss: 0.750451\n",
      "(Iteration 7515 / 14700) loss: 0.823321\n",
      "(Iteration 7516 / 14700) loss: 0.867069\n",
      "(Iteration 7517 / 14700) loss: 0.691611\n",
      "(Iteration 7518 / 14700) loss: 0.708689\n",
      "(Iteration 7519 / 14700) loss: 0.793334\n",
      "(Iteration 7520 / 14700) loss: 0.861817\n",
      "(Iteration 7521 / 14700) loss: 0.681663\n",
      "(Iteration 7522 / 14700) loss: 0.880058\n",
      "(Iteration 7523 / 14700) loss: 0.562779\n",
      "(Iteration 7524 / 14700) loss: 0.623980\n",
      "(Iteration 7525 / 14700) loss: 0.979910\n",
      "(Iteration 7526 / 14700) loss: 0.507217\n",
      "(Iteration 7527 / 14700) loss: 0.589731\n",
      "(Iteration 7528 / 14700) loss: 0.887464\n",
      "(Iteration 7529 / 14700) loss: 0.640056\n",
      "(Iteration 7530 / 14700) loss: 0.672682\n",
      "(Iteration 7531 / 14700) loss: 1.019204\n",
      "(Iteration 7532 / 14700) loss: 0.790045\n",
      "(Iteration 7533 / 14700) loss: 0.753642\n",
      "(Iteration 7534 / 14700) loss: 0.768826\n",
      "(Iteration 7535 / 14700) loss: 0.609645\n",
      "(Iteration 7536 / 14700) loss: 0.713597\n",
      "(Iteration 7537 / 14700) loss: 0.986883\n",
      "(Iteration 7538 / 14700) loss: 0.757561\n",
      "(Iteration 7539 / 14700) loss: 0.642711\n",
      "(Iteration 7540 / 14700) loss: 1.089657\n",
      "(Iteration 7541 / 14700) loss: 0.592257\n",
      "(Iteration 7542 / 14700) loss: 1.069843\n",
      "(Iteration 7543 / 14700) loss: 0.711008\n",
      "(Iteration 7544 / 14700) loss: 0.863796\n",
      "(Iteration 7545 / 14700) loss: 0.767848\n",
      "(Iteration 7546 / 14700) loss: 0.891490\n",
      "(Iteration 7547 / 14700) loss: 0.646341\n",
      "(Iteration 7548 / 14700) loss: 0.808078\n",
      "(Iteration 7549 / 14700) loss: 1.143756\n",
      "(Iteration 7550 / 14700) loss: 0.918850\n",
      "(Iteration 7551 / 14700) loss: 0.773907\n",
      "(Iteration 7552 / 14700) loss: 0.909601\n",
      "(Iteration 7553 / 14700) loss: 0.882711\n",
      "(Iteration 7554 / 14700) loss: 0.611835\n",
      "(Iteration 7555 / 14700) loss: 0.644065\n",
      "(Iteration 7556 / 14700) loss: 0.791478\n",
      "(Iteration 7557 / 14700) loss: 0.624672\n",
      "(Iteration 7558 / 14700) loss: 0.896729\n",
      "(Iteration 7559 / 14700) loss: 0.741696\n",
      "(Iteration 7560 / 14700) loss: 0.813467\n",
      "(Iteration 7561 / 14700) loss: 0.783838\n",
      "(Iteration 7562 / 14700) loss: 0.659135\n",
      "(Iteration 7563 / 14700) loss: 0.876163\n",
      "(Iteration 7564 / 14700) loss: 0.753380\n",
      "(Iteration 7565 / 14700) loss: 0.843262\n",
      "(Iteration 7566 / 14700) loss: 0.903158\n",
      "(Iteration 7567 / 14700) loss: 0.847478\n",
      "(Iteration 7568 / 14700) loss: 0.651398\n",
      "(Iteration 7569 / 14700) loss: 0.875357\n",
      "(Iteration 7570 / 14700) loss: 0.494175\n",
      "(Iteration 7571 / 14700) loss: 0.672225\n",
      "(Iteration 7572 / 14700) loss: 0.584668\n",
      "(Iteration 7573 / 14700) loss: 0.562651\n",
      "(Iteration 7574 / 14700) loss: 0.823304\n",
      "(Iteration 7575 / 14700) loss: 0.756041\n",
      "(Iteration 7576 / 14700) loss: 1.001984\n",
      "(Iteration 7577 / 14700) loss: 0.564455\n",
      "(Iteration 7578 / 14700) loss: 0.878670\n",
      "(Iteration 7579 / 14700) loss: 0.993283\n",
      "(Iteration 7580 / 14700) loss: 0.835272\n",
      "(Iteration 7581 / 14700) loss: 0.772953\n",
      "(Iteration 7582 / 14700) loss: 0.788440\n",
      "(Iteration 7583 / 14700) loss: 0.637046\n",
      "(Iteration 7584 / 14700) loss: 0.687764\n",
      "(Iteration 7585 / 14700) loss: 0.853943\n",
      "(Iteration 7586 / 14700) loss: 0.753633\n",
      "(Iteration 7587 / 14700) loss: 0.664596\n",
      "(Iteration 7588 / 14700) loss: 0.883708\n",
      "(Iteration 7589 / 14700) loss: 0.639872\n",
      "(Iteration 7590 / 14700) loss: 0.795731\n",
      "(Iteration 7591 / 14700) loss: 0.704343\n",
      "(Iteration 7592 / 14700) loss: 1.063920\n",
      "(Iteration 7593 / 14700) loss: 0.723220\n",
      "(Iteration 7594 / 14700) loss: 0.731954\n",
      "(Iteration 7595 / 14700) loss: 0.692724\n",
      "(Iteration 7596 / 14700) loss: 0.838799\n",
      "(Iteration 7597 / 14700) loss: 0.668996\n",
      "(Iteration 7598 / 14700) loss: 0.864429\n",
      "(Iteration 7599 / 14700) loss: 0.783132\n",
      "(Iteration 7600 / 14700) loss: 0.824852\n",
      "(Iteration 7601 / 14700) loss: 0.984069\n",
      "(Iteration 7602 / 14700) loss: 0.801375\n",
      "(Iteration 7603 / 14700) loss: 1.020172\n",
      "(Iteration 7604 / 14700) loss: 1.088625\n",
      "(Iteration 7605 / 14700) loss: 0.739852\n",
      "(Iteration 7606 / 14700) loss: 0.783550\n",
      "(Iteration 7607 / 14700) loss: 0.637499\n",
      "(Iteration 7608 / 14700) loss: 0.657807\n",
      "(Iteration 7609 / 14700) loss: 1.180646\n",
      "(Iteration 7610 / 14700) loss: 0.725510\n",
      "(Iteration 7611 / 14700) loss: 0.494624\n",
      "(Iteration 7612 / 14700) loss: 0.807395\n",
      "(Iteration 7613 / 14700) loss: 0.926370\n",
      "(Iteration 7614 / 14700) loss: 0.609928\n",
      "(Iteration 7615 / 14700) loss: 0.629403\n",
      "(Iteration 7616 / 14700) loss: 0.782169\n",
      "(Iteration 7617 / 14700) loss: 0.519793\n",
      "(Iteration 7618 / 14700) loss: 0.760164\n",
      "(Iteration 7619 / 14700) loss: 0.702874\n",
      "(Iteration 7620 / 14700) loss: 0.858007\n",
      "(Iteration 7621 / 14700) loss: 1.019665\n",
      "(Iteration 7622 / 14700) loss: 0.870466\n",
      "(Iteration 7623 / 14700) loss: 0.798309\n",
      "(Iteration 7624 / 14700) loss: 1.015290\n",
      "(Iteration 7625 / 14700) loss: 0.756961\n",
      "(Iteration 7626 / 14700) loss: 0.792470\n",
      "(Iteration 7627 / 14700) loss: 1.326335\n",
      "(Iteration 7628 / 14700) loss: 0.833567\n",
      "(Iteration 7629 / 14700) loss: 0.611136\n",
      "(Iteration 7630 / 14700) loss: 0.668407\n",
      "(Iteration 7631 / 14700) loss: 0.980521\n",
      "(Iteration 7632 / 14700) loss: 0.718692\n",
      "(Iteration 7633 / 14700) loss: 0.645920\n",
      "(Iteration 7634 / 14700) loss: 0.769703\n",
      "(Iteration 7635 / 14700) loss: 0.876153\n",
      "(Iteration 7636 / 14700) loss: 0.621115\n",
      "(Iteration 7637 / 14700) loss: 0.733312\n",
      "(Iteration 7638 / 14700) loss: 0.725995\n",
      "(Iteration 7639 / 14700) loss: 0.722698\n",
      "(Iteration 7640 / 14700) loss: 0.520736\n",
      "(Iteration 7641 / 14700) loss: 0.966161\n",
      "(Iteration 7642 / 14700) loss: 0.974085\n",
      "(Iteration 7643 / 14700) loss: 0.821802\n",
      "(Iteration 7644 / 14700) loss: 1.022662\n",
      "(Iteration 7645 / 14700) loss: 0.623498\n",
      "(Iteration 7646 / 14700) loss: 0.830475\n",
      "(Iteration 7647 / 14700) loss: 0.774704\n",
      "(Iteration 7648 / 14700) loss: 0.882625\n",
      "(Iteration 7649 / 14700) loss: 0.669656\n",
      "(Iteration 7650 / 14700) loss: 0.829676\n",
      "(Iteration 7651 / 14700) loss: 0.693678\n",
      "(Iteration 7652 / 14700) loss: 0.676604\n",
      "(Iteration 7653 / 14700) loss: 0.914955\n",
      "(Iteration 7654 / 14700) loss: 0.868317\n",
      "(Iteration 7655 / 14700) loss: 1.013573\n",
      "(Iteration 7656 / 14700) loss: 0.727029\n",
      "(Iteration 7657 / 14700) loss: 0.659257\n",
      "(Iteration 7658 / 14700) loss: 0.607703\n",
      "(Iteration 7659 / 14700) loss: 0.791013\n",
      "(Iteration 7660 / 14700) loss: 0.861961\n",
      "(Iteration 7661 / 14700) loss: 0.730358\n",
      "(Iteration 7662 / 14700) loss: 0.756356\n",
      "(Iteration 7663 / 14700) loss: 0.534937\n",
      "(Iteration 7664 / 14700) loss: 0.829565\n",
      "(Iteration 7665 / 14700) loss: 0.870093\n",
      "(Iteration 7666 / 14700) loss: 0.979005\n",
      "(Iteration 7667 / 14700) loss: 0.772342\n",
      "(Iteration 7668 / 14700) loss: 0.592801\n",
      "(Iteration 7669 / 14700) loss: 0.668761\n",
      "(Iteration 7670 / 14700) loss: 0.478417\n",
      "(Iteration 7671 / 14700) loss: 0.781439\n",
      "(Iteration 7672 / 14700) loss: 0.805282\n",
      "(Iteration 7673 / 14700) loss: 0.755896\n",
      "(Iteration 7674 / 14700) loss: 0.837300\n",
      "(Iteration 7675 / 14700) loss: 0.933397\n",
      "(Iteration 7676 / 14700) loss: 0.949177\n",
      "(Iteration 7677 / 14700) loss: 1.001621\n",
      "(Iteration 7678 / 14700) loss: 0.726990\n",
      "(Iteration 7679 / 14700) loss: 0.736237\n",
      "(Iteration 7680 / 14700) loss: 0.835329\n",
      "(Iteration 7681 / 14700) loss: 0.826320\n",
      "(Iteration 7682 / 14700) loss: 0.841946\n",
      "(Iteration 7683 / 14700) loss: 0.878662\n",
      "(Iteration 7684 / 14700) loss: 0.783585\n",
      "(Iteration 7685 / 14700) loss: 1.042663\n",
      "(Iteration 7686 / 14700) loss: 0.943161\n",
      "(Iteration 7687 / 14700) loss: 0.727518\n",
      "(Iteration 7688 / 14700) loss: 0.719624\n",
      "(Iteration 7689 / 14700) loss: 0.989995\n",
      "(Iteration 7690 / 14700) loss: 0.798280\n",
      "(Iteration 7691 / 14700) loss: 0.810365\n",
      "(Iteration 7692 / 14700) loss: 0.740550\n",
      "(Iteration 7693 / 14700) loss: 0.631905\n",
      "(Iteration 7694 / 14700) loss: 0.641171\n",
      "(Iteration 7695 / 14700) loss: 0.619252\n",
      "(Iteration 7696 / 14700) loss: 0.846842\n",
      "(Iteration 7697 / 14700) loss: 0.807809\n",
      "(Iteration 7698 / 14700) loss: 0.990562\n",
      "(Iteration 7699 / 14700) loss: 0.943252\n",
      "(Iteration 7700 / 14700) loss: 0.821538\n",
      "(Iteration 7701 / 14700) loss: 0.658225\n",
      "(Iteration 7702 / 14700) loss: 0.802658\n",
      "(Iteration 7703 / 14700) loss: 0.696757\n",
      "(Iteration 7704 / 14700) loss: 0.638553\n",
      "(Iteration 7705 / 14700) loss: 0.759714\n",
      "(Iteration 7706 / 14700) loss: 0.838355\n",
      "(Iteration 7707 / 14700) loss: 0.980880\n",
      "(Iteration 7708 / 14700) loss: 0.954636\n",
      "(Iteration 7709 / 14700) loss: 0.981987\n",
      "(Iteration 7710 / 14700) loss: 1.020620\n",
      "(Iteration 7711 / 14700) loss: 0.589994\n",
      "(Iteration 7712 / 14700) loss: 0.798928\n",
      "(Iteration 7713 / 14700) loss: 0.860204\n",
      "(Iteration 7714 / 14700) loss: 0.905371\n",
      "(Iteration 7715 / 14700) loss: 0.842611\n",
      "(Iteration 7716 / 14700) loss: 1.196486\n",
      "(Iteration 7717 / 14700) loss: 0.944701\n",
      "(Iteration 7718 / 14700) loss: 0.964807\n",
      "(Iteration 7719 / 14700) loss: 0.796297\n",
      "(Iteration 7720 / 14700) loss: 0.630857\n",
      "(Iteration 7721 / 14700) loss: 0.906329\n",
      "(Iteration 7722 / 14700) loss: 0.832789\n",
      "(Iteration 7723 / 14700) loss: 0.727011\n",
      "(Iteration 7724 / 14700) loss: 0.933927\n",
      "(Iteration 7725 / 14700) loss: 0.795022\n",
      "(Iteration 7726 / 14700) loss: 0.899154\n",
      "(Iteration 7727 / 14700) loss: 0.896772\n",
      "(Iteration 7728 / 14700) loss: 0.768017\n",
      "(Iteration 7729 / 14700) loss: 1.010340\n",
      "(Iteration 7730 / 14700) loss: 0.684808\n",
      "(Iteration 7731 / 14700) loss: 0.770044\n",
      "(Iteration 7732 / 14700) loss: 0.807186\n",
      "(Iteration 7733 / 14700) loss: 0.848564\n",
      "(Iteration 7734 / 14700) loss: 0.944509\n",
      "(Iteration 7735 / 14700) loss: 0.802971\n",
      "(Iteration 7736 / 14700) loss: 0.717803\n",
      "(Iteration 7737 / 14700) loss: 0.929421\n",
      "(Iteration 7738 / 14700) loss: 0.628583\n",
      "(Iteration 7739 / 14700) loss: 0.606210\n",
      "(Iteration 7740 / 14700) loss: 0.710461\n",
      "(Iteration 7741 / 14700) loss: 0.934689\n",
      "(Iteration 7742 / 14700) loss: 0.649538\n",
      "(Iteration 7743 / 14700) loss: 0.718606\n",
      "(Iteration 7744 / 14700) loss: 0.771143\n",
      "(Iteration 7745 / 14700) loss: 1.185904\n",
      "(Iteration 7746 / 14700) loss: 0.616617\n",
      "(Iteration 7747 / 14700) loss: 0.987977\n",
      "(Iteration 7748 / 14700) loss: 0.838459\n",
      "(Iteration 7749 / 14700) loss: 1.085621\n",
      "(Iteration 7750 / 14700) loss: 0.916549\n",
      "(Iteration 7751 / 14700) loss: 0.803336\n",
      "(Iteration 7752 / 14700) loss: 0.892609\n",
      "(Iteration 7753 / 14700) loss: 0.578753\n",
      "(Iteration 7754 / 14700) loss: 0.671364\n",
      "(Iteration 7755 / 14700) loss: 0.632826\n",
      "(Iteration 7756 / 14700) loss: 0.559706\n",
      "(Iteration 7757 / 14700) loss: 0.750641\n",
      "(Iteration 7758 / 14700) loss: 1.030573\n",
      "(Iteration 7759 / 14700) loss: 0.551096\n",
      "(Iteration 7760 / 14700) loss: 0.887080\n",
      "(Iteration 7761 / 14700) loss: 0.759512\n",
      "(Iteration 7762 / 14700) loss: 0.802322\n",
      "(Iteration 7763 / 14700) loss: 0.794702\n",
      "(Iteration 7764 / 14700) loss: 0.649484\n",
      "(Iteration 7765 / 14700) loss: 0.695952\n",
      "(Iteration 7766 / 14700) loss: 0.671327\n",
      "(Iteration 7767 / 14700) loss: 0.659033\n",
      "(Iteration 7768 / 14700) loss: 0.836782\n",
      "(Iteration 7769 / 14700) loss: 0.916810\n",
      "(Iteration 7770 / 14700) loss: 0.815296\n",
      "(Iteration 7771 / 14700) loss: 0.849860\n",
      "(Iteration 7772 / 14700) loss: 0.648730\n",
      "(Iteration 7773 / 14700) loss: 0.622002\n",
      "(Iteration 7774 / 14700) loss: 0.771777\n",
      "(Iteration 7775 / 14700) loss: 0.750952\n",
      "(Iteration 7776 / 14700) loss: 0.757922\n",
      "(Iteration 7777 / 14700) loss: 0.664447\n",
      "(Iteration 7778 / 14700) loss: 0.811419\n",
      "(Iteration 7779 / 14700) loss: 0.719659\n",
      "(Iteration 7780 / 14700) loss: 0.845276\n",
      "(Iteration 7781 / 14700) loss: 0.823211\n",
      "(Iteration 7782 / 14700) loss: 0.756619\n",
      "(Iteration 7783 / 14700) loss: 0.602120\n",
      "(Iteration 7784 / 14700) loss: 1.006956\n",
      "(Iteration 7785 / 14700) loss: 0.848668\n",
      "(Iteration 7786 / 14700) loss: 0.649878\n",
      "(Iteration 7787 / 14700) loss: 0.937768\n",
      "(Iteration 7788 / 14700) loss: 0.935963\n",
      "(Iteration 7789 / 14700) loss: 0.793828\n",
      "(Iteration 7790 / 14700) loss: 0.691822\n",
      "(Iteration 7791 / 14700) loss: 1.290094\n",
      "(Iteration 7792 / 14700) loss: 1.012891\n",
      "(Iteration 7793 / 14700) loss: 0.697468\n",
      "(Iteration 7794 / 14700) loss: 0.584836\n",
      "(Iteration 7795 / 14700) loss: 0.919750\n",
      "(Iteration 7796 / 14700) loss: 0.993127\n",
      "(Iteration 7797 / 14700) loss: 0.824241\n",
      "(Iteration 7798 / 14700) loss: 0.653104\n",
      "(Iteration 7799 / 14700) loss: 0.753558\n",
      "(Iteration 7800 / 14700) loss: 0.629087\n",
      "(Iteration 7801 / 14700) loss: 0.948473\n",
      "(Iteration 7802 / 14700) loss: 0.647226\n",
      "(Iteration 7803 / 14700) loss: 0.697500\n",
      "(Iteration 7804 / 14700) loss: 0.530057\n",
      "(Iteration 7805 / 14700) loss: 0.737960\n",
      "(Iteration 7806 / 14700) loss: 0.918166\n",
      "(Iteration 7807 / 14700) loss: 0.884710\n",
      "(Iteration 7808 / 14700) loss: 0.757980\n",
      "(Iteration 7809 / 14700) loss: 0.947741\n",
      "(Iteration 7810 / 14700) loss: 0.765906\n",
      "(Iteration 7811 / 14700) loss: 0.692016\n",
      "(Iteration 7812 / 14700) loss: 0.853995\n",
      "(Iteration 7813 / 14700) loss: 0.701194\n",
      "(Iteration 7814 / 14700) loss: 0.719720\n",
      "(Iteration 7815 / 14700) loss: 0.764602\n",
      "(Iteration 7816 / 14700) loss: 0.738942\n",
      "(Iteration 7817 / 14700) loss: 0.635176\n",
      "(Iteration 7818 / 14700) loss: 0.974329\n",
      "(Iteration 7819 / 14700) loss: 0.573349\n",
      "(Iteration 7820 / 14700) loss: 0.760771\n",
      "(Iteration 7821 / 14700) loss: 0.673059\n",
      "(Iteration 7822 / 14700) loss: 0.838281\n",
      "(Iteration 7823 / 14700) loss: 0.627551\n",
      "(Iteration 7824 / 14700) loss: 0.892035\n",
      "(Iteration 7825 / 14700) loss: 0.807446\n",
      "(Iteration 7826 / 14700) loss: 0.625157\n",
      "(Iteration 7827 / 14700) loss: 0.926399\n",
      "(Iteration 7828 / 14700) loss: 0.827786\n",
      "(Iteration 7829 / 14700) loss: 0.560866\n",
      "(Iteration 7830 / 14700) loss: 0.824612\n",
      "(Iteration 7831 / 14700) loss: 0.780542\n",
      "(Iteration 7832 / 14700) loss: 0.523340\n",
      "(Iteration 7833 / 14700) loss: 0.512935\n",
      "(Iteration 7834 / 14700) loss: 0.614264\n",
      "(Iteration 7835 / 14700) loss: 0.749191\n",
      "(Iteration 7836 / 14700) loss: 0.789761\n",
      "(Iteration 7837 / 14700) loss: 0.940406\n",
      "(Iteration 7838 / 14700) loss: 1.087235\n",
      "(Iteration 7839 / 14700) loss: 0.735354\n",
      "(Iteration 7840 / 14700) loss: 0.685099\n",
      "(Epoch 8 / 15) train acc: 0.782000; val_acc: 0.764000\n",
      "(Iteration 7841 / 14700) loss: 0.782970\n",
      "(Iteration 7842 / 14700) loss: 0.793108\n",
      "(Iteration 7843 / 14700) loss: 0.650728\n",
      "(Iteration 7844 / 14700) loss: 1.067840\n",
      "(Iteration 7845 / 14700) loss: 0.857846\n",
      "(Iteration 7846 / 14700) loss: 0.933915\n",
      "(Iteration 7847 / 14700) loss: 0.715585\n",
      "(Iteration 7848 / 14700) loss: 0.810451\n",
      "(Iteration 7849 / 14700) loss: 0.728272\n",
      "(Iteration 7850 / 14700) loss: 0.628852\n",
      "(Iteration 7851 / 14700) loss: 0.691274\n",
      "(Iteration 7852 / 14700) loss: 0.650296\n",
      "(Iteration 7853 / 14700) loss: 0.757916\n",
      "(Iteration 7854 / 14700) loss: 0.539944\n",
      "(Iteration 7855 / 14700) loss: 0.843031\n",
      "(Iteration 7856 / 14700) loss: 1.067340\n",
      "(Iteration 7857 / 14700) loss: 0.515809\n",
      "(Iteration 7858 / 14700) loss: 1.053665\n",
      "(Iteration 7859 / 14700) loss: 0.724354\n",
      "(Iteration 7860 / 14700) loss: 0.740512\n",
      "(Iteration 7861 / 14700) loss: 0.663281\n",
      "(Iteration 7862 / 14700) loss: 1.023237\n",
      "(Iteration 7863 / 14700) loss: 0.734882\n",
      "(Iteration 7864 / 14700) loss: 0.710408\n",
      "(Iteration 7865 / 14700) loss: 0.955745\n",
      "(Iteration 7866 / 14700) loss: 0.913905\n",
      "(Iteration 7867 / 14700) loss: 0.718409\n",
      "(Iteration 7868 / 14700) loss: 0.832002\n",
      "(Iteration 7869 / 14700) loss: 0.776879\n",
      "(Iteration 7870 / 14700) loss: 0.536618\n",
      "(Iteration 7871 / 14700) loss: 0.878241\n",
      "(Iteration 7872 / 14700) loss: 0.913712\n",
      "(Iteration 7873 / 14700) loss: 0.782134\n",
      "(Iteration 7874 / 14700) loss: 1.045957\n",
      "(Iteration 7875 / 14700) loss: 0.653356\n",
      "(Iteration 7876 / 14700) loss: 0.681583\n",
      "(Iteration 7877 / 14700) loss: 0.801473\n",
      "(Iteration 7878 / 14700) loss: 0.690323\n",
      "(Iteration 7879 / 14700) loss: 0.839616\n",
      "(Iteration 7880 / 14700) loss: 0.557139\n",
      "(Iteration 7881 / 14700) loss: 0.968233\n",
      "(Iteration 7882 / 14700) loss: 0.768967\n",
      "(Iteration 7883 / 14700) loss: 0.724083\n",
      "(Iteration 7884 / 14700) loss: 0.637765\n",
      "(Iteration 7885 / 14700) loss: 0.734376\n",
      "(Iteration 7886 / 14700) loss: 0.701126\n",
      "(Iteration 7887 / 14700) loss: 0.844517\n",
      "(Iteration 7888 / 14700) loss: 0.707848\n",
      "(Iteration 7889 / 14700) loss: 0.975731\n",
      "(Iteration 7890 / 14700) loss: 0.728645\n",
      "(Iteration 7891 / 14700) loss: 0.815973\n",
      "(Iteration 7892 / 14700) loss: 1.010400\n",
      "(Iteration 7893 / 14700) loss: 0.901943\n",
      "(Iteration 7894 / 14700) loss: 0.928799\n",
      "(Iteration 7895 / 14700) loss: 0.508716\n",
      "(Iteration 7896 / 14700) loss: 0.693449\n",
      "(Iteration 7897 / 14700) loss: 0.645950\n",
      "(Iteration 7898 / 14700) loss: 0.804732\n",
      "(Iteration 7899 / 14700) loss: 0.639018\n",
      "(Iteration 7900 / 14700) loss: 0.842935\n",
      "(Iteration 7901 / 14700) loss: 0.641284\n",
      "(Iteration 7902 / 14700) loss: 0.720199\n",
      "(Iteration 7903 / 14700) loss: 0.637811\n",
      "(Iteration 7904 / 14700) loss: 0.641523\n",
      "(Iteration 7905 / 14700) loss: 0.716300\n",
      "(Iteration 7906 / 14700) loss: 0.766215\n",
      "(Iteration 7907 / 14700) loss: 0.717897\n",
      "(Iteration 7908 / 14700) loss: 0.909800\n",
      "(Iteration 7909 / 14700) loss: 0.714576\n",
      "(Iteration 7910 / 14700) loss: 0.900278\n",
      "(Iteration 7911 / 14700) loss: 0.782807\n",
      "(Iteration 7912 / 14700) loss: 0.996676\n",
      "(Iteration 7913 / 14700) loss: 0.997878\n",
      "(Iteration 7914 / 14700) loss: 0.822852\n",
      "(Iteration 7915 / 14700) loss: 0.833935\n",
      "(Iteration 7916 / 14700) loss: 0.678073\n",
      "(Iteration 7917 / 14700) loss: 0.517054\n",
      "(Iteration 7918 / 14700) loss: 0.774003\n",
      "(Iteration 7919 / 14700) loss: 0.776935\n",
      "(Iteration 7920 / 14700) loss: 1.018833\n",
      "(Iteration 7921 / 14700) loss: 0.947373\n",
      "(Iteration 7922 / 14700) loss: 0.909471\n",
      "(Iteration 7923 / 14700) loss: 0.879125\n",
      "(Iteration 7924 / 14700) loss: 0.815093\n",
      "(Iteration 7925 / 14700) loss: 0.614753\n",
      "(Iteration 7926 / 14700) loss: 0.876458\n",
      "(Iteration 7927 / 14700) loss: 1.178114\n",
      "(Iteration 7928 / 14700) loss: 0.997987\n",
      "(Iteration 7929 / 14700) loss: 0.985901\n",
      "(Iteration 7930 / 14700) loss: 0.882998\n",
      "(Iteration 7931 / 14700) loss: 1.189872\n",
      "(Iteration 7932 / 14700) loss: 0.727018\n",
      "(Iteration 7933 / 14700) loss: 0.723435\n",
      "(Iteration 7934 / 14700) loss: 0.597428\n",
      "(Iteration 7935 / 14700) loss: 0.751412\n",
      "(Iteration 7936 / 14700) loss: 0.812588\n",
      "(Iteration 7937 / 14700) loss: 0.668825\n",
      "(Iteration 7938 / 14700) loss: 0.756675\n",
      "(Iteration 7939 / 14700) loss: 0.842524\n",
      "(Iteration 7940 / 14700) loss: 0.863447\n",
      "(Iteration 7941 / 14700) loss: 0.717962\n",
      "(Iteration 7942 / 14700) loss: 0.981319\n",
      "(Iteration 7943 / 14700) loss: 0.778001\n",
      "(Iteration 7944 / 14700) loss: 0.933293\n",
      "(Iteration 7945 / 14700) loss: 0.829567\n",
      "(Iteration 7946 / 14700) loss: 1.074708\n",
      "(Iteration 7947 / 14700) loss: 0.698471\n",
      "(Iteration 7948 / 14700) loss: 0.534358\n",
      "(Iteration 7949 / 14700) loss: 0.833005\n",
      "(Iteration 7950 / 14700) loss: 0.938790\n",
      "(Iteration 7951 / 14700) loss: 0.678835\n",
      "(Iteration 7952 / 14700) loss: 0.916321\n",
      "(Iteration 7953 / 14700) loss: 0.718651\n",
      "(Iteration 7954 / 14700) loss: 0.869880\n",
      "(Iteration 7955 / 14700) loss: 0.661485\n",
      "(Iteration 7956 / 14700) loss: 0.892851\n",
      "(Iteration 7957 / 14700) loss: 0.604037\n",
      "(Iteration 7958 / 14700) loss: 0.995794\n",
      "(Iteration 7959 / 14700) loss: 0.637090\n",
      "(Iteration 7960 / 14700) loss: 1.034677\n",
      "(Iteration 7961 / 14700) loss: 0.908294\n",
      "(Iteration 7962 / 14700) loss: 0.871258\n",
      "(Iteration 7963 / 14700) loss: 0.948586\n",
      "(Iteration 7964 / 14700) loss: 0.933566\n",
      "(Iteration 7965 / 14700) loss: 0.762967\n",
      "(Iteration 7966 / 14700) loss: 0.576014\n",
      "(Iteration 7967 / 14700) loss: 0.835030\n",
      "(Iteration 7968 / 14700) loss: 0.741405\n",
      "(Iteration 7969 / 14700) loss: 0.802365\n",
      "(Iteration 7970 / 14700) loss: 1.074600\n",
      "(Iteration 7971 / 14700) loss: 0.702426\n",
      "(Iteration 7972 / 14700) loss: 0.700384\n",
      "(Iteration 7973 / 14700) loss: 0.664106\n",
      "(Iteration 7974 / 14700) loss: 0.815444\n",
      "(Iteration 7975 / 14700) loss: 0.766018\n",
      "(Iteration 7976 / 14700) loss: 0.886605\n",
      "(Iteration 7977 / 14700) loss: 0.712985\n",
      "(Iteration 7978 / 14700) loss: 1.007039\n",
      "(Iteration 7979 / 14700) loss: 0.726481\n",
      "(Iteration 7980 / 14700) loss: 0.897577\n",
      "(Iteration 7981 / 14700) loss: 0.842021\n",
      "(Iteration 7982 / 14700) loss: 0.935590\n",
      "(Iteration 7983 / 14700) loss: 0.663648\n",
      "(Iteration 7984 / 14700) loss: 0.834850\n",
      "(Iteration 7985 / 14700) loss: 0.783355\n",
      "(Iteration 7986 / 14700) loss: 0.711023\n",
      "(Iteration 7987 / 14700) loss: 0.739170\n",
      "(Iteration 7988 / 14700) loss: 0.635948\n",
      "(Iteration 7989 / 14700) loss: 0.726934\n",
      "(Iteration 7990 / 14700) loss: 0.945765\n",
      "(Iteration 7991 / 14700) loss: 0.830509\n",
      "(Iteration 7992 / 14700) loss: 0.705439\n",
      "(Iteration 7993 / 14700) loss: 0.693575\n",
      "(Iteration 7994 / 14700) loss: 0.791587\n",
      "(Iteration 7995 / 14700) loss: 0.577691\n",
      "(Iteration 7996 / 14700) loss: 0.699256\n",
      "(Iteration 7997 / 14700) loss: 0.785184\n",
      "(Iteration 7998 / 14700) loss: 1.067052\n",
      "(Iteration 7999 / 14700) loss: 0.933244\n",
      "(Iteration 8000 / 14700) loss: 0.741921\n",
      "(Iteration 8001 / 14700) loss: 0.883543\n",
      "(Iteration 8002 / 14700) loss: 0.795808\n",
      "(Iteration 8003 / 14700) loss: 0.970481\n",
      "(Iteration 8004 / 14700) loss: 0.890122\n",
      "(Iteration 8005 / 14700) loss: 0.667974\n",
      "(Iteration 8006 / 14700) loss: 0.815283\n",
      "(Iteration 8007 / 14700) loss: 0.709511\n",
      "(Iteration 8008 / 14700) loss: 0.586017\n",
      "(Iteration 8009 / 14700) loss: 0.799861\n",
      "(Iteration 8010 / 14700) loss: 0.884676\n",
      "(Iteration 8011 / 14700) loss: 0.624568\n",
      "(Iteration 8012 / 14700) loss: 0.646277\n",
      "(Iteration 8013 / 14700) loss: 0.703413\n",
      "(Iteration 8014 / 14700) loss: 0.788954\n",
      "(Iteration 8015 / 14700) loss: 0.801930\n",
      "(Iteration 8016 / 14700) loss: 0.528810\n",
      "(Iteration 8017 / 14700) loss: 0.855438\n",
      "(Iteration 8018 / 14700) loss: 0.758216\n",
      "(Iteration 8019 / 14700) loss: 0.697851\n",
      "(Iteration 8020 / 14700) loss: 0.693751\n",
      "(Iteration 8021 / 14700) loss: 0.642590\n",
      "(Iteration 8022 / 14700) loss: 1.009126\n",
      "(Iteration 8023 / 14700) loss: 0.709883\n",
      "(Iteration 8024 / 14700) loss: 0.676196\n",
      "(Iteration 8025 / 14700) loss: 0.748590\n",
      "(Iteration 8026 / 14700) loss: 0.808178\n",
      "(Iteration 8027 / 14700) loss: 0.835760\n",
      "(Iteration 8028 / 14700) loss: 0.674904\n",
      "(Iteration 8029 / 14700) loss: 0.982134\n",
      "(Iteration 8030 / 14700) loss: 0.694260\n",
      "(Iteration 8031 / 14700) loss: 0.578443\n",
      "(Iteration 8032 / 14700) loss: 0.581141\n",
      "(Iteration 8033 / 14700) loss: 0.575756\n",
      "(Iteration 8034 / 14700) loss: 0.911108\n",
      "(Iteration 8035 / 14700) loss: 0.656449\n",
      "(Iteration 8036 / 14700) loss: 0.725474\n",
      "(Iteration 8037 / 14700) loss: 0.664289\n",
      "(Iteration 8038 / 14700) loss: 0.596791\n",
      "(Iteration 8039 / 14700) loss: 0.726075\n",
      "(Iteration 8040 / 14700) loss: 0.893322\n",
      "(Iteration 8041 / 14700) loss: 0.910543\n",
      "(Iteration 8042 / 14700) loss: 0.760434\n",
      "(Iteration 8043 / 14700) loss: 0.508536\n",
      "(Iteration 8044 / 14700) loss: 0.689118\n",
      "(Iteration 8045 / 14700) loss: 0.698389\n",
      "(Iteration 8046 / 14700) loss: 0.829139\n",
      "(Iteration 8047 / 14700) loss: 0.652804\n",
      "(Iteration 8048 / 14700) loss: 0.650348\n",
      "(Iteration 8049 / 14700) loss: 0.790946\n",
      "(Iteration 8050 / 14700) loss: 0.884404\n",
      "(Iteration 8051 / 14700) loss: 0.774287\n",
      "(Iteration 8052 / 14700) loss: 0.870732\n",
      "(Iteration 8053 / 14700) loss: 0.871115\n",
      "(Iteration 8054 / 14700) loss: 0.892025\n",
      "(Iteration 8055 / 14700) loss: 0.775929\n",
      "(Iteration 8056 / 14700) loss: 0.676503\n",
      "(Iteration 8057 / 14700) loss: 0.687045\n",
      "(Iteration 8058 / 14700) loss: 0.833544\n",
      "(Iteration 8059 / 14700) loss: 0.916239\n",
      "(Iteration 8060 / 14700) loss: 0.877172\n",
      "(Iteration 8061 / 14700) loss: 0.826099\n",
      "(Iteration 8062 / 14700) loss: 1.007995\n",
      "(Iteration 8063 / 14700) loss: 0.774694\n",
      "(Iteration 8064 / 14700) loss: 0.926998\n",
      "(Iteration 8065 / 14700) loss: 0.829340\n",
      "(Iteration 8066 / 14700) loss: 0.717871\n",
      "(Iteration 8067 / 14700) loss: 0.843363\n",
      "(Iteration 8068 / 14700) loss: 0.708014\n",
      "(Iteration 8069 / 14700) loss: 0.703784\n",
      "(Iteration 8070 / 14700) loss: 0.686461\n",
      "(Iteration 8071 / 14700) loss: 0.743858\n",
      "(Iteration 8072 / 14700) loss: 0.629661\n",
      "(Iteration 8073 / 14700) loss: 0.726785\n",
      "(Iteration 8074 / 14700) loss: 0.852175\n",
      "(Iteration 8075 / 14700) loss: 0.865867\n",
      "(Iteration 8076 / 14700) loss: 0.704618\n",
      "(Iteration 8077 / 14700) loss: 0.829738\n",
      "(Iteration 8078 / 14700) loss: 0.747843\n",
      "(Iteration 8079 / 14700) loss: 0.795128\n",
      "(Iteration 8080 / 14700) loss: 0.912195\n",
      "(Iteration 8081 / 14700) loss: 0.653338\n",
      "(Iteration 8082 / 14700) loss: 0.822845\n",
      "(Iteration 8083 / 14700) loss: 1.080936\n",
      "(Iteration 8084 / 14700) loss: 0.871497\n",
      "(Iteration 8085 / 14700) loss: 0.838357\n",
      "(Iteration 8086 / 14700) loss: 0.857853\n",
      "(Iteration 8087 / 14700) loss: 0.882007\n",
      "(Iteration 8088 / 14700) loss: 0.728942\n",
      "(Iteration 8089 / 14700) loss: 0.977644\n",
      "(Iteration 8090 / 14700) loss: 0.707211\n",
      "(Iteration 8091 / 14700) loss: 0.716766\n",
      "(Iteration 8092 / 14700) loss: 0.804359\n",
      "(Iteration 8093 / 14700) loss: 0.763721\n",
      "(Iteration 8094 / 14700) loss: 0.874965\n",
      "(Iteration 8095 / 14700) loss: 0.476060\n",
      "(Iteration 8096 / 14700) loss: 0.641087\n",
      "(Iteration 8097 / 14700) loss: 0.840975\n",
      "(Iteration 8098 / 14700) loss: 0.660976\n",
      "(Iteration 8099 / 14700) loss: 0.937358\n",
      "(Iteration 8100 / 14700) loss: 0.474142\n",
      "(Iteration 8101 / 14700) loss: 0.871969\n",
      "(Iteration 8102 / 14700) loss: 0.659065\n",
      "(Iteration 8103 / 14700) loss: 0.457094\n",
      "(Iteration 8104 / 14700) loss: 0.660081\n",
      "(Iteration 8105 / 14700) loss: 0.773315\n",
      "(Iteration 8106 / 14700) loss: 0.990588\n",
      "(Iteration 8107 / 14700) loss: 0.619363\n",
      "(Iteration 8108 / 14700) loss: 0.753909\n",
      "(Iteration 8109 / 14700) loss: 0.710050\n",
      "(Iteration 8110 / 14700) loss: 0.762966\n",
      "(Iteration 8111 / 14700) loss: 0.863286\n",
      "(Iteration 8112 / 14700) loss: 0.778837\n",
      "(Iteration 8113 / 14700) loss: 0.787159\n",
      "(Iteration 8114 / 14700) loss: 0.708866\n",
      "(Iteration 8115 / 14700) loss: 0.631245\n",
      "(Iteration 8116 / 14700) loss: 0.600534\n",
      "(Iteration 8117 / 14700) loss: 0.810484\n",
      "(Iteration 8118 / 14700) loss: 1.012111\n",
      "(Iteration 8119 / 14700) loss: 0.750615\n",
      "(Iteration 8120 / 14700) loss: 0.763048\n",
      "(Iteration 8121 / 14700) loss: 0.664442\n",
      "(Iteration 8122 / 14700) loss: 0.678139\n",
      "(Iteration 8123 / 14700) loss: 0.961731\n",
      "(Iteration 8124 / 14700) loss: 0.796271\n",
      "(Iteration 8125 / 14700) loss: 0.631612\n",
      "(Iteration 8126 / 14700) loss: 0.693016\n",
      "(Iteration 8127 / 14700) loss: 0.770984\n",
      "(Iteration 8128 / 14700) loss: 0.795850\n",
      "(Iteration 8129 / 14700) loss: 0.866379\n",
      "(Iteration 8130 / 14700) loss: 0.809292\n",
      "(Iteration 8131 / 14700) loss: 0.702296\n",
      "(Iteration 8132 / 14700) loss: 0.633795\n",
      "(Iteration 8133 / 14700) loss: 1.013857\n",
      "(Iteration 8134 / 14700) loss: 0.799766\n",
      "(Iteration 8135 / 14700) loss: 0.650795\n",
      "(Iteration 8136 / 14700) loss: 0.625496\n",
      "(Iteration 8137 / 14700) loss: 0.924842\n",
      "(Iteration 8138 / 14700) loss: 0.835497\n",
      "(Iteration 8139 / 14700) loss: 0.737108\n",
      "(Iteration 8140 / 14700) loss: 0.785837\n",
      "(Iteration 8141 / 14700) loss: 0.824994\n",
      "(Iteration 8142 / 14700) loss: 0.901373\n",
      "(Iteration 8143 / 14700) loss: 0.545018\n",
      "(Iteration 8144 / 14700) loss: 0.759743\n",
      "(Iteration 8145 / 14700) loss: 0.897580\n",
      "(Iteration 8146 / 14700) loss: 0.553706\n",
      "(Iteration 8147 / 14700) loss: 0.755021\n",
      "(Iteration 8148 / 14700) loss: 0.922790\n",
      "(Iteration 8149 / 14700) loss: 0.703702\n",
      "(Iteration 8150 / 14700) loss: 0.823239\n",
      "(Iteration 8151 / 14700) loss: 0.592446\n",
      "(Iteration 8152 / 14700) loss: 0.910805\n",
      "(Iteration 8153 / 14700) loss: 0.669752\n",
      "(Iteration 8154 / 14700) loss: 1.021345\n",
      "(Iteration 8155 / 14700) loss: 0.694731\n",
      "(Iteration 8156 / 14700) loss: 0.680671\n",
      "(Iteration 8157 / 14700) loss: 0.925421\n",
      "(Iteration 8158 / 14700) loss: 0.829598\n",
      "(Iteration 8159 / 14700) loss: 0.641492\n",
      "(Iteration 8160 / 14700) loss: 0.616436\n",
      "(Iteration 8161 / 14700) loss: 0.721250\n",
      "(Iteration 8162 / 14700) loss: 0.880693\n",
      "(Iteration 8163 / 14700) loss: 0.710683\n",
      "(Iteration 8164 / 14700) loss: 0.979205\n",
      "(Iteration 8165 / 14700) loss: 0.748708\n",
      "(Iteration 8166 / 14700) loss: 0.831149\n",
      "(Iteration 8167 / 14700) loss: 0.743445\n",
      "(Iteration 8168 / 14700) loss: 0.705708\n",
      "(Iteration 8169 / 14700) loss: 0.826408\n",
      "(Iteration 8170 / 14700) loss: 0.869938\n",
      "(Iteration 8171 / 14700) loss: 1.056253\n",
      "(Iteration 8172 / 14700) loss: 0.599994\n",
      "(Iteration 8173 / 14700) loss: 0.648785\n",
      "(Iteration 8174 / 14700) loss: 0.625262\n",
      "(Iteration 8175 / 14700) loss: 0.646530\n",
      "(Iteration 8176 / 14700) loss: 0.829349\n",
      "(Iteration 8177 / 14700) loss: 0.759818\n",
      "(Iteration 8178 / 14700) loss: 0.710489\n",
      "(Iteration 8179 / 14700) loss: 0.650535\n",
      "(Iteration 8180 / 14700) loss: 0.656801\n",
      "(Iteration 8181 / 14700) loss: 0.630315\n",
      "(Iteration 8182 / 14700) loss: 1.004391\n",
      "(Iteration 8183 / 14700) loss: 0.807055\n",
      "(Iteration 8184 / 14700) loss: 0.608316\n",
      "(Iteration 8185 / 14700) loss: 0.815224\n",
      "(Iteration 8186 / 14700) loss: 0.518201\n",
      "(Iteration 8187 / 14700) loss: 0.741737\n",
      "(Iteration 8188 / 14700) loss: 0.941135\n",
      "(Iteration 8189 / 14700) loss: 0.745560\n",
      "(Iteration 8190 / 14700) loss: 0.731409\n",
      "(Iteration 8191 / 14700) loss: 0.792598\n",
      "(Iteration 8192 / 14700) loss: 0.936307\n",
      "(Iteration 8193 / 14700) loss: 0.547430\n",
      "(Iteration 8194 / 14700) loss: 0.917194\n",
      "(Iteration 8195 / 14700) loss: 1.040472\n",
      "(Iteration 8196 / 14700) loss: 0.887770\n",
      "(Iteration 8197 / 14700) loss: 0.540763\n",
      "(Iteration 8198 / 14700) loss: 0.794300\n",
      "(Iteration 8199 / 14700) loss: 0.631472\n",
      "(Iteration 8200 / 14700) loss: 0.851775\n",
      "(Iteration 8201 / 14700) loss: 0.910057\n",
      "(Iteration 8202 / 14700) loss: 0.964776\n",
      "(Iteration 8203 / 14700) loss: 0.682970\n",
      "(Iteration 8204 / 14700) loss: 0.632784\n",
      "(Iteration 8205 / 14700) loss: 0.803443\n",
      "(Iteration 8206 / 14700) loss: 0.555259\n",
      "(Iteration 8207 / 14700) loss: 0.867653\n",
      "(Iteration 8208 / 14700) loss: 0.607422\n",
      "(Iteration 8209 / 14700) loss: 0.729933\n",
      "(Iteration 8210 / 14700) loss: 1.092978\n",
      "(Iteration 8211 / 14700) loss: 0.863527\n",
      "(Iteration 8212 / 14700) loss: 0.871620\n",
      "(Iteration 8213 / 14700) loss: 0.686560\n",
      "(Iteration 8214 / 14700) loss: 0.814310\n",
      "(Iteration 8215 / 14700) loss: 0.699888\n",
      "(Iteration 8216 / 14700) loss: 0.800627\n",
      "(Iteration 8217 / 14700) loss: 0.826248\n",
      "(Iteration 8218 / 14700) loss: 0.842336\n",
      "(Iteration 8219 / 14700) loss: 1.052093\n",
      "(Iteration 8220 / 14700) loss: 0.624544\n",
      "(Iteration 8221 / 14700) loss: 0.910636\n",
      "(Iteration 8222 / 14700) loss: 0.948279\n",
      "(Iteration 8223 / 14700) loss: 0.844059\n",
      "(Iteration 8224 / 14700) loss: 0.839782\n",
      "(Iteration 8225 / 14700) loss: 0.693965\n",
      "(Iteration 8226 / 14700) loss: 0.565523\n",
      "(Iteration 8227 / 14700) loss: 0.750423\n",
      "(Iteration 8228 / 14700) loss: 0.682004\n",
      "(Iteration 8229 / 14700) loss: 0.861826\n",
      "(Iteration 8230 / 14700) loss: 0.502361\n",
      "(Iteration 8231 / 14700) loss: 0.709094\n",
      "(Iteration 8232 / 14700) loss: 0.980975\n",
      "(Iteration 8233 / 14700) loss: 0.781940\n",
      "(Iteration 8234 / 14700) loss: 0.545014\n",
      "(Iteration 8235 / 14700) loss: 0.519368\n",
      "(Iteration 8236 / 14700) loss: 0.626101\n",
      "(Iteration 8237 / 14700) loss: 0.897959\n",
      "(Iteration 8238 / 14700) loss: 0.858373\n",
      "(Iteration 8239 / 14700) loss: 0.700354\n",
      "(Iteration 8240 / 14700) loss: 0.903050\n",
      "(Iteration 8241 / 14700) loss: 0.677686\n",
      "(Iteration 8242 / 14700) loss: 1.015372\n",
      "(Iteration 8243 / 14700) loss: 0.592414\n",
      "(Iteration 8244 / 14700) loss: 0.842800\n",
      "(Iteration 8245 / 14700) loss: 0.768059\n",
      "(Iteration 8246 / 14700) loss: 0.652703\n",
      "(Iteration 8247 / 14700) loss: 0.850184\n",
      "(Iteration 8248 / 14700) loss: 0.705559\n",
      "(Iteration 8249 / 14700) loss: 0.729854\n",
      "(Iteration 8250 / 14700) loss: 0.971534\n",
      "(Iteration 8251 / 14700) loss: 0.643301\n",
      "(Iteration 8252 / 14700) loss: 0.691402\n",
      "(Iteration 8253 / 14700) loss: 0.686122\n",
      "(Iteration 8254 / 14700) loss: 0.815167\n",
      "(Iteration 8255 / 14700) loss: 0.876193\n",
      "(Iteration 8256 / 14700) loss: 0.803175\n",
      "(Iteration 8257 / 14700) loss: 0.605222\n",
      "(Iteration 8258 / 14700) loss: 0.656382\n",
      "(Iteration 8259 / 14700) loss: 1.250264\n",
      "(Iteration 8260 / 14700) loss: 0.694717\n",
      "(Iteration 8261 / 14700) loss: 0.746781\n",
      "(Iteration 8262 / 14700) loss: 0.548383\n",
      "(Iteration 8263 / 14700) loss: 0.720542\n",
      "(Iteration 8264 / 14700) loss: 0.761558\n",
      "(Iteration 8265 / 14700) loss: 0.719444\n",
      "(Iteration 8266 / 14700) loss: 0.673920\n",
      "(Iteration 8267 / 14700) loss: 0.596293\n",
      "(Iteration 8268 / 14700) loss: 0.715182\n",
      "(Iteration 8269 / 14700) loss: 0.708054\n",
      "(Iteration 8270 / 14700) loss: 0.815568\n",
      "(Iteration 8271 / 14700) loss: 0.670408\n",
      "(Iteration 8272 / 14700) loss: 0.868824\n",
      "(Iteration 8273 / 14700) loss: 0.654940\n",
      "(Iteration 8274 / 14700) loss: 0.703341\n",
      "(Iteration 8275 / 14700) loss: 0.617653\n",
      "(Iteration 8276 / 14700) loss: 0.656374\n",
      "(Iteration 8277 / 14700) loss: 0.918607\n",
      "(Iteration 8278 / 14700) loss: 0.659494\n",
      "(Iteration 8279 / 14700) loss: 0.841991\n",
      "(Iteration 8280 / 14700) loss: 0.766820\n",
      "(Iteration 8281 / 14700) loss: 0.844044\n",
      "(Iteration 8282 / 14700) loss: 0.643114\n",
      "(Iteration 8283 / 14700) loss: 0.765055\n",
      "(Iteration 8284 / 14700) loss: 0.777037\n",
      "(Iteration 8285 / 14700) loss: 0.794360\n",
      "(Iteration 8286 / 14700) loss: 0.694572\n",
      "(Iteration 8287 / 14700) loss: 0.434729\n",
      "(Iteration 8288 / 14700) loss: 0.862876\n",
      "(Iteration 8289 / 14700) loss: 0.782818\n",
      "(Iteration 8290 / 14700) loss: 0.772429\n",
      "(Iteration 8291 / 14700) loss: 0.938652\n",
      "(Iteration 8292 / 14700) loss: 0.822143\n",
      "(Iteration 8293 / 14700) loss: 0.754082\n",
      "(Iteration 8294 / 14700) loss: 0.742603\n",
      "(Iteration 8295 / 14700) loss: 0.787271\n",
      "(Iteration 8296 / 14700) loss: 0.829482\n",
      "(Iteration 8297 / 14700) loss: 0.683847\n",
      "(Iteration 8298 / 14700) loss: 0.524721\n",
      "(Iteration 8299 / 14700) loss: 0.760066\n",
      "(Iteration 8300 / 14700) loss: 0.765112\n",
      "(Iteration 8301 / 14700) loss: 0.595651\n",
      "(Iteration 8302 / 14700) loss: 0.737600\n",
      "(Iteration 8303 / 14700) loss: 0.833600\n",
      "(Iteration 8304 / 14700) loss: 0.739509\n",
      "(Iteration 8305 / 14700) loss: 0.893574\n",
      "(Iteration 8306 / 14700) loss: 0.868638\n",
      "(Iteration 8307 / 14700) loss: 0.725571\n",
      "(Iteration 8308 / 14700) loss: 0.572780\n",
      "(Iteration 8309 / 14700) loss: 0.681483\n",
      "(Iteration 8310 / 14700) loss: 0.782688\n",
      "(Iteration 8311 / 14700) loss: 0.871624\n",
      "(Iteration 8312 / 14700) loss: 0.691547\n",
      "(Iteration 8313 / 14700) loss: 0.755558\n",
      "(Iteration 8314 / 14700) loss: 0.985733\n",
      "(Iteration 8315 / 14700) loss: 0.634411\n",
      "(Iteration 8316 / 14700) loss: 0.638936\n",
      "(Iteration 8317 / 14700) loss: 0.731222\n",
      "(Iteration 8318 / 14700) loss: 1.050365\n",
      "(Iteration 8319 / 14700) loss: 0.674722\n",
      "(Iteration 8320 / 14700) loss: 0.808042\n",
      "(Iteration 8321 / 14700) loss: 0.581964\n",
      "(Iteration 8322 / 14700) loss: 0.873392\n",
      "(Iteration 8323 / 14700) loss: 0.807946\n",
      "(Iteration 8324 / 14700) loss: 0.824354\n",
      "(Iteration 8325 / 14700) loss: 0.734029\n",
      "(Iteration 8326 / 14700) loss: 0.882568\n",
      "(Iteration 8327 / 14700) loss: 0.824317\n",
      "(Iteration 8328 / 14700) loss: 0.751429\n",
      "(Iteration 8329 / 14700) loss: 0.664870\n",
      "(Iteration 8330 / 14700) loss: 0.837385\n",
      "(Iteration 8331 / 14700) loss: 0.709056\n",
      "(Iteration 8332 / 14700) loss: 0.731053\n",
      "(Iteration 8333 / 14700) loss: 0.751356\n",
      "(Iteration 8334 / 14700) loss: 0.720229\n",
      "(Iteration 8335 / 14700) loss: 0.832558\n",
      "(Iteration 8336 / 14700) loss: 0.912784\n",
      "(Iteration 8337 / 14700) loss: 0.760379\n",
      "(Iteration 8338 / 14700) loss: 0.590729\n",
      "(Iteration 8339 / 14700) loss: 1.142789\n",
      "(Iteration 8340 / 14700) loss: 0.718962\n",
      "(Iteration 8341 / 14700) loss: 1.034993\n",
      "(Iteration 8342 / 14700) loss: 0.815450\n",
      "(Iteration 8343 / 14700) loss: 0.785119\n",
      "(Iteration 8344 / 14700) loss: 0.690999\n",
      "(Iteration 8345 / 14700) loss: 0.757777\n",
      "(Iteration 8346 / 14700) loss: 0.838397\n",
      "(Iteration 8347 / 14700) loss: 0.819190\n",
      "(Iteration 8348 / 14700) loss: 0.692730\n",
      "(Iteration 8349 / 14700) loss: 0.677184\n",
      "(Iteration 8350 / 14700) loss: 0.640237\n",
      "(Iteration 8351 / 14700) loss: 0.643223\n",
      "(Iteration 8352 / 14700) loss: 0.817065\n",
      "(Iteration 8353 / 14700) loss: 0.693938\n",
      "(Iteration 8354 / 14700) loss: 0.719121\n",
      "(Iteration 8355 / 14700) loss: 0.769611\n",
      "(Iteration 8356 / 14700) loss: 0.877779\n",
      "(Iteration 8357 / 14700) loss: 0.810164\n",
      "(Iteration 8358 / 14700) loss: 0.655339\n",
      "(Iteration 8359 / 14700) loss: 1.079721\n",
      "(Iteration 8360 / 14700) loss: 1.012241\n",
      "(Iteration 8361 / 14700) loss: 0.697767\n",
      "(Iteration 8362 / 14700) loss: 0.609202\n",
      "(Iteration 8363 / 14700) loss: 0.651365\n",
      "(Iteration 8364 / 14700) loss: 0.696985\n",
      "(Iteration 8365 / 14700) loss: 0.677347\n",
      "(Iteration 8366 / 14700) loss: 0.589510\n",
      "(Iteration 8367 / 14700) loss: 0.868102\n",
      "(Iteration 8368 / 14700) loss: 0.594394\n",
      "(Iteration 8369 / 14700) loss: 1.139384\n",
      "(Iteration 8370 / 14700) loss: 0.771170\n",
      "(Iteration 8371 / 14700) loss: 0.995886\n",
      "(Iteration 8372 / 14700) loss: 0.682682\n",
      "(Iteration 8373 / 14700) loss: 0.802522\n",
      "(Iteration 8374 / 14700) loss: 0.828494\n",
      "(Iteration 8375 / 14700) loss: 0.984923\n",
      "(Iteration 8376 / 14700) loss: 0.739232\n",
      "(Iteration 8377 / 14700) loss: 0.616188\n",
      "(Iteration 8378 / 14700) loss: 0.623648\n",
      "(Iteration 8379 / 14700) loss: 0.873548\n",
      "(Iteration 8380 / 14700) loss: 0.901233\n",
      "(Iteration 8381 / 14700) loss: 0.647263\n",
      "(Iteration 8382 / 14700) loss: 0.780894\n",
      "(Iteration 8383 / 14700) loss: 0.738071\n",
      "(Iteration 8384 / 14700) loss: 0.758260\n",
      "(Iteration 8385 / 14700) loss: 0.701843\n",
      "(Iteration 8386 / 14700) loss: 0.688755\n",
      "(Iteration 8387 / 14700) loss: 0.783280\n",
      "(Iteration 8388 / 14700) loss: 0.597966\n",
      "(Iteration 8389 / 14700) loss: 1.045326\n",
      "(Iteration 8390 / 14700) loss: 0.775616\n",
      "(Iteration 8391 / 14700) loss: 0.743371\n",
      "(Iteration 8392 / 14700) loss: 0.923068\n",
      "(Iteration 8393 / 14700) loss: 0.886268\n",
      "(Iteration 8394 / 14700) loss: 0.749460\n",
      "(Iteration 8395 / 14700) loss: 0.753868\n",
      "(Iteration 8396 / 14700) loss: 0.723851\n",
      "(Iteration 8397 / 14700) loss: 0.718659\n",
      "(Iteration 8398 / 14700) loss: 0.592106\n",
      "(Iteration 8399 / 14700) loss: 0.808528\n",
      "(Iteration 8400 / 14700) loss: 0.882517\n",
      "(Iteration 8401 / 14700) loss: 0.700052\n",
      "(Iteration 8402 / 14700) loss: 0.767533\n",
      "(Iteration 8403 / 14700) loss: 0.716383\n",
      "(Iteration 8404 / 14700) loss: 0.831135\n",
      "(Iteration 8405 / 14700) loss: 1.067325\n",
      "(Iteration 8406 / 14700) loss: 0.715541\n",
      "(Iteration 8407 / 14700) loss: 0.633985\n",
      "(Iteration 8408 / 14700) loss: 0.601971\n",
      "(Iteration 8409 / 14700) loss: 0.982989\n",
      "(Iteration 8410 / 14700) loss: 0.720145\n",
      "(Iteration 8411 / 14700) loss: 0.627731\n",
      "(Iteration 8412 / 14700) loss: 0.821700\n",
      "(Iteration 8413 / 14700) loss: 0.928598\n",
      "(Iteration 8414 / 14700) loss: 0.800351\n",
      "(Iteration 8415 / 14700) loss: 1.019467\n",
      "(Iteration 8416 / 14700) loss: 0.745273\n",
      "(Iteration 8417 / 14700) loss: 1.023290\n",
      "(Iteration 8418 / 14700) loss: 0.661597\n",
      "(Iteration 8419 / 14700) loss: 0.981355\n",
      "(Iteration 8420 / 14700) loss: 0.905754\n",
      "(Iteration 8421 / 14700) loss: 0.864352\n",
      "(Iteration 8422 / 14700) loss: 0.780073\n",
      "(Iteration 8423 / 14700) loss: 0.540680\n",
      "(Iteration 8424 / 14700) loss: 0.725345\n",
      "(Iteration 8425 / 14700) loss: 1.040329\n",
      "(Iteration 8426 / 14700) loss: 0.651509\n",
      "(Iteration 8427 / 14700) loss: 0.693816\n",
      "(Iteration 8428 / 14700) loss: 0.743810\n",
      "(Iteration 8429 / 14700) loss: 0.759065\n",
      "(Iteration 8430 / 14700) loss: 0.800419\n",
      "(Iteration 8431 / 14700) loss: 1.132422\n",
      "(Iteration 8432 / 14700) loss: 0.838585\n",
      "(Iteration 8433 / 14700) loss: 0.701445\n",
      "(Iteration 8434 / 14700) loss: 1.062387\n",
      "(Iteration 8435 / 14700) loss: 0.783286\n",
      "(Iteration 8436 / 14700) loss: 0.658401\n",
      "(Iteration 8437 / 14700) loss: 0.650030\n",
      "(Iteration 8438 / 14700) loss: 0.741814\n",
      "(Iteration 8439 / 14700) loss: 1.075731\n",
      "(Iteration 8440 / 14700) loss: 0.573161\n",
      "(Iteration 8441 / 14700) loss: 0.740866\n",
      "(Iteration 8442 / 14700) loss: 0.601761\n",
      "(Iteration 8443 / 14700) loss: 0.745341\n",
      "(Iteration 8444 / 14700) loss: 0.951029\n",
      "(Iteration 8445 / 14700) loss: 0.760384\n",
      "(Iteration 8446 / 14700) loss: 0.686486\n",
      "(Iteration 8447 / 14700) loss: 0.647804\n",
      "(Iteration 8448 / 14700) loss: 0.764543\n",
      "(Iteration 8449 / 14700) loss: 0.855270\n",
      "(Iteration 8450 / 14700) loss: 0.668030\n",
      "(Iteration 8451 / 14700) loss: 0.699382\n",
      "(Iteration 8452 / 14700) loss: 0.679100\n",
      "(Iteration 8453 / 14700) loss: 0.725960\n",
      "(Iteration 8454 / 14700) loss: 0.878686\n",
      "(Iteration 8455 / 14700) loss: 0.630656\n",
      "(Iteration 8456 / 14700) loss: 0.592315\n",
      "(Iteration 8457 / 14700) loss: 0.913947\n",
      "(Iteration 8458 / 14700) loss: 0.665243\n",
      "(Iteration 8459 / 14700) loss: 0.979602\n",
      "(Iteration 8460 / 14700) loss: 0.612180\n",
      "(Iteration 8461 / 14700) loss: 0.758441\n",
      "(Iteration 8462 / 14700) loss: 0.679551\n",
      "(Iteration 8463 / 14700) loss: 0.626647\n",
      "(Iteration 8464 / 14700) loss: 0.687424\n",
      "(Iteration 8465 / 14700) loss: 0.816684\n",
      "(Iteration 8466 / 14700) loss: 0.807962\n",
      "(Iteration 8467 / 14700) loss: 0.597576\n",
      "(Iteration 8468 / 14700) loss: 0.910996\n",
      "(Iteration 8469 / 14700) loss: 0.683305\n",
      "(Iteration 8470 / 14700) loss: 0.801437\n",
      "(Iteration 8471 / 14700) loss: 0.801074\n",
      "(Iteration 8472 / 14700) loss: 0.761715\n",
      "(Iteration 8473 / 14700) loss: 0.655832\n",
      "(Iteration 8474 / 14700) loss: 0.638946\n",
      "(Iteration 8475 / 14700) loss: 0.774694\n",
      "(Iteration 8476 / 14700) loss: 0.939859\n",
      "(Iteration 8477 / 14700) loss: 0.777177\n",
      "(Iteration 8478 / 14700) loss: 0.905676\n",
      "(Iteration 8479 / 14700) loss: 0.547832\n",
      "(Iteration 8480 / 14700) loss: 0.623883\n",
      "(Iteration 8481 / 14700) loss: 0.813451\n",
      "(Iteration 8482 / 14700) loss: 0.679061\n",
      "(Iteration 8483 / 14700) loss: 0.912455\n",
      "(Iteration 8484 / 14700) loss: 0.664152\n",
      "(Iteration 8485 / 14700) loss: 0.774878\n",
      "(Iteration 8486 / 14700) loss: 0.764769\n",
      "(Iteration 8487 / 14700) loss: 0.783140\n",
      "(Iteration 8488 / 14700) loss: 0.678405\n",
      "(Iteration 8489 / 14700) loss: 0.758347\n",
      "(Iteration 8490 / 14700) loss: 0.773026\n",
      "(Iteration 8491 / 14700) loss: 0.796558\n",
      "(Iteration 8492 / 14700) loss: 0.568419\n",
      "(Iteration 8493 / 14700) loss: 0.672725\n",
      "(Iteration 8494 / 14700) loss: 0.620650\n",
      "(Iteration 8495 / 14700) loss: 0.821647\n",
      "(Iteration 8496 / 14700) loss: 0.635546\n",
      "(Iteration 8497 / 14700) loss: 0.703194\n",
      "(Iteration 8498 / 14700) loss: 0.959471\n",
      "(Iteration 8499 / 14700) loss: 0.785689\n",
      "(Iteration 8500 / 14700) loss: 0.811980\n",
      "(Iteration 8501 / 14700) loss: 0.892956\n",
      "(Iteration 8502 / 14700) loss: 0.637667\n",
      "(Iteration 8503 / 14700) loss: 0.709531\n",
      "(Iteration 8504 / 14700) loss: 0.652272\n",
      "(Iteration 8505 / 14700) loss: 0.609324\n",
      "(Iteration 8506 / 14700) loss: 0.910466\n",
      "(Iteration 8507 / 14700) loss: 0.731158\n",
      "(Iteration 8508 / 14700) loss: 0.785607\n",
      "(Iteration 8509 / 14700) loss: 0.940323\n",
      "(Iteration 8510 / 14700) loss: 0.728661\n",
      "(Iteration 8511 / 14700) loss: 0.743533\n",
      "(Iteration 8512 / 14700) loss: 0.677910\n",
      "(Iteration 8513 / 14700) loss: 0.796266\n",
      "(Iteration 8514 / 14700) loss: 0.894717\n",
      "(Iteration 8515 / 14700) loss: 0.879772\n",
      "(Iteration 8516 / 14700) loss: 0.765159\n",
      "(Iteration 8517 / 14700) loss: 0.702858\n",
      "(Iteration 8518 / 14700) loss: 0.662152\n",
      "(Iteration 8519 / 14700) loss: 0.605442\n",
      "(Iteration 8520 / 14700) loss: 0.668798\n",
      "(Iteration 8521 / 14700) loss: 0.683635\n",
      "(Iteration 8522 / 14700) loss: 0.511556\n",
      "(Iteration 8523 / 14700) loss: 0.784824\n",
      "(Iteration 8524 / 14700) loss: 1.003002\n",
      "(Iteration 8525 / 14700) loss: 0.885195\n",
      "(Iteration 8526 / 14700) loss: 0.713665\n",
      "(Iteration 8527 / 14700) loss: 0.684695\n",
      "(Iteration 8528 / 14700) loss: 0.892968\n",
      "(Iteration 8529 / 14700) loss: 0.595166\n",
      "(Iteration 8530 / 14700) loss: 0.468217\n",
      "(Iteration 8531 / 14700) loss: 0.726064\n",
      "(Iteration 8532 / 14700) loss: 0.692778\n",
      "(Iteration 8533 / 14700) loss: 0.544351\n",
      "(Iteration 8534 / 14700) loss: 0.863525\n",
      "(Iteration 8535 / 14700) loss: 0.605747\n",
      "(Iteration 8536 / 14700) loss: 0.729480\n",
      "(Iteration 8537 / 14700) loss: 0.437570\n",
      "(Iteration 8538 / 14700) loss: 0.659097\n",
      "(Iteration 8539 / 14700) loss: 0.696893\n",
      "(Iteration 8540 / 14700) loss: 0.748856\n",
      "(Iteration 8541 / 14700) loss: 0.710392\n",
      "(Iteration 8542 / 14700) loss: 0.698230\n",
      "(Iteration 8543 / 14700) loss: 0.785248\n",
      "(Iteration 8544 / 14700) loss: 0.769424\n",
      "(Iteration 8545 / 14700) loss: 0.887390\n",
      "(Iteration 8546 / 14700) loss: 0.465837\n",
      "(Iteration 8547 / 14700) loss: 0.629704\n",
      "(Iteration 8548 / 14700) loss: 0.642627\n",
      "(Iteration 8549 / 14700) loss: 0.646008\n",
      "(Iteration 8550 / 14700) loss: 0.575334\n",
      "(Iteration 8551 / 14700) loss: 0.834864\n",
      "(Iteration 8552 / 14700) loss: 0.978455\n",
      "(Iteration 8553 / 14700) loss: 0.837353\n",
      "(Iteration 8554 / 14700) loss: 0.739257\n",
      "(Iteration 8555 / 14700) loss: 0.936204\n",
      "(Iteration 8556 / 14700) loss: 1.010598\n",
      "(Iteration 8557 / 14700) loss: 0.629517\n",
      "(Iteration 8558 / 14700) loss: 0.926542\n",
      "(Iteration 8559 / 14700) loss: 0.969146\n",
      "(Iteration 8560 / 14700) loss: 0.611168\n",
      "(Iteration 8561 / 14700) loss: 0.653822\n",
      "(Iteration 8562 / 14700) loss: 0.674465\n",
      "(Iteration 8563 / 14700) loss: 0.752615\n",
      "(Iteration 8564 / 14700) loss: 0.882584\n",
      "(Iteration 8565 / 14700) loss: 0.589145\n",
      "(Iteration 8566 / 14700) loss: 0.654123\n",
      "(Iteration 8567 / 14700) loss: 0.592420\n",
      "(Iteration 8568 / 14700) loss: 0.779626\n",
      "(Iteration 8569 / 14700) loss: 0.626259\n",
      "(Iteration 8570 / 14700) loss: 0.971382\n",
      "(Iteration 8571 / 14700) loss: 0.761245\n",
      "(Iteration 8572 / 14700) loss: 0.593664\n",
      "(Iteration 8573 / 14700) loss: 0.652026\n",
      "(Iteration 8574 / 14700) loss: 1.077179\n",
      "(Iteration 8575 / 14700) loss: 0.831706\n",
      "(Iteration 8576 / 14700) loss: 0.830777\n",
      "(Iteration 8577 / 14700) loss: 1.107990\n",
      "(Iteration 8578 / 14700) loss: 0.786104\n",
      "(Iteration 8579 / 14700) loss: 0.708447\n",
      "(Iteration 8580 / 14700) loss: 0.675521\n",
      "(Iteration 8581 / 14700) loss: 0.561272\n",
      "(Iteration 8582 / 14700) loss: 0.712674\n",
      "(Iteration 8583 / 14700) loss: 0.711171\n",
      "(Iteration 8584 / 14700) loss: 0.895805\n",
      "(Iteration 8585 / 14700) loss: 0.882036\n",
      "(Iteration 8586 / 14700) loss: 0.751562\n",
      "(Iteration 8587 / 14700) loss: 0.817240\n",
      "(Iteration 8588 / 14700) loss: 0.658682\n",
      "(Iteration 8589 / 14700) loss: 0.896283\n",
      "(Iteration 8590 / 14700) loss: 0.637208\n",
      "(Iteration 8591 / 14700) loss: 0.771047\n",
      "(Iteration 8592 / 14700) loss: 0.808665\n",
      "(Iteration 8593 / 14700) loss: 0.896700\n",
      "(Iteration 8594 / 14700) loss: 0.844163\n",
      "(Iteration 8595 / 14700) loss: 0.698178\n",
      "(Iteration 8596 / 14700) loss: 0.844193\n",
      "(Iteration 8597 / 14700) loss: 0.810354\n",
      "(Iteration 8598 / 14700) loss: 0.848759\n",
      "(Iteration 8599 / 14700) loss: 0.673854\n",
      "(Iteration 8600 / 14700) loss: 1.051834\n",
      "(Iteration 8601 / 14700) loss: 1.080222\n",
      "(Iteration 8602 / 14700) loss: 0.809429\n",
      "(Iteration 8603 / 14700) loss: 0.810942\n",
      "(Iteration 8604 / 14700) loss: 1.062230\n",
      "(Iteration 8605 / 14700) loss: 0.800151\n",
      "(Iteration 8606 / 14700) loss: 0.885005\n",
      "(Iteration 8607 / 14700) loss: 0.842474\n",
      "(Iteration 8608 / 14700) loss: 0.889502\n",
      "(Iteration 8609 / 14700) loss: 0.933954\n",
      "(Iteration 8610 / 14700) loss: 0.600454\n",
      "(Iteration 8611 / 14700) loss: 0.718916\n",
      "(Iteration 8612 / 14700) loss: 0.656994\n",
      "(Iteration 8613 / 14700) loss: 0.842080\n",
      "(Iteration 8614 / 14700) loss: 0.905578\n",
      "(Iteration 8615 / 14700) loss: 0.644033\n",
      "(Iteration 8616 / 14700) loss: 0.808823\n",
      "(Iteration 8617 / 14700) loss: 0.805647\n",
      "(Iteration 8618 / 14700) loss: 0.738438\n",
      "(Iteration 8619 / 14700) loss: 0.663954\n",
      "(Iteration 8620 / 14700) loss: 0.803193\n",
      "(Iteration 8621 / 14700) loss: 0.642608\n",
      "(Iteration 8622 / 14700) loss: 0.779257\n",
      "(Iteration 8623 / 14700) loss: 1.000796\n",
      "(Iteration 8624 / 14700) loss: 0.648566\n",
      "(Iteration 8625 / 14700) loss: 0.678092\n",
      "(Iteration 8626 / 14700) loss: 0.875319\n",
      "(Iteration 8627 / 14700) loss: 0.614034\n",
      "(Iteration 8628 / 14700) loss: 0.750253\n",
      "(Iteration 8629 / 14700) loss: 0.611321\n",
      "(Iteration 8630 / 14700) loss: 0.861594\n",
      "(Iteration 8631 / 14700) loss: 0.597738\n",
      "(Iteration 8632 / 14700) loss: 1.176269\n",
      "(Iteration 8633 / 14700) loss: 0.701094\n",
      "(Iteration 8634 / 14700) loss: 0.799368\n",
      "(Iteration 8635 / 14700) loss: 0.870628\n",
      "(Iteration 8636 / 14700) loss: 0.715187\n",
      "(Iteration 8637 / 14700) loss: 0.580752\n",
      "(Iteration 8638 / 14700) loss: 0.665524\n",
      "(Iteration 8639 / 14700) loss: 0.775123\n",
      "(Iteration 8640 / 14700) loss: 0.914848\n",
      "(Iteration 8641 / 14700) loss: 1.085615\n",
      "(Iteration 8642 / 14700) loss: 0.812932\n",
      "(Iteration 8643 / 14700) loss: 0.704097\n",
      "(Iteration 8644 / 14700) loss: 0.688150\n",
      "(Iteration 8645 / 14700) loss: 0.802510\n",
      "(Iteration 8646 / 14700) loss: 0.761771\n",
      "(Iteration 8647 / 14700) loss: 0.708160\n",
      "(Iteration 8648 / 14700) loss: 0.773273\n",
      "(Iteration 8649 / 14700) loss: 0.839325\n",
      "(Iteration 8650 / 14700) loss: 0.683255\n",
      "(Iteration 8651 / 14700) loss: 0.541265\n",
      "(Iteration 8652 / 14700) loss: 0.937709\n",
      "(Iteration 8653 / 14700) loss: 0.859239\n",
      "(Iteration 8654 / 14700) loss: 0.791197\n",
      "(Iteration 8655 / 14700) loss: 0.637527\n",
      "(Iteration 8656 / 14700) loss: 0.701084\n",
      "(Iteration 8657 / 14700) loss: 0.870655\n",
      "(Iteration 8658 / 14700) loss: 0.915425\n",
      "(Iteration 8659 / 14700) loss: 0.619159\n",
      "(Iteration 8660 / 14700) loss: 0.543641\n",
      "(Iteration 8661 / 14700) loss: 1.031850\n",
      "(Iteration 8662 / 14700) loss: 0.657845\n",
      "(Iteration 8663 / 14700) loss: 0.808547\n",
      "(Iteration 8664 / 14700) loss: 0.605341\n",
      "(Iteration 8665 / 14700) loss: 0.730567\n",
      "(Iteration 8666 / 14700) loss: 0.926195\n",
      "(Iteration 8667 / 14700) loss: 0.561306\n",
      "(Iteration 8668 / 14700) loss: 0.747149\n",
      "(Iteration 8669 / 14700) loss: 0.608985\n",
      "(Iteration 8670 / 14700) loss: 0.736442\n",
      "(Iteration 8671 / 14700) loss: 0.666101\n",
      "(Iteration 8672 / 14700) loss: 1.038559\n",
      "(Iteration 8673 / 14700) loss: 0.778544\n",
      "(Iteration 8674 / 14700) loss: 0.785858\n",
      "(Iteration 8675 / 14700) loss: 0.922778\n",
      "(Iteration 8676 / 14700) loss: 0.776160\n",
      "(Iteration 8677 / 14700) loss: 0.594939\n",
      "(Iteration 8678 / 14700) loss: 0.704072\n",
      "(Iteration 8679 / 14700) loss: 0.671071\n",
      "(Iteration 8680 / 14700) loss: 0.649813\n",
      "(Iteration 8681 / 14700) loss: 0.856498\n",
      "(Iteration 8682 / 14700) loss: 0.874267\n",
      "(Iteration 8683 / 14700) loss: 0.805844\n",
      "(Iteration 8684 / 14700) loss: 0.816150\n",
      "(Iteration 8685 / 14700) loss: 0.685505\n",
      "(Iteration 8686 / 14700) loss: 1.052776\n",
      "(Iteration 8687 / 14700) loss: 0.612892\n",
      "(Iteration 8688 / 14700) loss: 0.898905\n",
      "(Iteration 8689 / 14700) loss: 0.914916\n",
      "(Iteration 8690 / 14700) loss: 0.742011\n",
      "(Iteration 8691 / 14700) loss: 0.568326\n",
      "(Iteration 8692 / 14700) loss: 0.610956\n",
      "(Iteration 8693 / 14700) loss: 0.453966\n",
      "(Iteration 8694 / 14700) loss: 0.712072\n",
      "(Iteration 8695 / 14700) loss: 0.705614\n",
      "(Iteration 8696 / 14700) loss: 0.721158\n",
      "(Iteration 8697 / 14700) loss: 0.729570\n",
      "(Iteration 8698 / 14700) loss: 0.717569\n",
      "(Iteration 8699 / 14700) loss: 0.855057\n",
      "(Iteration 8700 / 14700) loss: 0.509010\n",
      "(Iteration 8701 / 14700) loss: 0.679047\n",
      "(Iteration 8702 / 14700) loss: 0.901979\n",
      "(Iteration 8703 / 14700) loss: 0.595188\n",
      "(Iteration 8704 / 14700) loss: 0.585844\n",
      "(Iteration 8705 / 14700) loss: 0.668227\n",
      "(Iteration 8706 / 14700) loss: 0.848448\n",
      "(Iteration 8707 / 14700) loss: 0.627827\n",
      "(Iteration 8708 / 14700) loss: 0.609339\n",
      "(Iteration 8709 / 14700) loss: 0.648614\n",
      "(Iteration 8710 / 14700) loss: 0.827555\n",
      "(Iteration 8711 / 14700) loss: 0.800416\n",
      "(Iteration 8712 / 14700) loss: 0.874008\n",
      "(Iteration 8713 / 14700) loss: 0.674577\n",
      "(Iteration 8714 / 14700) loss: 0.544612\n",
      "(Iteration 8715 / 14700) loss: 0.918879\n",
      "(Iteration 8716 / 14700) loss: 0.807772\n",
      "(Iteration 8717 / 14700) loss: 1.029817\n",
      "(Iteration 8718 / 14700) loss: 0.862247\n",
      "(Iteration 8719 / 14700) loss: 0.878902\n",
      "(Iteration 8720 / 14700) loss: 0.637658\n",
      "(Iteration 8721 / 14700) loss: 1.216777\n",
      "(Iteration 8722 / 14700) loss: 0.796676\n",
      "(Iteration 8723 / 14700) loss: 0.972866\n",
      "(Iteration 8724 / 14700) loss: 0.811985\n",
      "(Iteration 8725 / 14700) loss: 0.899925\n",
      "(Iteration 8726 / 14700) loss: 0.770535\n",
      "(Iteration 8727 / 14700) loss: 0.840077\n",
      "(Iteration 8728 / 14700) loss: 0.885369\n",
      "(Iteration 8729 / 14700) loss: 0.786019\n",
      "(Iteration 8730 / 14700) loss: 0.673269\n",
      "(Iteration 8731 / 14700) loss: 0.883996\n",
      "(Iteration 8732 / 14700) loss: 0.643606\n",
      "(Iteration 8733 / 14700) loss: 0.884341\n",
      "(Iteration 8734 / 14700) loss: 0.942528\n",
      "(Iteration 8735 / 14700) loss: 0.625165\n",
      "(Iteration 8736 / 14700) loss: 0.562792\n",
      "(Iteration 8737 / 14700) loss: 0.738585\n",
      "(Iteration 8738 / 14700) loss: 1.121505\n",
      "(Iteration 8739 / 14700) loss: 0.886002\n",
      "(Iteration 8740 / 14700) loss: 0.723702\n",
      "(Iteration 8741 / 14700) loss: 0.775890\n",
      "(Iteration 8742 / 14700) loss: 1.021759\n",
      "(Iteration 8743 / 14700) loss: 0.730546\n",
      "(Iteration 8744 / 14700) loss: 0.783384\n",
      "(Iteration 8745 / 14700) loss: 0.666816\n",
      "(Iteration 8746 / 14700) loss: 0.695738\n",
      "(Iteration 8747 / 14700) loss: 0.770203\n",
      "(Iteration 8748 / 14700) loss: 1.023400\n",
      "(Iteration 8749 / 14700) loss: 0.559740\n",
      "(Iteration 8750 / 14700) loss: 0.805598\n",
      "(Iteration 8751 / 14700) loss: 0.897972\n",
      "(Iteration 8752 / 14700) loss: 0.691031\n",
      "(Iteration 8753 / 14700) loss: 0.751188\n",
      "(Iteration 8754 / 14700) loss: 0.758744\n",
      "(Iteration 8755 / 14700) loss: 0.592878\n",
      "(Iteration 8756 / 14700) loss: 0.525412\n",
      "(Iteration 8757 / 14700) loss: 0.738725\n",
      "(Iteration 8758 / 14700) loss: 0.846038\n",
      "(Iteration 8759 / 14700) loss: 0.732819\n",
      "(Iteration 8760 / 14700) loss: 0.868267\n",
      "(Iteration 8761 / 14700) loss: 0.638627\n",
      "(Iteration 8762 / 14700) loss: 0.643979\n",
      "(Iteration 8763 / 14700) loss: 1.045134\n",
      "(Iteration 8764 / 14700) loss: 0.752868\n",
      "(Iteration 8765 / 14700) loss: 1.068254\n",
      "(Iteration 8766 / 14700) loss: 0.850807\n",
      "(Iteration 8767 / 14700) loss: 0.912448\n",
      "(Iteration 8768 / 14700) loss: 0.797834\n",
      "(Iteration 8769 / 14700) loss: 0.724908\n",
      "(Iteration 8770 / 14700) loss: 0.879982\n",
      "(Iteration 8771 / 14700) loss: 0.795416\n",
      "(Iteration 8772 / 14700) loss: 0.756003\n",
      "(Iteration 8773 / 14700) loss: 0.860625\n",
      "(Iteration 8774 / 14700) loss: 0.722576\n",
      "(Iteration 8775 / 14700) loss: 0.943447\n",
      "(Iteration 8776 / 14700) loss: 0.837995\n",
      "(Iteration 8777 / 14700) loss: 0.874025\n",
      "(Iteration 8778 / 14700) loss: 0.755120\n",
      "(Iteration 8779 / 14700) loss: 0.979439\n",
      "(Iteration 8780 / 14700) loss: 0.801534\n",
      "(Iteration 8781 / 14700) loss: 0.571179\n",
      "(Iteration 8782 / 14700) loss: 0.839745\n",
      "(Iteration 8783 / 14700) loss: 0.897609\n",
      "(Iteration 8784 / 14700) loss: 0.917339\n",
      "(Iteration 8785 / 14700) loss: 0.803699\n",
      "(Iteration 8786 / 14700) loss: 0.768860\n",
      "(Iteration 8787 / 14700) loss: 0.802964\n",
      "(Iteration 8788 / 14700) loss: 0.530044\n",
      "(Iteration 8789 / 14700) loss: 0.462207\n",
      "(Iteration 8790 / 14700) loss: 0.759597\n",
      "(Iteration 8791 / 14700) loss: 0.814204\n",
      "(Iteration 8792 / 14700) loss: 0.646153\n",
      "(Iteration 8793 / 14700) loss: 0.797639\n",
      "(Iteration 8794 / 14700) loss: 0.735019\n",
      "(Iteration 8795 / 14700) loss: 0.663192\n",
      "(Iteration 8796 / 14700) loss: 0.498628\n",
      "(Iteration 8797 / 14700) loss: 0.844185\n",
      "(Iteration 8798 / 14700) loss: 0.683032\n",
      "(Iteration 8799 / 14700) loss: 0.852229\n",
      "(Iteration 8800 / 14700) loss: 0.557020\n",
      "(Iteration 8801 / 14700) loss: 0.640170\n",
      "(Iteration 8802 / 14700) loss: 0.761840\n",
      "(Iteration 8803 / 14700) loss: 0.683145\n",
      "(Iteration 8804 / 14700) loss: 0.915032\n",
      "(Iteration 8805 / 14700) loss: 0.820407\n",
      "(Iteration 8806 / 14700) loss: 0.740220\n",
      "(Iteration 8807 / 14700) loss: 1.052744\n",
      "(Iteration 8808 / 14700) loss: 0.886828\n",
      "(Iteration 8809 / 14700) loss: 1.240663\n",
      "(Iteration 8810 / 14700) loss: 0.887274\n",
      "(Iteration 8811 / 14700) loss: 0.974160\n",
      "(Iteration 8812 / 14700) loss: 0.521813\n",
      "(Iteration 8813 / 14700) loss: 0.658596\n",
      "(Iteration 8814 / 14700) loss: 0.683573\n",
      "(Iteration 8815 / 14700) loss: 0.700292\n",
      "(Iteration 8816 / 14700) loss: 0.637098\n",
      "(Iteration 8817 / 14700) loss: 0.737946\n",
      "(Iteration 8818 / 14700) loss: 0.715856\n",
      "(Iteration 8819 / 14700) loss: 1.131631\n",
      "(Iteration 8820 / 14700) loss: 0.711922\n",
      "(Epoch 9 / 15) train acc: 0.805000; val_acc: 0.741000\n",
      "(Iteration 8821 / 14700) loss: 0.708353\n",
      "(Iteration 8822 / 14700) loss: 0.882914\n",
      "(Iteration 8823 / 14700) loss: 0.639767\n",
      "(Iteration 8824 / 14700) loss: 0.619585\n",
      "(Iteration 8825 / 14700) loss: 0.955125\n",
      "(Iteration 8826 / 14700) loss: 0.818911\n",
      "(Iteration 8827 / 14700) loss: 0.645418\n",
      "(Iteration 8828 / 14700) loss: 0.772837\n",
      "(Iteration 8829 / 14700) loss: 0.924784\n",
      "(Iteration 8830 / 14700) loss: 0.887740\n",
      "(Iteration 8831 / 14700) loss: 0.577799\n",
      "(Iteration 8832 / 14700) loss: 1.005890\n",
      "(Iteration 8833 / 14700) loss: 0.836501\n",
      "(Iteration 8834 / 14700) loss: 0.950714\n",
      "(Iteration 8835 / 14700) loss: 0.749432\n",
      "(Iteration 8836 / 14700) loss: 0.815025\n",
      "(Iteration 8837 / 14700) loss: 0.777106\n",
      "(Iteration 8838 / 14700) loss: 0.783102\n",
      "(Iteration 8839 / 14700) loss: 0.454834\n",
      "(Iteration 8840 / 14700) loss: 0.810968\n",
      "(Iteration 8841 / 14700) loss: 0.595250\n",
      "(Iteration 8842 / 14700) loss: 0.734518\n",
      "(Iteration 8843 / 14700) loss: 0.967946\n",
      "(Iteration 8844 / 14700) loss: 0.700124\n",
      "(Iteration 8845 / 14700) loss: 0.669253\n",
      "(Iteration 8846 / 14700) loss: 1.021876\n",
      "(Iteration 8847 / 14700) loss: 0.929559\n",
      "(Iteration 8848 / 14700) loss: 0.697922\n",
      "(Iteration 8849 / 14700) loss: 0.530115\n",
      "(Iteration 8850 / 14700) loss: 0.835722\n",
      "(Iteration 8851 / 14700) loss: 0.779582\n",
      "(Iteration 8852 / 14700) loss: 0.656601\n",
      "(Iteration 8853 / 14700) loss: 0.836281\n",
      "(Iteration 8854 / 14700) loss: 0.612730\n",
      "(Iteration 8855 / 14700) loss: 0.780727\n",
      "(Iteration 8856 / 14700) loss: 0.947628\n",
      "(Iteration 8857 / 14700) loss: 0.833685\n",
      "(Iteration 8858 / 14700) loss: 0.854835\n",
      "(Iteration 8859 / 14700) loss: 0.912938\n",
      "(Iteration 8860 / 14700) loss: 0.687700\n",
      "(Iteration 8861 / 14700) loss: 0.703423\n",
      "(Iteration 8862 / 14700) loss: 0.730507\n",
      "(Iteration 8863 / 14700) loss: 0.777129\n",
      "(Iteration 8864 / 14700) loss: 0.924540\n",
      "(Iteration 8865 / 14700) loss: 0.703912\n",
      "(Iteration 8866 / 14700) loss: 0.998839\n",
      "(Iteration 8867 / 14700) loss: 0.609773\n",
      "(Iteration 8868 / 14700) loss: 0.858982\n",
      "(Iteration 8869 / 14700) loss: 0.958556\n",
      "(Iteration 8870 / 14700) loss: 0.794856\n",
      "(Iteration 8871 / 14700) loss: 0.855857\n",
      "(Iteration 8872 / 14700) loss: 0.940433\n",
      "(Iteration 8873 / 14700) loss: 0.657124\n",
      "(Iteration 8874 / 14700) loss: 0.701250\n",
      "(Iteration 8875 / 14700) loss: 0.846322\n",
      "(Iteration 8876 / 14700) loss: 0.970691\n",
      "(Iteration 8877 / 14700) loss: 0.816993\n",
      "(Iteration 8878 / 14700) loss: 0.799431\n",
      "(Iteration 8879 / 14700) loss: 0.621779\n",
      "(Iteration 8880 / 14700) loss: 1.187084\n",
      "(Iteration 8881 / 14700) loss: 0.693599\n",
      "(Iteration 8882 / 14700) loss: 0.878550\n",
      "(Iteration 8883 / 14700) loss: 0.753162\n",
      "(Iteration 8884 / 14700) loss: 0.922826\n",
      "(Iteration 8885 / 14700) loss: 0.702074\n",
      "(Iteration 8886 / 14700) loss: 0.790570\n",
      "(Iteration 8887 / 14700) loss: 0.827920\n",
      "(Iteration 8888 / 14700) loss: 1.276799\n",
      "(Iteration 8889 / 14700) loss: 0.878098\n",
      "(Iteration 8890 / 14700) loss: 0.679668\n",
      "(Iteration 8891 / 14700) loss: 0.806184\n",
      "(Iteration 8892 / 14700) loss: 0.503564\n",
      "(Iteration 8893 / 14700) loss: 0.995620\n",
      "(Iteration 8894 / 14700) loss: 0.622972\n",
      "(Iteration 8895 / 14700) loss: 0.722447\n",
      "(Iteration 8896 / 14700) loss: 0.493302\n",
      "(Iteration 8897 / 14700) loss: 0.585137\n",
      "(Iteration 8898 / 14700) loss: 0.921046\n",
      "(Iteration 8899 / 14700) loss: 0.593085\n",
      "(Iteration 8900 / 14700) loss: 0.825750\n",
      "(Iteration 8901 / 14700) loss: 0.676695\n",
      "(Iteration 8902 / 14700) loss: 0.841510\n",
      "(Iteration 8903 / 14700) loss: 0.788852\n",
      "(Iteration 8904 / 14700) loss: 1.117986\n",
      "(Iteration 8905 / 14700) loss: 0.642331\n",
      "(Iteration 8906 / 14700) loss: 0.842323\n",
      "(Iteration 8907 / 14700) loss: 0.721039\n",
      "(Iteration 8908 / 14700) loss: 0.717695\n",
      "(Iteration 8909 / 14700) loss: 0.845653\n",
      "(Iteration 8910 / 14700) loss: 0.875132\n",
      "(Iteration 8911 / 14700) loss: 0.921576\n",
      "(Iteration 8912 / 14700) loss: 0.897135\n",
      "(Iteration 8913 / 14700) loss: 0.889603\n",
      "(Iteration 8914 / 14700) loss: 0.783388\n",
      "(Iteration 8915 / 14700) loss: 0.632917\n",
      "(Iteration 8916 / 14700) loss: 0.860012\n",
      "(Iteration 8917 / 14700) loss: 0.818404\n",
      "(Iteration 8918 / 14700) loss: 0.896252\n",
      "(Iteration 8919 / 14700) loss: 0.893851\n",
      "(Iteration 8920 / 14700) loss: 0.804676\n",
      "(Iteration 8921 / 14700) loss: 0.766959\n",
      "(Iteration 8922 / 14700) loss: 0.772038\n",
      "(Iteration 8923 / 14700) loss: 1.009524\n",
      "(Iteration 8924 / 14700) loss: 0.872560\n",
      "(Iteration 8925 / 14700) loss: 0.560876\n",
      "(Iteration 8926 / 14700) loss: 0.768581\n",
      "(Iteration 8927 / 14700) loss: 0.874160\n",
      "(Iteration 8928 / 14700) loss: 0.640694\n",
      "(Iteration 8929 / 14700) loss: 0.536920\n",
      "(Iteration 8930 / 14700) loss: 0.834877\n",
      "(Iteration 8931 / 14700) loss: 0.698745\n",
      "(Iteration 8932 / 14700) loss: 0.847133\n",
      "(Iteration 8933 / 14700) loss: 1.044625\n",
      "(Iteration 8934 / 14700) loss: 0.917980\n",
      "(Iteration 8935 / 14700) loss: 0.672705\n",
      "(Iteration 8936 / 14700) loss: 0.793516\n",
      "(Iteration 8937 / 14700) loss: 0.657571\n",
      "(Iteration 8938 / 14700) loss: 0.561723\n",
      "(Iteration 8939 / 14700) loss: 0.771597\n",
      "(Iteration 8940 / 14700) loss: 0.636802\n",
      "(Iteration 8941 / 14700) loss: 0.659079\n",
      "(Iteration 8942 / 14700) loss: 0.778179\n",
      "(Iteration 8943 / 14700) loss: 0.727907\n",
      "(Iteration 8944 / 14700) loss: 0.582206\n",
      "(Iteration 8945 / 14700) loss: 0.524443\n",
      "(Iteration 8946 / 14700) loss: 0.845121\n",
      "(Iteration 8947 / 14700) loss: 0.764961\n",
      "(Iteration 8948 / 14700) loss: 0.670664\n",
      "(Iteration 8949 / 14700) loss: 0.832313\n",
      "(Iteration 8950 / 14700) loss: 0.737717\n",
      "(Iteration 8951 / 14700) loss: 0.788362\n",
      "(Iteration 8952 / 14700) loss: 0.743159\n",
      "(Iteration 8953 / 14700) loss: 0.544150\n",
      "(Iteration 8954 / 14700) loss: 0.872565\n",
      "(Iteration 8955 / 14700) loss: 0.759720\n",
      "(Iteration 8956 / 14700) loss: 0.831242\n",
      "(Iteration 8957 / 14700) loss: 0.815262\n",
      "(Iteration 8958 / 14700) loss: 0.593533\n",
      "(Iteration 8959 / 14700) loss: 0.848098\n",
      "(Iteration 8960 / 14700) loss: 0.669126\n",
      "(Iteration 8961 / 14700) loss: 0.766939\n",
      "(Iteration 8962 / 14700) loss: 0.621000\n",
      "(Iteration 8963 / 14700) loss: 0.603730\n",
      "(Iteration 8964 / 14700) loss: 0.576607\n",
      "(Iteration 8965 / 14700) loss: 0.614945\n",
      "(Iteration 8966 / 14700) loss: 0.780861\n",
      "(Iteration 8967 / 14700) loss: 0.808734\n",
      "(Iteration 8968 / 14700) loss: 0.668000\n",
      "(Iteration 8969 / 14700) loss: 0.735302\n",
      "(Iteration 8970 / 14700) loss: 0.657172\n",
      "(Iteration 8971 / 14700) loss: 0.837513\n",
      "(Iteration 8972 / 14700) loss: 0.728415\n",
      "(Iteration 8973 / 14700) loss: 0.804809\n",
      "(Iteration 8974 / 14700) loss: 0.641576\n",
      "(Iteration 8975 / 14700) loss: 0.692900\n",
      "(Iteration 8976 / 14700) loss: 0.754989\n",
      "(Iteration 8977 / 14700) loss: 0.626474\n",
      "(Iteration 8978 / 14700) loss: 0.551387\n",
      "(Iteration 8979 / 14700) loss: 0.813447\n",
      "(Iteration 8980 / 14700) loss: 0.959753\n",
      "(Iteration 8981 / 14700) loss: 0.933260\n",
      "(Iteration 8982 / 14700) loss: 0.986261\n",
      "(Iteration 8983 / 14700) loss: 0.605992\n",
      "(Iteration 8984 / 14700) loss: 1.095025\n",
      "(Iteration 8985 / 14700) loss: 0.705956\n",
      "(Iteration 8986 / 14700) loss: 0.982761\n",
      "(Iteration 8987 / 14700) loss: 0.777601\n",
      "(Iteration 8988 / 14700) loss: 0.820619\n",
      "(Iteration 8989 / 14700) loss: 0.647900\n",
      "(Iteration 8990 / 14700) loss: 0.846977\n",
      "(Iteration 8991 / 14700) loss: 0.861490\n",
      "(Iteration 8992 / 14700) loss: 0.659300\n",
      "(Iteration 8993 / 14700) loss: 0.906265\n",
      "(Iteration 8994 / 14700) loss: 0.737644\n",
      "(Iteration 8995 / 14700) loss: 0.802992\n",
      "(Iteration 8996 / 14700) loss: 0.949877\n",
      "(Iteration 8997 / 14700) loss: 0.658148\n",
      "(Iteration 8998 / 14700) loss: 0.676490\n",
      "(Iteration 8999 / 14700) loss: 0.912318\n",
      "(Iteration 9000 / 14700) loss: 0.697234\n",
      "(Iteration 9001 / 14700) loss: 0.502532\n",
      "(Iteration 9002 / 14700) loss: 0.869813\n",
      "(Iteration 9003 / 14700) loss: 0.662061\n",
      "(Iteration 9004 / 14700) loss: 0.760388\n",
      "(Iteration 9005 / 14700) loss: 0.607702\n",
      "(Iteration 9006 / 14700) loss: 1.449722\n",
      "(Iteration 9007 / 14700) loss: 0.600657\n",
      "(Iteration 9008 / 14700) loss: 0.767631\n",
      "(Iteration 9009 / 14700) loss: 0.757417\n",
      "(Iteration 9010 / 14700) loss: 0.560235\n",
      "(Iteration 9011 / 14700) loss: 0.867829\n",
      "(Iteration 9012 / 14700) loss: 0.582848\n",
      "(Iteration 9013 / 14700) loss: 0.609786\n",
      "(Iteration 9014 / 14700) loss: 0.543063\n",
      "(Iteration 9015 / 14700) loss: 0.704854\n",
      "(Iteration 9016 / 14700) loss: 1.052125\n",
      "(Iteration 9017 / 14700) loss: 0.737047\n",
      "(Iteration 9018 / 14700) loss: 0.734501\n",
      "(Iteration 9019 / 14700) loss: 0.911916\n",
      "(Iteration 9020 / 14700) loss: 0.789524\n",
      "(Iteration 9021 / 14700) loss: 0.947643\n",
      "(Iteration 9022 / 14700) loss: 0.720139\n",
      "(Iteration 9023 / 14700) loss: 0.617080\n",
      "(Iteration 9024 / 14700) loss: 0.908536\n",
      "(Iteration 9025 / 14700) loss: 0.864192\n",
      "(Iteration 9026 / 14700) loss: 0.751369\n",
      "(Iteration 9027 / 14700) loss: 0.797509\n",
      "(Iteration 9028 / 14700) loss: 0.894053\n",
      "(Iteration 9029 / 14700) loss: 0.648891\n",
      "(Iteration 9030 / 14700) loss: 0.683978\n",
      "(Iteration 9031 / 14700) loss: 0.821901\n",
      "(Iteration 9032 / 14700) loss: 0.658995\n",
      "(Iteration 9033 / 14700) loss: 0.905519\n",
      "(Iteration 9034 / 14700) loss: 0.963191\n",
      "(Iteration 9035 / 14700) loss: 0.644673\n",
      "(Iteration 9036 / 14700) loss: 1.065405\n",
      "(Iteration 9037 / 14700) loss: 0.644154\n",
      "(Iteration 9038 / 14700) loss: 0.813282\n",
      "(Iteration 9039 / 14700) loss: 0.505706\n",
      "(Iteration 9040 / 14700) loss: 0.935287\n",
      "(Iteration 9041 / 14700) loss: 0.859506\n",
      "(Iteration 9042 / 14700) loss: 0.708860\n",
      "(Iteration 9043 / 14700) loss: 0.564386\n",
      "(Iteration 9044 / 14700) loss: 0.969346\n",
      "(Iteration 9045 / 14700) loss: 1.031967\n",
      "(Iteration 9046 / 14700) loss: 0.880161\n",
      "(Iteration 9047 / 14700) loss: 0.836371\n",
      "(Iteration 9048 / 14700) loss: 0.663117\n",
      "(Iteration 9049 / 14700) loss: 0.853444\n",
      "(Iteration 9050 / 14700) loss: 0.969317\n",
      "(Iteration 9051 / 14700) loss: 0.718723\n",
      "(Iteration 9052 / 14700) loss: 0.878006\n",
      "(Iteration 9053 / 14700) loss: 0.708140\n",
      "(Iteration 9054 / 14700) loss: 0.607629\n",
      "(Iteration 9055 / 14700) loss: 0.596050\n",
      "(Iteration 9056 / 14700) loss: 0.886936\n",
      "(Iteration 9057 / 14700) loss: 0.668191\n",
      "(Iteration 9058 / 14700) loss: 0.827118\n",
      "(Iteration 9059 / 14700) loss: 0.646703\n",
      "(Iteration 9060 / 14700) loss: 0.672721\n",
      "(Iteration 9061 / 14700) loss: 0.947906\n",
      "(Iteration 9062 / 14700) loss: 0.635140\n",
      "(Iteration 9063 / 14700) loss: 0.742556\n",
      "(Iteration 9064 / 14700) loss: 0.790167\n",
      "(Iteration 9065 / 14700) loss: 0.784136\n",
      "(Iteration 9066 / 14700) loss: 0.894080\n",
      "(Iteration 9067 / 14700) loss: 0.643704\n",
      "(Iteration 9068 / 14700) loss: 0.693566\n",
      "(Iteration 9069 / 14700) loss: 0.701114\n",
      "(Iteration 9070 / 14700) loss: 0.726922\n",
      "(Iteration 9071 / 14700) loss: 0.662655\n",
      "(Iteration 9072 / 14700) loss: 0.683758\n",
      "(Iteration 9073 / 14700) loss: 0.913316\n",
      "(Iteration 9074 / 14700) loss: 0.764903\n",
      "(Iteration 9075 / 14700) loss: 0.562732\n",
      "(Iteration 9076 / 14700) loss: 0.756689\n",
      "(Iteration 9077 / 14700) loss: 0.918728\n",
      "(Iteration 9078 / 14700) loss: 0.716269\n",
      "(Iteration 9079 / 14700) loss: 0.518422\n",
      "(Iteration 9080 / 14700) loss: 0.821170\n",
      "(Iteration 9081 / 14700) loss: 0.810822\n",
      "(Iteration 9082 / 14700) loss: 0.902751\n",
      "(Iteration 9083 / 14700) loss: 1.052623\n",
      "(Iteration 9084 / 14700) loss: 0.685579\n",
      "(Iteration 9085 / 14700) loss: 0.709762\n",
      "(Iteration 9086 / 14700) loss: 0.746276\n",
      "(Iteration 9087 / 14700) loss: 0.617686\n",
      "(Iteration 9088 / 14700) loss: 0.754592\n",
      "(Iteration 9089 / 14700) loss: 0.706591\n",
      "(Iteration 9090 / 14700) loss: 0.791055\n",
      "(Iteration 9091 / 14700) loss: 0.534150\n",
      "(Iteration 9092 / 14700) loss: 0.628105\n",
      "(Iteration 9093 / 14700) loss: 0.808726\n",
      "(Iteration 9094 / 14700) loss: 0.775915\n",
      "(Iteration 9095 / 14700) loss: 0.997221\n",
      "(Iteration 9096 / 14700) loss: 1.053869\n",
      "(Iteration 9097 / 14700) loss: 0.757036\n",
      "(Iteration 9098 / 14700) loss: 0.746860\n",
      "(Iteration 9099 / 14700) loss: 0.867801\n",
      "(Iteration 9100 / 14700) loss: 0.835825\n",
      "(Iteration 9101 / 14700) loss: 0.737690\n",
      "(Iteration 9102 / 14700) loss: 0.698526\n",
      "(Iteration 9103 / 14700) loss: 0.650872\n",
      "(Iteration 9104 / 14700) loss: 0.567572\n",
      "(Iteration 9105 / 14700) loss: 0.642118\n",
      "(Iteration 9106 / 14700) loss: 0.660595\n",
      "(Iteration 9107 / 14700) loss: 0.821457\n",
      "(Iteration 9108 / 14700) loss: 0.713129\n",
      "(Iteration 9109 / 14700) loss: 1.199535\n",
      "(Iteration 9110 / 14700) loss: 0.618100\n",
      "(Iteration 9111 / 14700) loss: 0.814272\n",
      "(Iteration 9112 / 14700) loss: 0.715699\n",
      "(Iteration 9113 / 14700) loss: 0.902608\n",
      "(Iteration 9114 / 14700) loss: 0.657078\n",
      "(Iteration 9115 / 14700) loss: 0.658686\n",
      "(Iteration 9116 / 14700) loss: 0.711562\n",
      "(Iteration 9117 / 14700) loss: 0.841097\n",
      "(Iteration 9118 / 14700) loss: 0.725949\n",
      "(Iteration 9119 / 14700) loss: 0.749922\n",
      "(Iteration 9120 / 14700) loss: 0.854901\n",
      "(Iteration 9121 / 14700) loss: 0.621781\n",
      "(Iteration 9122 / 14700) loss: 0.729859\n",
      "(Iteration 9123 / 14700) loss: 0.721636\n",
      "(Iteration 9124 / 14700) loss: 0.634030\n",
      "(Iteration 9125 / 14700) loss: 0.678661\n",
      "(Iteration 9126 / 14700) loss: 0.624362\n",
      "(Iteration 9127 / 14700) loss: 0.874599\n",
      "(Iteration 9128 / 14700) loss: 0.915349\n",
      "(Iteration 9129 / 14700) loss: 0.804771\n",
      "(Iteration 9130 / 14700) loss: 0.688003\n",
      "(Iteration 9131 / 14700) loss: 0.654145\n",
      "(Iteration 9132 / 14700) loss: 0.985952\n",
      "(Iteration 9133 / 14700) loss: 0.911126\n",
      "(Iteration 9134 / 14700) loss: 0.683015\n",
      "(Iteration 9135 / 14700) loss: 0.895063\n",
      "(Iteration 9136 / 14700) loss: 0.734899\n",
      "(Iteration 9137 / 14700) loss: 1.027014\n",
      "(Iteration 9138 / 14700) loss: 0.717546\n",
      "(Iteration 9139 / 14700) loss: 0.586777\n",
      "(Iteration 9140 / 14700) loss: 0.896747\n",
      "(Iteration 9141 / 14700) loss: 0.690324\n",
      "(Iteration 9142 / 14700) loss: 0.656061\n",
      "(Iteration 9143 / 14700) loss: 0.690352\n",
      "(Iteration 9144 / 14700) loss: 0.706230\n",
      "(Iteration 9145 / 14700) loss: 0.778412\n",
      "(Iteration 9146 / 14700) loss: 0.612666\n",
      "(Iteration 9147 / 14700) loss: 0.680559\n",
      "(Iteration 9148 / 14700) loss: 0.610148\n",
      "(Iteration 9149 / 14700) loss: 0.560176\n",
      "(Iteration 9150 / 14700) loss: 0.753430\n",
      "(Iteration 9151 / 14700) loss: 0.574556\n",
      "(Iteration 9152 / 14700) loss: 0.807520\n",
      "(Iteration 9153 / 14700) loss: 0.598988\n",
      "(Iteration 9154 / 14700) loss: 0.712498\n",
      "(Iteration 9155 / 14700) loss: 0.841902\n",
      "(Iteration 9156 / 14700) loss: 0.492643\n",
      "(Iteration 9157 / 14700) loss: 0.717294\n",
      "(Iteration 9158 / 14700) loss: 0.654211\n",
      "(Iteration 9159 / 14700) loss: 0.689352\n",
      "(Iteration 9160 / 14700) loss: 0.897300\n",
      "(Iteration 9161 / 14700) loss: 0.725695\n",
      "(Iteration 9162 / 14700) loss: 0.883768\n",
      "(Iteration 9163 / 14700) loss: 0.514122\n",
      "(Iteration 9164 / 14700) loss: 0.949028\n",
      "(Iteration 9165 / 14700) loss: 0.568532\n",
      "(Iteration 9166 / 14700) loss: 0.868057\n",
      "(Iteration 9167 / 14700) loss: 0.923406\n",
      "(Iteration 9168 / 14700) loss: 0.655937\n",
      "(Iteration 9169 / 14700) loss: 0.659751\n",
      "(Iteration 9170 / 14700) loss: 0.835274\n",
      "(Iteration 9171 / 14700) loss: 0.800431\n",
      "(Iteration 9172 / 14700) loss: 0.836862\n",
      "(Iteration 9173 / 14700) loss: 0.688085\n",
      "(Iteration 9174 / 14700) loss: 0.825727\n",
      "(Iteration 9175 / 14700) loss: 0.798049\n",
      "(Iteration 9176 / 14700) loss: 0.645577\n",
      "(Iteration 9177 / 14700) loss: 0.659071\n",
      "(Iteration 9178 / 14700) loss: 0.820040\n",
      "(Iteration 9179 / 14700) loss: 0.908844\n",
      "(Iteration 9180 / 14700) loss: 0.635458\n",
      "(Iteration 9181 / 14700) loss: 0.838516\n",
      "(Iteration 9182 / 14700) loss: 0.697997\n",
      "(Iteration 9183 / 14700) loss: 0.822270\n",
      "(Iteration 9184 / 14700) loss: 0.823854\n",
      "(Iteration 9185 / 14700) loss: 0.624717\n",
      "(Iteration 9186 / 14700) loss: 0.778364\n",
      "(Iteration 9187 / 14700) loss: 0.667346\n",
      "(Iteration 9188 / 14700) loss: 1.086863\n",
      "(Iteration 9189 / 14700) loss: 0.818469\n",
      "(Iteration 9190 / 14700) loss: 0.653277\n",
      "(Iteration 9191 / 14700) loss: 0.665610\n",
      "(Iteration 9192 / 14700) loss: 0.768122\n",
      "(Iteration 9193 / 14700) loss: 0.836096\n",
      "(Iteration 9194 / 14700) loss: 0.799642\n",
      "(Iteration 9195 / 14700) loss: 1.076664\n",
      "(Iteration 9196 / 14700) loss: 0.622868\n",
      "(Iteration 9197 / 14700) loss: 0.886711\n",
      "(Iteration 9198 / 14700) loss: 0.847052\n",
      "(Iteration 9199 / 14700) loss: 0.790623\n",
      "(Iteration 9200 / 14700) loss: 0.744743\n",
      "(Iteration 9201 / 14700) loss: 0.678513\n",
      "(Iteration 9202 / 14700) loss: 0.946821\n",
      "(Iteration 9203 / 14700) loss: 0.775950\n",
      "(Iteration 9204 / 14700) loss: 0.722599\n",
      "(Iteration 9205 / 14700) loss: 0.679508\n",
      "(Iteration 9206 / 14700) loss: 0.633333\n",
      "(Iteration 9207 / 14700) loss: 0.818035\n",
      "(Iteration 9208 / 14700) loss: 0.749729\n",
      "(Iteration 9209 / 14700) loss: 0.781712\n",
      "(Iteration 9210 / 14700) loss: 0.648910\n",
      "(Iteration 9211 / 14700) loss: 0.707965\n",
      "(Iteration 9212 / 14700) loss: 0.566299\n",
      "(Iteration 9213 / 14700) loss: 0.726604\n",
      "(Iteration 9214 / 14700) loss: 0.509449\n",
      "(Iteration 9215 / 14700) loss: 1.033001\n",
      "(Iteration 9216 / 14700) loss: 0.707345\n",
      "(Iteration 9217 / 14700) loss: 0.662543\n",
      "(Iteration 9218 / 14700) loss: 0.725065\n",
      "(Iteration 9219 / 14700) loss: 1.111569\n",
      "(Iteration 9220 / 14700) loss: 0.861410\n",
      "(Iteration 9221 / 14700) loss: 0.883659\n",
      "(Iteration 9222 / 14700) loss: 0.702757\n",
      "(Iteration 9223 / 14700) loss: 0.623578\n",
      "(Iteration 9224 / 14700) loss: 0.835096\n",
      "(Iteration 9225 / 14700) loss: 0.547538\n",
      "(Iteration 9226 / 14700) loss: 0.775390\n",
      "(Iteration 9227 / 14700) loss: 0.912927\n",
      "(Iteration 9228 / 14700) loss: 0.599271\n",
      "(Iteration 9229 / 14700) loss: 0.677384\n",
      "(Iteration 9230 / 14700) loss: 0.649328\n",
      "(Iteration 9231 / 14700) loss: 0.946706\n",
      "(Iteration 9232 / 14700) loss: 0.674263\n",
      "(Iteration 9233 / 14700) loss: 0.736197\n",
      "(Iteration 9234 / 14700) loss: 0.975350\n",
      "(Iteration 9235 / 14700) loss: 0.863519\n",
      "(Iteration 9236 / 14700) loss: 0.724308\n",
      "(Iteration 9237 / 14700) loss: 0.735135\n",
      "(Iteration 9238 / 14700) loss: 0.550544\n",
      "(Iteration 9239 / 14700) loss: 0.670965\n",
      "(Iteration 9240 / 14700) loss: 0.590663\n",
      "(Iteration 9241 / 14700) loss: 0.610903\n",
      "(Iteration 9242 / 14700) loss: 0.724105\n",
      "(Iteration 9243 / 14700) loss: 0.953946\n",
      "(Iteration 9244 / 14700) loss: 0.829631\n",
      "(Iteration 9245 / 14700) loss: 0.653342\n",
      "(Iteration 9246 / 14700) loss: 0.718284\n",
      "(Iteration 9247 / 14700) loss: 0.694985\n",
      "(Iteration 9248 / 14700) loss: 0.552736\n",
      "(Iteration 9249 / 14700) loss: 0.951747\n",
      "(Iteration 9250 / 14700) loss: 1.017052\n",
      "(Iteration 9251 / 14700) loss: 0.656608\n",
      "(Iteration 9252 / 14700) loss: 1.021342\n",
      "(Iteration 9253 / 14700) loss: 0.905213\n",
      "(Iteration 9254 / 14700) loss: 0.716742\n",
      "(Iteration 9255 / 14700) loss: 0.702763\n",
      "(Iteration 9256 / 14700) loss: 1.008134\n",
      "(Iteration 9257 / 14700) loss: 0.639578\n",
      "(Iteration 9258 / 14700) loss: 0.961722\n",
      "(Iteration 9259 / 14700) loss: 1.206320\n",
      "(Iteration 9260 / 14700) loss: 0.601231\n",
      "(Iteration 9261 / 14700) loss: 0.585375\n",
      "(Iteration 9262 / 14700) loss: 0.647708\n",
      "(Iteration 9263 / 14700) loss: 0.643548\n",
      "(Iteration 9264 / 14700) loss: 0.831482\n",
      "(Iteration 9265 / 14700) loss: 0.704387\n",
      "(Iteration 9266 / 14700) loss: 0.746869\n",
      "(Iteration 9267 / 14700) loss: 0.721230\n",
      "(Iteration 9268 / 14700) loss: 0.778765\n",
      "(Iteration 9269 / 14700) loss: 0.766997\n",
      "(Iteration 9270 / 14700) loss: 0.748719\n",
      "(Iteration 9271 / 14700) loss: 0.554443\n",
      "(Iteration 9272 / 14700) loss: 0.778451\n",
      "(Iteration 9273 / 14700) loss: 0.764608\n",
      "(Iteration 9274 / 14700) loss: 0.726059\n",
      "(Iteration 9275 / 14700) loss: 0.704955\n",
      "(Iteration 9276 / 14700) loss: 0.494233\n",
      "(Iteration 9277 / 14700) loss: 0.843402\n",
      "(Iteration 9278 / 14700) loss: 0.679143\n",
      "(Iteration 9279 / 14700) loss: 0.697501\n",
      "(Iteration 9280 / 14700) loss: 0.604111\n",
      "(Iteration 9281 / 14700) loss: 0.793924\n",
      "(Iteration 9282 / 14700) loss: 0.719554\n",
      "(Iteration 9283 / 14700) loss: 0.646467\n",
      "(Iteration 9284 / 14700) loss: 0.850732\n",
      "(Iteration 9285 / 14700) loss: 0.879563\n",
      "(Iteration 9286 / 14700) loss: 0.763759\n",
      "(Iteration 9287 / 14700) loss: 0.583557\n",
      "(Iteration 9288 / 14700) loss: 0.629066\n",
      "(Iteration 9289 / 14700) loss: 0.813047\n",
      "(Iteration 9290 / 14700) loss: 0.700931\n",
      "(Iteration 9291 / 14700) loss: 0.877961\n",
      "(Iteration 9292 / 14700) loss: 0.735809\n",
      "(Iteration 9293 / 14700) loss: 0.716087\n",
      "(Iteration 9294 / 14700) loss: 0.689203\n",
      "(Iteration 9295 / 14700) loss: 0.875055\n",
      "(Iteration 9296 / 14700) loss: 0.720694\n",
      "(Iteration 9297 / 14700) loss: 0.807407\n",
      "(Iteration 9298 / 14700) loss: 0.819958\n",
      "(Iteration 9299 / 14700) loss: 0.786081\n",
      "(Iteration 9300 / 14700) loss: 0.517353\n",
      "(Iteration 9301 / 14700) loss: 0.750941\n",
      "(Iteration 9302 / 14700) loss: 0.801461\n",
      "(Iteration 9303 / 14700) loss: 1.090593\n",
      "(Iteration 9304 / 14700) loss: 0.708384\n",
      "(Iteration 9305 / 14700) loss: 0.939962\n",
      "(Iteration 9306 / 14700) loss: 0.571297\n",
      "(Iteration 9307 / 14700) loss: 0.618897\n",
      "(Iteration 9308 / 14700) loss: 1.036768\n",
      "(Iteration 9309 / 14700) loss: 0.580384\n",
      "(Iteration 9310 / 14700) loss: 0.780360\n",
      "(Iteration 9311 / 14700) loss: 0.882650\n",
      "(Iteration 9312 / 14700) loss: 0.744778\n",
      "(Iteration 9313 / 14700) loss: 0.687193\n",
      "(Iteration 9314 / 14700) loss: 0.739984\n",
      "(Iteration 9315 / 14700) loss: 0.712998\n",
      "(Iteration 9316 / 14700) loss: 0.502819\n",
      "(Iteration 9317 / 14700) loss: 0.592088\n",
      "(Iteration 9318 / 14700) loss: 0.892118\n",
      "(Iteration 9319 / 14700) loss: 0.954311\n",
      "(Iteration 9320 / 14700) loss: 0.572295\n",
      "(Iteration 9321 / 14700) loss: 0.602825\n",
      "(Iteration 9322 / 14700) loss: 0.794719\n",
      "(Iteration 9323 / 14700) loss: 0.714977\n",
      "(Iteration 9324 / 14700) loss: 0.743591\n",
      "(Iteration 9325 / 14700) loss: 0.658707\n",
      "(Iteration 9326 / 14700) loss: 0.681812\n",
      "(Iteration 9327 / 14700) loss: 0.718786\n",
      "(Iteration 9328 / 14700) loss: 0.509988\n",
      "(Iteration 9329 / 14700) loss: 0.759964\n",
      "(Iteration 9330 / 14700) loss: 0.702618\n",
      "(Iteration 9331 / 14700) loss: 0.653056\n",
      "(Iteration 9332 / 14700) loss: 0.770483\n",
      "(Iteration 9333 / 14700) loss: 0.884462\n",
      "(Iteration 9334 / 14700) loss: 0.995536\n",
      "(Iteration 9335 / 14700) loss: 1.094338\n",
      "(Iteration 9336 / 14700) loss: 0.655723\n",
      "(Iteration 9337 / 14700) loss: 0.758345\n",
      "(Iteration 9338 / 14700) loss: 0.554245\n",
      "(Iteration 9339 / 14700) loss: 0.665154\n",
      "(Iteration 9340 / 14700) loss: 0.799039\n",
      "(Iteration 9341 / 14700) loss: 0.779266\n",
      "(Iteration 9342 / 14700) loss: 1.163024\n",
      "(Iteration 9343 / 14700) loss: 0.677939\n",
      "(Iteration 9344 / 14700) loss: 0.640056\n",
      "(Iteration 9345 / 14700) loss: 0.775882\n",
      "(Iteration 9346 / 14700) loss: 0.805183\n",
      "(Iteration 9347 / 14700) loss: 0.764675\n",
      "(Iteration 9348 / 14700) loss: 0.628265\n",
      "(Iteration 9349 / 14700) loss: 1.140701\n",
      "(Iteration 9350 / 14700) loss: 0.825588\n",
      "(Iteration 9351 / 14700) loss: 0.866046\n",
      "(Iteration 9352 / 14700) loss: 1.077666\n",
      "(Iteration 9353 / 14700) loss: 0.778523\n",
      "(Iteration 9354 / 14700) loss: 0.617180\n",
      "(Iteration 9355 / 14700) loss: 0.816775\n",
      "(Iteration 9356 / 14700) loss: 0.655518\n",
      "(Iteration 9357 / 14700) loss: 0.613677\n",
      "(Iteration 9358 / 14700) loss: 0.646118\n",
      "(Iteration 9359 / 14700) loss: 0.797588\n",
      "(Iteration 9360 / 14700) loss: 0.932044\n",
      "(Iteration 9361 / 14700) loss: 0.688428\n",
      "(Iteration 9362 / 14700) loss: 0.713445\n",
      "(Iteration 9363 / 14700) loss: 0.884394\n",
      "(Iteration 9364 / 14700) loss: 0.906644\n",
      "(Iteration 9365 / 14700) loss: 0.773824\n",
      "(Iteration 9366 / 14700) loss: 0.686634\n",
      "(Iteration 9367 / 14700) loss: 1.006290\n",
      "(Iteration 9368 / 14700) loss: 0.701531\n",
      "(Iteration 9369 / 14700) loss: 0.870986\n",
      "(Iteration 9370 / 14700) loss: 0.654673\n",
      "(Iteration 9371 / 14700) loss: 0.513305\n",
      "(Iteration 9372 / 14700) loss: 0.666980\n",
      "(Iteration 9373 / 14700) loss: 0.648817\n",
      "(Iteration 9374 / 14700) loss: 0.764931\n",
      "(Iteration 9375 / 14700) loss: 0.647644\n",
      "(Iteration 9376 / 14700) loss: 0.897153\n",
      "(Iteration 9377 / 14700) loss: 0.859598\n",
      "(Iteration 9378 / 14700) loss: 0.642012\n",
      "(Iteration 9379 / 14700) loss: 1.025483\n",
      "(Iteration 9380 / 14700) loss: 0.715709\n",
      "(Iteration 9381 / 14700) loss: 0.900714\n",
      "(Iteration 9382 / 14700) loss: 0.508538\n",
      "(Iteration 9383 / 14700) loss: 0.739050\n",
      "(Iteration 9384 / 14700) loss: 0.811036\n",
      "(Iteration 9385 / 14700) loss: 0.847236\n",
      "(Iteration 9386 / 14700) loss: 0.612645\n",
      "(Iteration 9387 / 14700) loss: 0.533398\n",
      "(Iteration 9388 / 14700) loss: 0.857658\n",
      "(Iteration 9389 / 14700) loss: 0.562233\n",
      "(Iteration 9390 / 14700) loss: 0.600317\n",
      "(Iteration 9391 / 14700) loss: 0.648965\n",
      "(Iteration 9392 / 14700) loss: 0.839175\n",
      "(Iteration 9393 / 14700) loss: 0.522741\n",
      "(Iteration 9394 / 14700) loss: 0.577203\n",
      "(Iteration 9395 / 14700) loss: 0.693411\n",
      "(Iteration 9396 / 14700) loss: 1.010535\n",
      "(Iteration 9397 / 14700) loss: 0.654644\n",
      "(Iteration 9398 / 14700) loss: 0.522543\n",
      "(Iteration 9399 / 14700) loss: 0.710665\n",
      "(Iteration 9400 / 14700) loss: 0.774193\n",
      "(Iteration 9401 / 14700) loss: 0.755388\n",
      "(Iteration 9402 / 14700) loss: 0.864748\n",
      "(Iteration 9403 / 14700) loss: 0.431276\n",
      "(Iteration 9404 / 14700) loss: 0.654052\n",
      "(Iteration 9405 / 14700) loss: 0.565683\n",
      "(Iteration 9406 / 14700) loss: 0.868411\n",
      "(Iteration 9407 / 14700) loss: 0.840674\n",
      "(Iteration 9408 / 14700) loss: 0.662091\n",
      "(Iteration 9409 / 14700) loss: 0.924710\n",
      "(Iteration 9410 / 14700) loss: 0.753972\n",
      "(Iteration 9411 / 14700) loss: 0.799689\n",
      "(Iteration 9412 / 14700) loss: 0.864292\n",
      "(Iteration 9413 / 14700) loss: 0.887742\n",
      "(Iteration 9414 / 14700) loss: 0.595331\n",
      "(Iteration 9415 / 14700) loss: 0.866675\n",
      "(Iteration 9416 / 14700) loss: 0.826542\n",
      "(Iteration 9417 / 14700) loss: 0.632544\n",
      "(Iteration 9418 / 14700) loss: 0.863009\n",
      "(Iteration 9419 / 14700) loss: 0.742739\n",
      "(Iteration 9420 / 14700) loss: 0.697151\n",
      "(Iteration 9421 / 14700) loss: 0.678940\n",
      "(Iteration 9422 / 14700) loss: 0.874284\n",
      "(Iteration 9423 / 14700) loss: 0.908736\n",
      "(Iteration 9424 / 14700) loss: 0.866619\n",
      "(Iteration 9425 / 14700) loss: 0.684587\n",
      "(Iteration 9426 / 14700) loss: 0.802417\n",
      "(Iteration 9427 / 14700) loss: 0.819479\n",
      "(Iteration 9428 / 14700) loss: 0.699304\n",
      "(Iteration 9429 / 14700) loss: 0.828860\n",
      "(Iteration 9430 / 14700) loss: 0.573337\n",
      "(Iteration 9431 / 14700) loss: 0.655814\n",
      "(Iteration 9432 / 14700) loss: 0.814482\n",
      "(Iteration 9433 / 14700) loss: 0.701809\n",
      "(Iteration 9434 / 14700) loss: 0.801366\n",
      "(Iteration 9435 / 14700) loss: 0.805204\n",
      "(Iteration 9436 / 14700) loss: 0.667269\n",
      "(Iteration 9437 / 14700) loss: 0.880080\n",
      "(Iteration 9438 / 14700) loss: 0.704516\n",
      "(Iteration 9439 / 14700) loss: 0.618755\n",
      "(Iteration 9440 / 14700) loss: 0.856118\n",
      "(Iteration 9441 / 14700) loss: 0.716703\n",
      "(Iteration 9442 / 14700) loss: 0.594973\n",
      "(Iteration 9443 / 14700) loss: 0.654323\n",
      "(Iteration 9444 / 14700) loss: 0.816522\n",
      "(Iteration 9445 / 14700) loss: 0.882162\n",
      "(Iteration 9446 / 14700) loss: 0.680816\n",
      "(Iteration 9447 / 14700) loss: 0.588434\n",
      "(Iteration 9448 / 14700) loss: 0.791067\n",
      "(Iteration 9449 / 14700) loss: 0.710429\n",
      "(Iteration 9450 / 14700) loss: 0.856965\n",
      "(Iteration 9451 / 14700) loss: 0.517722\n",
      "(Iteration 9452 / 14700) loss: 0.517515\n",
      "(Iteration 9453 / 14700) loss: 0.780282\n",
      "(Iteration 9454 / 14700) loss: 0.551382\n",
      "(Iteration 9455 / 14700) loss: 0.749858\n",
      "(Iteration 9456 / 14700) loss: 0.798954\n",
      "(Iteration 9457 / 14700) loss: 1.084350\n",
      "(Iteration 9458 / 14700) loss: 0.619102\n",
      "(Iteration 9459 / 14700) loss: 0.610770\n",
      "(Iteration 9460 / 14700) loss: 0.660133\n",
      "(Iteration 9461 / 14700) loss: 0.652949\n",
      "(Iteration 9462 / 14700) loss: 0.529795\n",
      "(Iteration 9463 / 14700) loss: 0.885372\n",
      "(Iteration 9464 / 14700) loss: 0.728753\n",
      "(Iteration 9465 / 14700) loss: 0.560322\n",
      "(Iteration 9466 / 14700) loss: 0.867007\n",
      "(Iteration 9467 / 14700) loss: 0.784706\n",
      "(Iteration 9468 / 14700) loss: 0.888737\n",
      "(Iteration 9469 / 14700) loss: 0.468558\n",
      "(Iteration 9470 / 14700) loss: 0.685899\n",
      "(Iteration 9471 / 14700) loss: 0.767525\n",
      "(Iteration 9472 / 14700) loss: 0.771283\n",
      "(Iteration 9473 / 14700) loss: 0.696384\n",
      "(Iteration 9474 / 14700) loss: 0.743443\n",
      "(Iteration 9475 / 14700) loss: 0.716312\n",
      "(Iteration 9476 / 14700) loss: 0.789968\n",
      "(Iteration 9477 / 14700) loss: 0.912292\n",
      "(Iteration 9478 / 14700) loss: 0.890316\n",
      "(Iteration 9479 / 14700) loss: 0.625768\n",
      "(Iteration 9480 / 14700) loss: 0.657097\n",
      "(Iteration 9481 / 14700) loss: 0.636005\n",
      "(Iteration 9482 / 14700) loss: 0.873345\n",
      "(Iteration 9483 / 14700) loss: 0.849549\n",
      "(Iteration 9484 / 14700) loss: 0.844726\n",
      "(Iteration 9485 / 14700) loss: 0.625735\n",
      "(Iteration 9486 / 14700) loss: 0.686787\n",
      "(Iteration 9487 / 14700) loss: 0.810689\n",
      "(Iteration 9488 / 14700) loss: 0.685895\n",
      "(Iteration 9489 / 14700) loss: 0.835803\n",
      "(Iteration 9490 / 14700) loss: 0.814646\n",
      "(Iteration 9491 / 14700) loss: 0.594158\n",
      "(Iteration 9492 / 14700) loss: 0.852043\n",
      "(Iteration 9493 / 14700) loss: 0.785228\n",
      "(Iteration 9494 / 14700) loss: 0.792040\n",
      "(Iteration 9495 / 14700) loss: 0.626946\n",
      "(Iteration 9496 / 14700) loss: 0.917334\n",
      "(Iteration 9497 / 14700) loss: 0.731037\n",
      "(Iteration 9498 / 14700) loss: 0.710183\n",
      "(Iteration 9499 / 14700) loss: 0.700617\n",
      "(Iteration 9500 / 14700) loss: 0.658003\n",
      "(Iteration 9501 / 14700) loss: 0.700735\n",
      "(Iteration 9502 / 14700) loss: 0.861270\n",
      "(Iteration 9503 / 14700) loss: 0.724767\n",
      "(Iteration 9504 / 14700) loss: 0.615799\n",
      "(Iteration 9505 / 14700) loss: 0.575444\n",
      "(Iteration 9506 / 14700) loss: 0.647667\n",
      "(Iteration 9507 / 14700) loss: 0.676432\n",
      "(Iteration 9508 / 14700) loss: 0.617605\n",
      "(Iteration 9509 / 14700) loss: 0.695973\n",
      "(Iteration 9510 / 14700) loss: 0.835607\n",
      "(Iteration 9511 / 14700) loss: 0.620963\n",
      "(Iteration 9512 / 14700) loss: 0.677133\n",
      "(Iteration 9513 / 14700) loss: 0.793210\n",
      "(Iteration 9514 / 14700) loss: 0.964345\n",
      "(Iteration 9515 / 14700) loss: 0.868613\n",
      "(Iteration 9516 / 14700) loss: 0.776671\n",
      "(Iteration 9517 / 14700) loss: 0.920873\n",
      "(Iteration 9518 / 14700) loss: 0.681778\n",
      "(Iteration 9519 / 14700) loss: 0.714860\n",
      "(Iteration 9520 / 14700) loss: 0.714012\n",
      "(Iteration 9521 / 14700) loss: 0.820301\n",
      "(Iteration 9522 / 14700) loss: 0.619895\n",
      "(Iteration 9523 / 14700) loss: 0.825101\n",
      "(Iteration 9524 / 14700) loss: 0.664062\n",
      "(Iteration 9525 / 14700) loss: 1.023101\n",
      "(Iteration 9526 / 14700) loss: 0.918315\n",
      "(Iteration 9527 / 14700) loss: 0.771839\n",
      "(Iteration 9528 / 14700) loss: 0.732374\n",
      "(Iteration 9529 / 14700) loss: 0.670892\n",
      "(Iteration 9530 / 14700) loss: 0.812530\n",
      "(Iteration 9531 / 14700) loss: 0.531051\n",
      "(Iteration 9532 / 14700) loss: 0.948120\n",
      "(Iteration 9533 / 14700) loss: 0.667899\n",
      "(Iteration 9534 / 14700) loss: 0.781701\n",
      "(Iteration 9535 / 14700) loss: 0.759989\n",
      "(Iteration 9536 / 14700) loss: 1.043679\n",
      "(Iteration 9537 / 14700) loss: 0.784846\n",
      "(Iteration 9538 / 14700) loss: 0.710172\n",
      "(Iteration 9539 / 14700) loss: 1.180796\n",
      "(Iteration 9540 / 14700) loss: 0.650463\n",
      "(Iteration 9541 / 14700) loss: 1.062005\n",
      "(Iteration 9542 / 14700) loss: 0.892955\n",
      "(Iteration 9543 / 14700) loss: 0.753029\n",
      "(Iteration 9544 / 14700) loss: 0.616677\n",
      "(Iteration 9545 / 14700) loss: 0.795327\n",
      "(Iteration 9546 / 14700) loss: 0.532041\n",
      "(Iteration 9547 / 14700) loss: 0.568505\n",
      "(Iteration 9548 / 14700) loss: 0.756676\n",
      "(Iteration 9549 / 14700) loss: 0.702252\n",
      "(Iteration 9550 / 14700) loss: 0.881709\n",
      "(Iteration 9551 / 14700) loss: 0.675034\n",
      "(Iteration 9552 / 14700) loss: 0.575278\n",
      "(Iteration 9553 / 14700) loss: 0.609983\n",
      "(Iteration 9554 / 14700) loss: 0.813945\n",
      "(Iteration 9555 / 14700) loss: 0.970856\n",
      "(Iteration 9556 / 14700) loss: 0.699878\n",
      "(Iteration 9557 / 14700) loss: 0.631212\n",
      "(Iteration 9558 / 14700) loss: 0.692038\n",
      "(Iteration 9559 / 14700) loss: 0.779021\n",
      "(Iteration 9560 / 14700) loss: 0.770619\n",
      "(Iteration 9561 / 14700) loss: 0.790672\n",
      "(Iteration 9562 / 14700) loss: 0.732357\n",
      "(Iteration 9563 / 14700) loss: 0.854194\n",
      "(Iteration 9564 / 14700) loss: 0.896328\n",
      "(Iteration 9565 / 14700) loss: 0.618513\n",
      "(Iteration 9566 / 14700) loss: 0.738831\n",
      "(Iteration 9567 / 14700) loss: 0.714619\n",
      "(Iteration 9568 / 14700) loss: 0.983160\n",
      "(Iteration 9569 / 14700) loss: 0.903346\n",
      "(Iteration 9570 / 14700) loss: 0.694144\n",
      "(Iteration 9571 / 14700) loss: 0.749430\n",
      "(Iteration 9572 / 14700) loss: 0.787336\n",
      "(Iteration 9573 / 14700) loss: 0.805232\n",
      "(Iteration 9574 / 14700) loss: 0.798097\n",
      "(Iteration 9575 / 14700) loss: 0.645134\n",
      "(Iteration 9576 / 14700) loss: 0.803140\n",
      "(Iteration 9577 / 14700) loss: 0.802725\n",
      "(Iteration 9578 / 14700) loss: 0.721060\n",
      "(Iteration 9579 / 14700) loss: 0.874805\n",
      "(Iteration 9580 / 14700) loss: 0.715432\n",
      "(Iteration 9581 / 14700) loss: 0.614662\n",
      "(Iteration 9582 / 14700) loss: 0.745731\n",
      "(Iteration 9583 / 14700) loss: 0.635965\n",
      "(Iteration 9584 / 14700) loss: 0.595827\n",
      "(Iteration 9585 / 14700) loss: 0.849969\n",
      "(Iteration 9586 / 14700) loss: 0.828027\n",
      "(Iteration 9587 / 14700) loss: 0.612586\n",
      "(Iteration 9588 / 14700) loss: 0.575079\n",
      "(Iteration 9589 / 14700) loss: 0.725095\n",
      "(Iteration 9590 / 14700) loss: 0.848624\n",
      "(Iteration 9591 / 14700) loss: 0.976451\n",
      "(Iteration 9592 / 14700) loss: 0.690547\n",
      "(Iteration 9593 / 14700) loss: 0.669693\n",
      "(Iteration 9594 / 14700) loss: 0.646376\n",
      "(Iteration 9595 / 14700) loss: 1.035911\n",
      "(Iteration 9596 / 14700) loss: 0.589394\n",
      "(Iteration 9597 / 14700) loss: 0.758864\n",
      "(Iteration 9598 / 14700) loss: 0.792371\n",
      "(Iteration 9599 / 14700) loss: 0.607282\n",
      "(Iteration 9600 / 14700) loss: 0.779855\n",
      "(Iteration 9601 / 14700) loss: 0.572319\n",
      "(Iteration 9602 / 14700) loss: 0.889016\n",
      "(Iteration 9603 / 14700) loss: 0.706877\n",
      "(Iteration 9604 / 14700) loss: 0.876389\n",
      "(Iteration 9605 / 14700) loss: 0.733196\n",
      "(Iteration 9606 / 14700) loss: 0.942471\n",
      "(Iteration 9607 / 14700) loss: 0.461407\n",
      "(Iteration 9608 / 14700) loss: 0.753767\n",
      "(Iteration 9609 / 14700) loss: 0.783671\n",
      "(Iteration 9610 / 14700) loss: 0.775013\n",
      "(Iteration 9611 / 14700) loss: 0.693067\n",
      "(Iteration 9612 / 14700) loss: 0.987445\n",
      "(Iteration 9613 / 14700) loss: 0.559271\n",
      "(Iteration 9614 / 14700) loss: 0.920476\n",
      "(Iteration 9615 / 14700) loss: 0.860646\n",
      "(Iteration 9616 / 14700) loss: 0.624228\n",
      "(Iteration 9617 / 14700) loss: 0.516355\n",
      "(Iteration 9618 / 14700) loss: 0.778668\n",
      "(Iteration 9619 / 14700) loss: 0.733126\n",
      "(Iteration 9620 / 14700) loss: 0.740407\n",
      "(Iteration 9621 / 14700) loss: 0.685068\n",
      "(Iteration 9622 / 14700) loss: 0.887561\n",
      "(Iteration 9623 / 14700) loss: 0.736916\n",
      "(Iteration 9624 / 14700) loss: 0.675091\n",
      "(Iteration 9625 / 14700) loss: 0.868450\n",
      "(Iteration 9626 / 14700) loss: 1.183441\n",
      "(Iteration 9627 / 14700) loss: 0.631779\n",
      "(Iteration 9628 / 14700) loss: 0.771110\n",
      "(Iteration 9629 / 14700) loss: 0.824220\n",
      "(Iteration 9630 / 14700) loss: 0.573899\n",
      "(Iteration 9631 / 14700) loss: 0.718031\n",
      "(Iteration 9632 / 14700) loss: 0.665069\n",
      "(Iteration 9633 / 14700) loss: 0.793682\n",
      "(Iteration 9634 / 14700) loss: 0.716161\n",
      "(Iteration 9635 / 14700) loss: 0.725376\n",
      "(Iteration 9636 / 14700) loss: 1.031360\n",
      "(Iteration 9637 / 14700) loss: 0.826605\n",
      "(Iteration 9638 / 14700) loss: 0.782536\n",
      "(Iteration 9639 / 14700) loss: 0.900997\n",
      "(Iteration 9640 / 14700) loss: 0.739936\n",
      "(Iteration 9641 / 14700) loss: 0.724114\n",
      "(Iteration 9642 / 14700) loss: 0.677048\n",
      "(Iteration 9643 / 14700) loss: 0.644158\n",
      "(Iteration 9644 / 14700) loss: 0.977956\n",
      "(Iteration 9645 / 14700) loss: 0.545251\n",
      "(Iteration 9646 / 14700) loss: 0.634079\n",
      "(Iteration 9647 / 14700) loss: 0.822558\n",
      "(Iteration 9648 / 14700) loss: 0.658275\n",
      "(Iteration 9649 / 14700) loss: 0.781854\n",
      "(Iteration 9650 / 14700) loss: 0.687176\n",
      "(Iteration 9651 / 14700) loss: 0.802954\n",
      "(Iteration 9652 / 14700) loss: 1.011782\n",
      "(Iteration 9653 / 14700) loss: 0.696555\n",
      "(Iteration 9654 / 14700) loss: 0.717602\n",
      "(Iteration 9655 / 14700) loss: 0.775555\n",
      "(Iteration 9656 / 14700) loss: 0.696321\n",
      "(Iteration 9657 / 14700) loss: 0.608154\n",
      "(Iteration 9658 / 14700) loss: 0.707076\n",
      "(Iteration 9659 / 14700) loss: 0.737585\n",
      "(Iteration 9660 / 14700) loss: 0.714116\n",
      "(Iteration 9661 / 14700) loss: 0.875272\n",
      "(Iteration 9662 / 14700) loss: 0.813126\n",
      "(Iteration 9663 / 14700) loss: 0.855932\n",
      "(Iteration 9664 / 14700) loss: 0.623406\n",
      "(Iteration 9665 / 14700) loss: 0.853842\n",
      "(Iteration 9666 / 14700) loss: 0.661663\n",
      "(Iteration 9667 / 14700) loss: 0.596371\n",
      "(Iteration 9668 / 14700) loss: 0.551020\n",
      "(Iteration 9669 / 14700) loss: 0.648125\n",
      "(Iteration 9670 / 14700) loss: 0.685894\n",
      "(Iteration 9671 / 14700) loss: 0.789056\n",
      "(Iteration 9672 / 14700) loss: 0.592532\n",
      "(Iteration 9673 / 14700) loss: 0.831963\n",
      "(Iteration 9674 / 14700) loss: 0.798075\n",
      "(Iteration 9675 / 14700) loss: 0.898285\n",
      "(Iteration 9676 / 14700) loss: 0.700503\n",
      "(Iteration 9677 / 14700) loss: 0.716728\n",
      "(Iteration 9678 / 14700) loss: 0.758379\n",
      "(Iteration 9679 / 14700) loss: 0.712451\n",
      "(Iteration 9680 / 14700) loss: 0.917662\n",
      "(Iteration 9681 / 14700) loss: 1.001647\n",
      "(Iteration 9682 / 14700) loss: 0.668278\n",
      "(Iteration 9683 / 14700) loss: 0.560253\n",
      "(Iteration 9684 / 14700) loss: 0.799788\n",
      "(Iteration 9685 / 14700) loss: 0.669074\n",
      "(Iteration 9686 / 14700) loss: 1.193190\n",
      "(Iteration 9687 / 14700) loss: 0.847359\n",
      "(Iteration 9688 / 14700) loss: 0.772538\n",
      "(Iteration 9689 / 14700) loss: 0.567498\n",
      "(Iteration 9690 / 14700) loss: 0.757321\n",
      "(Iteration 9691 / 14700) loss: 0.868212\n",
      "(Iteration 9692 / 14700) loss: 0.618319\n",
      "(Iteration 9693 / 14700) loss: 0.915816\n",
      "(Iteration 9694 / 14700) loss: 0.734549\n",
      "(Iteration 9695 / 14700) loss: 0.770395\n",
      "(Iteration 9696 / 14700) loss: 1.033932\n",
      "(Iteration 9697 / 14700) loss: 0.643131\n",
      "(Iteration 9698 / 14700) loss: 0.575925\n",
      "(Iteration 9699 / 14700) loss: 0.569620\n",
      "(Iteration 9700 / 14700) loss: 0.912665\n",
      "(Iteration 9701 / 14700) loss: 0.503989\n",
      "(Iteration 9702 / 14700) loss: 0.609968\n",
      "(Iteration 9703 / 14700) loss: 0.819452\n",
      "(Iteration 9704 / 14700) loss: 0.551253\n",
      "(Iteration 9705 / 14700) loss: 0.866228\n",
      "(Iteration 9706 / 14700) loss: 0.911830\n",
      "(Iteration 9707 / 14700) loss: 0.645627\n",
      "(Iteration 9708 / 14700) loss: 0.709995\n",
      "(Iteration 9709 / 14700) loss: 1.146561\n",
      "(Iteration 9710 / 14700) loss: 0.770200\n",
      "(Iteration 9711 / 14700) loss: 0.898294\n",
      "(Iteration 9712 / 14700) loss: 1.021186\n",
      "(Iteration 9713 / 14700) loss: 0.706726\n",
      "(Iteration 9714 / 14700) loss: 0.734904\n",
      "(Iteration 9715 / 14700) loss: 0.844176\n",
      "(Iteration 9716 / 14700) loss: 0.950112\n",
      "(Iteration 9717 / 14700) loss: 0.838887\n",
      "(Iteration 9718 / 14700) loss: 0.736056\n",
      "(Iteration 9719 / 14700) loss: 0.902051\n",
      "(Iteration 9720 / 14700) loss: 0.694490\n",
      "(Iteration 9721 / 14700) loss: 0.804684\n",
      "(Iteration 9722 / 14700) loss: 0.613518\n",
      "(Iteration 9723 / 14700) loss: 0.851248\n",
      "(Iteration 9724 / 14700) loss: 0.619379\n",
      "(Iteration 9725 / 14700) loss: 0.787906\n",
      "(Iteration 9726 / 14700) loss: 0.702522\n",
      "(Iteration 9727 / 14700) loss: 0.787319\n",
      "(Iteration 9728 / 14700) loss: 0.936879\n",
      "(Iteration 9729 / 14700) loss: 0.695299\n",
      "(Iteration 9730 / 14700) loss: 0.785589\n",
      "(Iteration 9731 / 14700) loss: 0.733483\n",
      "(Iteration 9732 / 14700) loss: 0.664100\n",
      "(Iteration 9733 / 14700) loss: 0.795874\n",
      "(Iteration 9734 / 14700) loss: 0.754012\n",
      "(Iteration 9735 / 14700) loss: 0.722303\n",
      "(Iteration 9736 / 14700) loss: 0.743639\n",
      "(Iteration 9737 / 14700) loss: 0.942341\n",
      "(Iteration 9738 / 14700) loss: 0.643190\n",
      "(Iteration 9739 / 14700) loss: 0.834157\n",
      "(Iteration 9740 / 14700) loss: 0.791712\n",
      "(Iteration 9741 / 14700) loss: 0.890448\n",
      "(Iteration 9742 / 14700) loss: 0.792963\n",
      "(Iteration 9743 / 14700) loss: 0.654827\n",
      "(Iteration 9744 / 14700) loss: 0.850299\n",
      "(Iteration 9745 / 14700) loss: 0.885803\n",
      "(Iteration 9746 / 14700) loss: 0.609553\n",
      "(Iteration 9747 / 14700) loss: 0.650310\n",
      "(Iteration 9748 / 14700) loss: 0.669142\n",
      "(Iteration 9749 / 14700) loss: 0.792299\n",
      "(Iteration 9750 / 14700) loss: 0.890239\n",
      "(Iteration 9751 / 14700) loss: 0.671473\n",
      "(Iteration 9752 / 14700) loss: 0.865846\n",
      "(Iteration 9753 / 14700) loss: 0.633854\n",
      "(Iteration 9754 / 14700) loss: 0.784345\n",
      "(Iteration 9755 / 14700) loss: 0.695120\n",
      "(Iteration 9756 / 14700) loss: 0.646286\n",
      "(Iteration 9757 / 14700) loss: 0.827284\n",
      "(Iteration 9758 / 14700) loss: 0.748278\n",
      "(Iteration 9759 / 14700) loss: 0.630840\n",
      "(Iteration 9760 / 14700) loss: 0.773744\n",
      "(Iteration 9761 / 14700) loss: 0.937610\n",
      "(Iteration 9762 / 14700) loss: 0.708708\n",
      "(Iteration 9763 / 14700) loss: 0.619515\n",
      "(Iteration 9764 / 14700) loss: 0.781174\n",
      "(Iteration 9765 / 14700) loss: 0.938269\n",
      "(Iteration 9766 / 14700) loss: 0.674693\n",
      "(Iteration 9767 / 14700) loss: 0.767118\n",
      "(Iteration 9768 / 14700) loss: 0.709415\n",
      "(Iteration 9769 / 14700) loss: 0.966773\n",
      "(Iteration 9770 / 14700) loss: 0.771302\n",
      "(Iteration 9771 / 14700) loss: 0.658356\n",
      "(Iteration 9772 / 14700) loss: 0.780547\n",
      "(Iteration 9773 / 14700) loss: 0.767599\n",
      "(Iteration 9774 / 14700) loss: 0.715423\n",
      "(Iteration 9775 / 14700) loss: 0.705514\n",
      "(Iteration 9776 / 14700) loss: 0.830798\n",
      "(Iteration 9777 / 14700) loss: 0.704093\n",
      "(Iteration 9778 / 14700) loss: 0.624381\n",
      "(Iteration 9779 / 14700) loss: 0.527640\n",
      "(Iteration 9780 / 14700) loss: 0.744284\n",
      "(Iteration 9781 / 14700) loss: 0.751463\n",
      "(Iteration 9782 / 14700) loss: 0.800364\n",
      "(Iteration 9783 / 14700) loss: 0.787846\n",
      "(Iteration 9784 / 14700) loss: 0.703130\n",
      "(Iteration 9785 / 14700) loss: 0.602173\n",
      "(Iteration 9786 / 14700) loss: 0.775832\n",
      "(Iteration 9787 / 14700) loss: 0.643520\n",
      "(Iteration 9788 / 14700) loss: 0.743593\n",
      "(Iteration 9789 / 14700) loss: 0.573706\n",
      "(Iteration 9790 / 14700) loss: 0.745317\n",
      "(Iteration 9791 / 14700) loss: 0.933356\n",
      "(Iteration 9792 / 14700) loss: 0.684365\n",
      "(Iteration 9793 / 14700) loss: 0.714712\n",
      "(Iteration 9794 / 14700) loss: 0.829102\n",
      "(Iteration 9795 / 14700) loss: 0.924284\n",
      "(Iteration 9796 / 14700) loss: 0.836501\n",
      "(Iteration 9797 / 14700) loss: 0.671398\n",
      "(Iteration 9798 / 14700) loss: 0.802836\n",
      "(Iteration 9799 / 14700) loss: 0.880332\n",
      "(Iteration 9800 / 14700) loss: 0.703365\n",
      "(Epoch 10 / 15) train acc: 0.832000; val_acc: 0.761000\n",
      "(Iteration 9801 / 14700) loss: 0.837111\n",
      "(Iteration 9802 / 14700) loss: 0.774814\n",
      "(Iteration 9803 / 14700) loss: 0.687502\n",
      "(Iteration 9804 / 14700) loss: 0.699765\n",
      "(Iteration 9805 / 14700) loss: 0.917382\n",
      "(Iteration 9806 / 14700) loss: 0.751087\n",
      "(Iteration 9807 / 14700) loss: 0.858092\n",
      "(Iteration 9808 / 14700) loss: 1.147186\n",
      "(Iteration 9809 / 14700) loss: 0.764064\n",
      "(Iteration 9810 / 14700) loss: 0.699587\n",
      "(Iteration 9811 / 14700) loss: 0.756073\n",
      "(Iteration 9812 / 14700) loss: 0.916979\n",
      "(Iteration 9813 / 14700) loss: 0.810248\n",
      "(Iteration 9814 / 14700) loss: 0.544735\n",
      "(Iteration 9815 / 14700) loss: 0.746705\n",
      "(Iteration 9816 / 14700) loss: 0.814162\n",
      "(Iteration 9817 / 14700) loss: 0.949717\n",
      "(Iteration 9818 / 14700) loss: 0.563950\n",
      "(Iteration 9819 / 14700) loss: 0.890164\n",
      "(Iteration 9820 / 14700) loss: 0.708404\n",
      "(Iteration 9821 / 14700) loss: 0.777628\n",
      "(Iteration 9822 / 14700) loss: 0.619638\n",
      "(Iteration 9823 / 14700) loss: 0.619478\n",
      "(Iteration 9824 / 14700) loss: 0.817234\n",
      "(Iteration 9825 / 14700) loss: 0.760378\n",
      "(Iteration 9826 / 14700) loss: 0.987208\n",
      "(Iteration 9827 / 14700) loss: 0.777247\n",
      "(Iteration 9828 / 14700) loss: 0.874055\n",
      "(Iteration 9829 / 14700) loss: 0.594198\n",
      "(Iteration 9830 / 14700) loss: 0.564222\n",
      "(Iteration 9831 / 14700) loss: 0.651692\n",
      "(Iteration 9832 / 14700) loss: 0.716138\n",
      "(Iteration 9833 / 14700) loss: 0.794515\n",
      "(Iteration 9834 / 14700) loss: 0.734121\n",
      "(Iteration 9835 / 14700) loss: 0.860051\n",
      "(Iteration 9836 / 14700) loss: 0.869273\n",
      "(Iteration 9837 / 14700) loss: 0.709722\n",
      "(Iteration 9838 / 14700) loss: 0.816646\n",
      "(Iteration 9839 / 14700) loss: 0.735961\n",
      "(Iteration 9840 / 14700) loss: 0.599150\n",
      "(Iteration 9841 / 14700) loss: 0.756791\n",
      "(Iteration 9842 / 14700) loss: 0.604963\n",
      "(Iteration 9843 / 14700) loss: 0.620891\n",
      "(Iteration 9844 / 14700) loss: 0.735441\n",
      "(Iteration 9845 / 14700) loss: 0.760259\n",
      "(Iteration 9846 / 14700) loss: 0.593864\n",
      "(Iteration 9847 / 14700) loss: 0.879372\n",
      "(Iteration 9848 / 14700) loss: 0.561855\n",
      "(Iteration 9849 / 14700) loss: 0.831033\n",
      "(Iteration 9850 / 14700) loss: 0.828226\n",
      "(Iteration 9851 / 14700) loss: 0.688106\n",
      "(Iteration 9852 / 14700) loss: 0.740937\n",
      "(Iteration 9853 / 14700) loss: 0.693608\n",
      "(Iteration 9854 / 14700) loss: 0.731605\n",
      "(Iteration 9855 / 14700) loss: 0.891272\n",
      "(Iteration 9856 / 14700) loss: 0.633482\n",
      "(Iteration 9857 / 14700) loss: 0.902410\n",
      "(Iteration 9858 / 14700) loss: 1.136625\n",
      "(Iteration 9859 / 14700) loss: 0.949282\n",
      "(Iteration 9860 / 14700) loss: 0.805277\n",
      "(Iteration 9861 / 14700) loss: 0.725947\n",
      "(Iteration 9862 / 14700) loss: 0.890081\n",
      "(Iteration 9863 / 14700) loss: 0.971866\n",
      "(Iteration 9864 / 14700) loss: 0.541302\n",
      "(Iteration 9865 / 14700) loss: 0.801073\n",
      "(Iteration 9866 / 14700) loss: 0.877534\n",
      "(Iteration 9867 / 14700) loss: 0.827142\n",
      "(Iteration 9868 / 14700) loss: 0.656234\n",
      "(Iteration 9869 / 14700) loss: 0.820564\n",
      "(Iteration 9870 / 14700) loss: 0.814882\n",
      "(Iteration 9871 / 14700) loss: 0.751437\n",
      "(Iteration 9872 / 14700) loss: 0.575521\n",
      "(Iteration 9873 / 14700) loss: 0.874877\n",
      "(Iteration 9874 / 14700) loss: 0.839569\n",
      "(Iteration 9875 / 14700) loss: 0.666293\n",
      "(Iteration 9876 / 14700) loss: 0.728916\n",
      "(Iteration 9877 / 14700) loss: 0.756673\n",
      "(Iteration 9878 / 14700) loss: 0.853292\n",
      "(Iteration 9879 / 14700) loss: 0.733469\n",
      "(Iteration 9880 / 14700) loss: 0.645980\n",
      "(Iteration 9881 / 14700) loss: 0.850162\n",
      "(Iteration 9882 / 14700) loss: 0.534407\n",
      "(Iteration 9883 / 14700) loss: 0.690983\n",
      "(Iteration 9884 / 14700) loss: 0.688215\n",
      "(Iteration 9885 / 14700) loss: 0.687512\n",
      "(Iteration 9886 / 14700) loss: 0.926918\n",
      "(Iteration 9887 / 14700) loss: 0.640913\n",
      "(Iteration 9888 / 14700) loss: 1.023324\n",
      "(Iteration 9889 / 14700) loss: 0.880148\n",
      "(Iteration 9890 / 14700) loss: 0.701001\n",
      "(Iteration 9891 / 14700) loss: 0.685927\n",
      "(Iteration 9892 / 14700) loss: 0.638907\n",
      "(Iteration 9893 / 14700) loss: 0.667840\n",
      "(Iteration 9894 / 14700) loss: 0.703521\n",
      "(Iteration 9895 / 14700) loss: 0.685546\n",
      "(Iteration 9896 / 14700) loss: 0.715848\n",
      "(Iteration 9897 / 14700) loss: 0.610689\n",
      "(Iteration 9898 / 14700) loss: 0.655980\n",
      "(Iteration 9899 / 14700) loss: 0.527576\n",
      "(Iteration 9900 / 14700) loss: 0.732921\n",
      "(Iteration 9901 / 14700) loss: 0.783387\n",
      "(Iteration 9902 / 14700) loss: 1.003528\n",
      "(Iteration 9903 / 14700) loss: 0.596156\n",
      "(Iteration 9904 / 14700) loss: 0.757248\n",
      "(Iteration 9905 / 14700) loss: 0.590761\n",
      "(Iteration 9906 / 14700) loss: 0.725859\n",
      "(Iteration 9907 / 14700) loss: 0.906780\n",
      "(Iteration 9908 / 14700) loss: 0.615283\n",
      "(Iteration 9909 / 14700) loss: 0.576025\n",
      "(Iteration 9910 / 14700) loss: 0.705621\n",
      "(Iteration 9911 / 14700) loss: 0.854420\n",
      "(Iteration 9912 / 14700) loss: 1.085051\n",
      "(Iteration 9913 / 14700) loss: 0.860994\n",
      "(Iteration 9914 / 14700) loss: 0.789224\n",
      "(Iteration 9915 / 14700) loss: 0.676414\n",
      "(Iteration 9916 / 14700) loss: 0.702887\n",
      "(Iteration 9917 / 14700) loss: 0.713618\n",
      "(Iteration 9918 / 14700) loss: 0.859257\n",
      "(Iteration 9919 / 14700) loss: 0.708825\n",
      "(Iteration 9920 / 14700) loss: 0.613235\n",
      "(Iteration 9921 / 14700) loss: 1.063471\n",
      "(Iteration 9922 / 14700) loss: 0.757907\n",
      "(Iteration 9923 / 14700) loss: 0.601002\n",
      "(Iteration 9924 / 14700) loss: 0.898501\n",
      "(Iteration 9925 / 14700) loss: 0.852137\n",
      "(Iteration 9926 / 14700) loss: 0.698146\n",
      "(Iteration 9927 / 14700) loss: 0.783170\n",
      "(Iteration 9928 / 14700) loss: 0.855243\n",
      "(Iteration 9929 / 14700) loss: 0.924395\n",
      "(Iteration 9930 / 14700) loss: 0.719894\n",
      "(Iteration 9931 / 14700) loss: 0.588472\n",
      "(Iteration 9932 / 14700) loss: 0.675216\n",
      "(Iteration 9933 / 14700) loss: 0.883464\n",
      "(Iteration 9934 / 14700) loss: 0.725458\n",
      "(Iteration 9935 / 14700) loss: 0.685435\n",
      "(Iteration 9936 / 14700) loss: 0.658324\n",
      "(Iteration 9937 / 14700) loss: 0.747843\n",
      "(Iteration 9938 / 14700) loss: 0.833318\n",
      "(Iteration 9939 / 14700) loss: 0.776608\n",
      "(Iteration 9940 / 14700) loss: 0.749242\n",
      "(Iteration 9941 / 14700) loss: 0.602520\n",
      "(Iteration 9942 / 14700) loss: 1.194951\n",
      "(Iteration 9943 / 14700) loss: 0.570115\n",
      "(Iteration 9944 / 14700) loss: 0.772455\n",
      "(Iteration 9945 / 14700) loss: 0.812643\n",
      "(Iteration 9946 / 14700) loss: 0.735894\n",
      "(Iteration 9947 / 14700) loss: 0.822787\n",
      "(Iteration 9948 / 14700) loss: 0.942715\n",
      "(Iteration 9949 / 14700) loss: 0.808053\n",
      "(Iteration 9950 / 14700) loss: 0.611286\n",
      "(Iteration 9951 / 14700) loss: 0.624762\n",
      "(Iteration 9952 / 14700) loss: 0.962799\n",
      "(Iteration 9953 / 14700) loss: 0.715610\n",
      "(Iteration 9954 / 14700) loss: 0.841109\n",
      "(Iteration 9955 / 14700) loss: 0.821632\n",
      "(Iteration 9956 / 14700) loss: 0.624425\n",
      "(Iteration 9957 / 14700) loss: 0.826304\n",
      "(Iteration 9958 / 14700) loss: 0.917391\n",
      "(Iteration 9959 / 14700) loss: 0.616933\n",
      "(Iteration 9960 / 14700) loss: 0.805060\n",
      "(Iteration 9961 / 14700) loss: 0.779674\n",
      "(Iteration 9962 / 14700) loss: 0.577767\n",
      "(Iteration 9963 / 14700) loss: 0.667248\n",
      "(Iteration 9964 / 14700) loss: 1.050772\n",
      "(Iteration 9965 / 14700) loss: 0.653017\n",
      "(Iteration 9966 / 14700) loss: 0.983664\n",
      "(Iteration 9967 / 14700) loss: 0.835148\n",
      "(Iteration 9968 / 14700) loss: 0.705995\n",
      "(Iteration 9969 / 14700) loss: 0.809594\n",
      "(Iteration 9970 / 14700) loss: 0.768364\n",
      "(Iteration 9971 / 14700) loss: 0.679429\n",
      "(Iteration 9972 / 14700) loss: 0.691032\n",
      "(Iteration 9973 / 14700) loss: 0.733824\n",
      "(Iteration 9974 / 14700) loss: 0.774081\n",
      "(Iteration 9975 / 14700) loss: 0.532860\n",
      "(Iteration 9976 / 14700) loss: 0.564138\n",
      "(Iteration 9977 / 14700) loss: 0.908479\n",
      "(Iteration 9978 / 14700) loss: 0.615457\n",
      "(Iteration 9979 / 14700) loss: 0.663703\n",
      "(Iteration 9980 / 14700) loss: 0.790757\n",
      "(Iteration 9981 / 14700) loss: 0.674787\n",
      "(Iteration 9982 / 14700) loss: 0.741311\n",
      "(Iteration 9983 / 14700) loss: 0.775204\n",
      "(Iteration 9984 / 14700) loss: 0.804129\n",
      "(Iteration 9985 / 14700) loss: 0.944591\n",
      "(Iteration 9986 / 14700) loss: 0.895501\n",
      "(Iteration 9987 / 14700) loss: 0.820422\n",
      "(Iteration 9988 / 14700) loss: 0.545923\n",
      "(Iteration 9989 / 14700) loss: 0.648125\n",
      "(Iteration 9990 / 14700) loss: 0.858290\n",
      "(Iteration 9991 / 14700) loss: 0.674202\n",
      "(Iteration 9992 / 14700) loss: 0.774914\n",
      "(Iteration 9993 / 14700) loss: 0.735773\n",
      "(Iteration 9994 / 14700) loss: 0.843976\n",
      "(Iteration 9995 / 14700) loss: 0.870415\n",
      "(Iteration 9996 / 14700) loss: 0.815085\n",
      "(Iteration 9997 / 14700) loss: 0.812674\n",
      "(Iteration 9998 / 14700) loss: 0.748374\n",
      "(Iteration 9999 / 14700) loss: 0.746793\n",
      "(Iteration 10000 / 14700) loss: 0.897157\n",
      "(Iteration 10001 / 14700) loss: 0.629528\n",
      "(Iteration 10002 / 14700) loss: 0.686314\n",
      "(Iteration 10003 / 14700) loss: 0.536413\n",
      "(Iteration 10004 / 14700) loss: 0.824826\n",
      "(Iteration 10005 / 14700) loss: 0.718314\n",
      "(Iteration 10006 / 14700) loss: 0.841238\n",
      "(Iteration 10007 / 14700) loss: 0.583059\n",
      "(Iteration 10008 / 14700) loss: 0.740447\n",
      "(Iteration 10009 / 14700) loss: 0.800652\n",
      "(Iteration 10010 / 14700) loss: 0.749866\n",
      "(Iteration 10011 / 14700) loss: 0.559617\n",
      "(Iteration 10012 / 14700) loss: 0.657533\n",
      "(Iteration 10013 / 14700) loss: 0.754244\n",
      "(Iteration 10014 / 14700) loss: 0.801882\n",
      "(Iteration 10015 / 14700) loss: 0.604791\n",
      "(Iteration 10016 / 14700) loss: 0.598997\n",
      "(Iteration 10017 / 14700) loss: 0.731655\n",
      "(Iteration 10018 / 14700) loss: 0.949178\n",
      "(Iteration 10019 / 14700) loss: 0.767032\n",
      "(Iteration 10020 / 14700) loss: 0.722858\n",
      "(Iteration 10021 / 14700) loss: 0.760908\n",
      "(Iteration 10022 / 14700) loss: 0.861338\n",
      "(Iteration 10023 / 14700) loss: 0.667444\n",
      "(Iteration 10024 / 14700) loss: 0.804689\n",
      "(Iteration 10025 / 14700) loss: 0.732811\n",
      "(Iteration 10026 / 14700) loss: 0.693458\n",
      "(Iteration 10027 / 14700) loss: 0.564240\n",
      "(Iteration 10028 / 14700) loss: 0.821375\n",
      "(Iteration 10029 / 14700) loss: 0.888805\n",
      "(Iteration 10030 / 14700) loss: 0.654682\n",
      "(Iteration 10031 / 14700) loss: 0.999471\n",
      "(Iteration 10032 / 14700) loss: 0.692366\n",
      "(Iteration 10033 / 14700) loss: 1.003095\n",
      "(Iteration 10034 / 14700) loss: 0.868857\n",
      "(Iteration 10035 / 14700) loss: 0.919310\n",
      "(Iteration 10036 / 14700) loss: 0.797114\n",
      "(Iteration 10037 / 14700) loss: 0.676039\n",
      "(Iteration 10038 / 14700) loss: 0.563690\n",
      "(Iteration 10039 / 14700) loss: 0.564594\n",
      "(Iteration 10040 / 14700) loss: 0.617937\n",
      "(Iteration 10041 / 14700) loss: 0.737010\n",
      "(Iteration 10042 / 14700) loss: 1.035497\n",
      "(Iteration 10043 / 14700) loss: 0.553677\n",
      "(Iteration 10044 / 14700) loss: 0.891244\n",
      "(Iteration 10045 / 14700) loss: 0.700543\n",
      "(Iteration 10046 / 14700) loss: 0.882398\n",
      "(Iteration 10047 / 14700) loss: 1.013513\n",
      "(Iteration 10048 / 14700) loss: 0.820994\n",
      "(Iteration 10049 / 14700) loss: 0.563054\n",
      "(Iteration 10050 / 14700) loss: 0.738546\n",
      "(Iteration 10051 / 14700) loss: 0.576601\n",
      "(Iteration 10052 / 14700) loss: 0.790803\n",
      "(Iteration 10053 / 14700) loss: 0.587304\n",
      "(Iteration 10054 / 14700) loss: 0.937370\n",
      "(Iteration 10055 / 14700) loss: 0.887573\n",
      "(Iteration 10056 / 14700) loss: 0.858748\n",
      "(Iteration 10057 / 14700) loss: 0.865537\n",
      "(Iteration 10058 / 14700) loss: 0.749290\n",
      "(Iteration 10059 / 14700) loss: 0.770344\n",
      "(Iteration 10060 / 14700) loss: 0.751185\n",
      "(Iteration 10061 / 14700) loss: 0.609890\n",
      "(Iteration 10062 / 14700) loss: 0.802372\n",
      "(Iteration 10063 / 14700) loss: 0.903483\n",
      "(Iteration 10064 / 14700) loss: 0.795511\n",
      "(Iteration 10065 / 14700) loss: 0.788354\n",
      "(Iteration 10066 / 14700) loss: 0.968879\n",
      "(Iteration 10067 / 14700) loss: 0.487685\n",
      "(Iteration 10068 / 14700) loss: 0.766116\n",
      "(Iteration 10069 / 14700) loss: 0.705465\n",
      "(Iteration 10070 / 14700) loss: 0.609593\n",
      "(Iteration 10071 / 14700) loss: 0.605502\n",
      "(Iteration 10072 / 14700) loss: 0.701405\n",
      "(Iteration 10073 / 14700) loss: 0.755041\n",
      "(Iteration 10074 / 14700) loss: 0.801914\n",
      "(Iteration 10075 / 14700) loss: 0.625919\n",
      "(Iteration 10076 / 14700) loss: 0.646063\n",
      "(Iteration 10077 / 14700) loss: 0.732152\n",
      "(Iteration 10078 / 14700) loss: 0.692579\n",
      "(Iteration 10079 / 14700) loss: 0.791598\n",
      "(Iteration 10080 / 14700) loss: 0.725558\n",
      "(Iteration 10081 / 14700) loss: 0.656836\n",
      "(Iteration 10082 / 14700) loss: 0.767889\n",
      "(Iteration 10083 / 14700) loss: 0.529811\n",
      "(Iteration 10084 / 14700) loss: 0.814817\n",
      "(Iteration 10085 / 14700) loss: 1.191103\n",
      "(Iteration 10086 / 14700) loss: 0.785372\n",
      "(Iteration 10087 / 14700) loss: 0.745766\n",
      "(Iteration 10088 / 14700) loss: 0.803485\n",
      "(Iteration 10089 / 14700) loss: 0.828451\n",
      "(Iteration 10090 / 14700) loss: 0.847165\n",
      "(Iteration 10091 / 14700) loss: 0.729322\n",
      "(Iteration 10092 / 14700) loss: 0.715561\n",
      "(Iteration 10093 / 14700) loss: 0.646309\n",
      "(Iteration 10094 / 14700) loss: 0.845645\n",
      "(Iteration 10095 / 14700) loss: 0.837054\n",
      "(Iteration 10096 / 14700) loss: 0.671196\n",
      "(Iteration 10097 / 14700) loss: 0.702884\n",
      "(Iteration 10098 / 14700) loss: 0.704785\n",
      "(Iteration 10099 / 14700) loss: 0.547594\n",
      "(Iteration 10100 / 14700) loss: 0.880070\n",
      "(Iteration 10101 / 14700) loss: 0.789846\n",
      "(Iteration 10102 / 14700) loss: 0.843560\n",
      "(Iteration 10103 / 14700) loss: 0.950858\n",
      "(Iteration 10104 / 14700) loss: 0.933775\n",
      "(Iteration 10105 / 14700) loss: 0.683401\n",
      "(Iteration 10106 / 14700) loss: 1.103277\n",
      "(Iteration 10107 / 14700) loss: 0.749379\n",
      "(Iteration 10108 / 14700) loss: 0.731380\n",
      "(Iteration 10109 / 14700) loss: 0.676120\n",
      "(Iteration 10110 / 14700) loss: 0.883061\n",
      "(Iteration 10111 / 14700) loss: 0.619780\n",
      "(Iteration 10112 / 14700) loss: 0.741907\n",
      "(Iteration 10113 / 14700) loss: 0.523725\n",
      "(Iteration 10114 / 14700) loss: 0.716859\n",
      "(Iteration 10115 / 14700) loss: 0.687319\n",
      "(Iteration 10116 / 14700) loss: 0.643975\n",
      "(Iteration 10117 / 14700) loss: 0.591956\n",
      "(Iteration 10118 / 14700) loss: 0.687528\n",
      "(Iteration 10119 / 14700) loss: 0.812427\n",
      "(Iteration 10120 / 14700) loss: 0.535085\n",
      "(Iteration 10121 / 14700) loss: 0.823424\n",
      "(Iteration 10122 / 14700) loss: 0.634739\n",
      "(Iteration 10123 / 14700) loss: 0.608184\n",
      "(Iteration 10124 / 14700) loss: 0.609379\n",
      "(Iteration 10125 / 14700) loss: 0.690813\n",
      "(Iteration 10126 / 14700) loss: 0.793119\n",
      "(Iteration 10127 / 14700) loss: 0.625672\n",
      "(Iteration 10128 / 14700) loss: 0.796860\n",
      "(Iteration 10129 / 14700) loss: 0.813730\n",
      "(Iteration 10130 / 14700) loss: 0.692944\n",
      "(Iteration 10131 / 14700) loss: 0.612911\n",
      "(Iteration 10132 / 14700) loss: 0.778131\n",
      "(Iteration 10133 / 14700) loss: 0.591308\n",
      "(Iteration 10134 / 14700) loss: 0.703203\n",
      "(Iteration 10135 / 14700) loss: 0.909548\n",
      "(Iteration 10136 / 14700) loss: 0.633854\n",
      "(Iteration 10137 / 14700) loss: 0.526007\n",
      "(Iteration 10138 / 14700) loss: 0.650960\n",
      "(Iteration 10139 / 14700) loss: 0.612956\n",
      "(Iteration 10140 / 14700) loss: 0.848013\n",
      "(Iteration 10141 / 14700) loss: 0.661402\n",
      "(Iteration 10142 / 14700) loss: 0.889023\n",
      "(Iteration 10143 / 14700) loss: 0.692829\n",
      "(Iteration 10144 / 14700) loss: 0.670424\n",
      "(Iteration 10145 / 14700) loss: 0.799826\n",
      "(Iteration 10146 / 14700) loss: 0.617085\n",
      "(Iteration 10147 / 14700) loss: 0.780696\n",
      "(Iteration 10148 / 14700) loss: 0.971448\n",
      "(Iteration 10149 / 14700) loss: 0.794146\n",
      "(Iteration 10150 / 14700) loss: 0.904294\n",
      "(Iteration 10151 / 14700) loss: 0.731757\n",
      "(Iteration 10152 / 14700) loss: 0.632177\n",
      "(Iteration 10153 / 14700) loss: 0.878005\n",
      "(Iteration 10154 / 14700) loss: 0.733445\n",
      "(Iteration 10155 / 14700) loss: 0.982262\n",
      "(Iteration 10156 / 14700) loss: 0.644661\n",
      "(Iteration 10157 / 14700) loss: 1.136346\n",
      "(Iteration 10158 / 14700) loss: 0.601243\n",
      "(Iteration 10159 / 14700) loss: 0.790129\n",
      "(Iteration 10160 / 14700) loss: 0.799184\n",
      "(Iteration 10161 / 14700) loss: 0.687996\n",
      "(Iteration 10162 / 14700) loss: 0.726145\n",
      "(Iteration 10163 / 14700) loss: 0.770425\n",
      "(Iteration 10164 / 14700) loss: 0.557300\n",
      "(Iteration 10165 / 14700) loss: 0.698886\n",
      "(Iteration 10166 / 14700) loss: 0.937264\n",
      "(Iteration 10167 / 14700) loss: 0.869333\n",
      "(Iteration 10168 / 14700) loss: 0.520117\n",
      "(Iteration 10169 / 14700) loss: 0.878684\n",
      "(Iteration 10170 / 14700) loss: 0.922784\n",
      "(Iteration 10171 / 14700) loss: 0.968555\n",
      "(Iteration 10172 / 14700) loss: 0.793373\n",
      "(Iteration 10173 / 14700) loss: 0.497410\n",
      "(Iteration 10174 / 14700) loss: 0.770557\n",
      "(Iteration 10175 / 14700) loss: 0.819038\n",
      "(Iteration 10176 / 14700) loss: 0.622078\n",
      "(Iteration 10177 / 14700) loss: 0.878104\n",
      "(Iteration 10178 / 14700) loss: 0.742685\n",
      "(Iteration 10179 / 14700) loss: 0.986936\n",
      "(Iteration 10180 / 14700) loss: 0.684540\n",
      "(Iteration 10181 / 14700) loss: 0.938343\n",
      "(Iteration 10182 / 14700) loss: 0.845800\n",
      "(Iteration 10183 / 14700) loss: 1.082392\n",
      "(Iteration 10184 / 14700) loss: 0.765612\n",
      "(Iteration 10185 / 14700) loss: 0.589582\n",
      "(Iteration 10186 / 14700) loss: 0.500348\n",
      "(Iteration 10187 / 14700) loss: 0.912127\n",
      "(Iteration 10188 / 14700) loss: 0.862105\n",
      "(Iteration 10189 / 14700) loss: 0.596975\n",
      "(Iteration 10190 / 14700) loss: 0.520945\n",
      "(Iteration 10191 / 14700) loss: 0.719411\n",
      "(Iteration 10192 / 14700) loss: 0.953743\n",
      "(Iteration 10193 / 14700) loss: 0.637800\n",
      "(Iteration 10194 / 14700) loss: 0.793175\n",
      "(Iteration 10195 / 14700) loss: 0.718161\n",
      "(Iteration 10196 / 14700) loss: 0.815889\n",
      "(Iteration 10197 / 14700) loss: 0.689134\n",
      "(Iteration 10198 / 14700) loss: 0.717425\n",
      "(Iteration 10199 / 14700) loss: 0.736336\n",
      "(Iteration 10200 / 14700) loss: 0.448346\n",
      "(Iteration 10201 / 14700) loss: 0.703429\n",
      "(Iteration 10202 / 14700) loss: 0.571626\n",
      "(Iteration 10203 / 14700) loss: 0.758660\n",
      "(Iteration 10204 / 14700) loss: 0.928096\n",
      "(Iteration 10205 / 14700) loss: 0.659536\n",
      "(Iteration 10206 / 14700) loss: 0.909587\n",
      "(Iteration 10207 / 14700) loss: 0.843276\n",
      "(Iteration 10208 / 14700) loss: 0.537722\n",
      "(Iteration 10209 / 14700) loss: 0.829012\n",
      "(Iteration 10210 / 14700) loss: 0.749212\n",
      "(Iteration 10211 / 14700) loss: 0.667812\n",
      "(Iteration 10212 / 14700) loss: 0.940377\n",
      "(Iteration 10213 / 14700) loss: 0.851621\n",
      "(Iteration 10214 / 14700) loss: 0.505023\n",
      "(Iteration 10215 / 14700) loss: 1.034037\n",
      "(Iteration 10216 / 14700) loss: 0.517606\n",
      "(Iteration 10217 / 14700) loss: 0.599211\n",
      "(Iteration 10218 / 14700) loss: 0.734727\n",
      "(Iteration 10219 / 14700) loss: 0.781814\n",
      "(Iteration 10220 / 14700) loss: 0.780042\n",
      "(Iteration 10221 / 14700) loss: 0.732886\n",
      "(Iteration 10222 / 14700) loss: 0.708041\n",
      "(Iteration 10223 / 14700) loss: 0.831245\n",
      "(Iteration 10224 / 14700) loss: 0.721699\n",
      "(Iteration 10225 / 14700) loss: 0.636418\n",
      "(Iteration 10226 / 14700) loss: 1.088036\n",
      "(Iteration 10227 / 14700) loss: 0.549860\n",
      "(Iteration 10228 / 14700) loss: 0.828410\n",
      "(Iteration 10229 / 14700) loss: 0.802378\n",
      "(Iteration 10230 / 14700) loss: 0.942265\n",
      "(Iteration 10231 / 14700) loss: 0.526915\n",
      "(Iteration 10232 / 14700) loss: 0.598946\n",
      "(Iteration 10233 / 14700) loss: 0.611431\n",
      "(Iteration 10234 / 14700) loss: 0.688221\n",
      "(Iteration 10235 / 14700) loss: 0.977462\n",
      "(Iteration 10236 / 14700) loss: 0.870483\n",
      "(Iteration 10237 / 14700) loss: 0.774973\n",
      "(Iteration 10238 / 14700) loss: 0.861310\n",
      "(Iteration 10239 / 14700) loss: 0.694917\n",
      "(Iteration 10240 / 14700) loss: 0.575652\n",
      "(Iteration 10241 / 14700) loss: 0.787273\n",
      "(Iteration 10242 / 14700) loss: 0.722836\n",
      "(Iteration 10243 / 14700) loss: 0.889106\n",
      "(Iteration 10244 / 14700) loss: 0.854238\n",
      "(Iteration 10245 / 14700) loss: 0.739918\n",
      "(Iteration 10246 / 14700) loss: 0.622872\n",
      "(Iteration 10247 / 14700) loss: 0.579852\n",
      "(Iteration 10248 / 14700) loss: 0.802906\n",
      "(Iteration 10249 / 14700) loss: 0.688106\n",
      "(Iteration 10250 / 14700) loss: 0.745289\n",
      "(Iteration 10251 / 14700) loss: 0.872926\n",
      "(Iteration 10252 / 14700) loss: 0.508265\n",
      "(Iteration 10253 / 14700) loss: 0.682191\n",
      "(Iteration 10254 / 14700) loss: 0.571274\n",
      "(Iteration 10255 / 14700) loss: 0.740631\n",
      "(Iteration 10256 / 14700) loss: 0.719419\n",
      "(Iteration 10257 / 14700) loss: 0.764796\n",
      "(Iteration 10258 / 14700) loss: 0.868550\n",
      "(Iteration 10259 / 14700) loss: 0.652866\n",
      "(Iteration 10260 / 14700) loss: 0.786686\n",
      "(Iteration 10261 / 14700) loss: 0.756376\n",
      "(Iteration 10262 / 14700) loss: 0.712296\n",
      "(Iteration 10263 / 14700) loss: 0.904622\n",
      "(Iteration 10264 / 14700) loss: 0.745741\n",
      "(Iteration 10265 / 14700) loss: 0.732362\n",
      "(Iteration 10266 / 14700) loss: 0.648462\n",
      "(Iteration 10267 / 14700) loss: 0.798238\n",
      "(Iteration 10268 / 14700) loss: 0.942253\n",
      "(Iteration 10269 / 14700) loss: 0.675428\n",
      "(Iteration 10270 / 14700) loss: 0.806373\n",
      "(Iteration 10271 / 14700) loss: 0.798558\n",
      "(Iteration 10272 / 14700) loss: 0.574428\n",
      "(Iteration 10273 / 14700) loss: 0.518605\n",
      "(Iteration 10274 / 14700) loss: 0.784840\n",
      "(Iteration 10275 / 14700) loss: 0.772344\n",
      "(Iteration 10276 / 14700) loss: 0.558760\n",
      "(Iteration 10277 / 14700) loss: 0.801202\n",
      "(Iteration 10278 / 14700) loss: 1.192506\n",
      "(Iteration 10279 / 14700) loss: 0.650180\n",
      "(Iteration 10280 / 14700) loss: 0.647594\n",
      "(Iteration 10281 / 14700) loss: 0.628122\n",
      "(Iteration 10282 / 14700) loss: 0.799176\n",
      "(Iteration 10283 / 14700) loss: 0.568537\n",
      "(Iteration 10284 / 14700) loss: 0.822396\n",
      "(Iteration 10285 / 14700) loss: 0.840989\n",
      "(Iteration 10286 / 14700) loss: 0.830381\n",
      "(Iteration 10287 / 14700) loss: 0.796451\n",
      "(Iteration 10288 / 14700) loss: 1.015518\n",
      "(Iteration 10289 / 14700) loss: 0.733420\n",
      "(Iteration 10290 / 14700) loss: 0.670761\n",
      "(Iteration 10291 / 14700) loss: 0.713397\n",
      "(Iteration 10292 / 14700) loss: 0.586010\n",
      "(Iteration 10293 / 14700) loss: 0.906072\n",
      "(Iteration 10294 / 14700) loss: 0.741467\n",
      "(Iteration 10295 / 14700) loss: 0.607021\n",
      "(Iteration 10296 / 14700) loss: 0.818759\n",
      "(Iteration 10297 / 14700) loss: 0.958203\n",
      "(Iteration 10298 / 14700) loss: 0.719757\n",
      "(Iteration 10299 / 14700) loss: 0.868090\n",
      "(Iteration 10300 / 14700) loss: 0.731093\n",
      "(Iteration 10301 / 14700) loss: 0.880583\n",
      "(Iteration 10302 / 14700) loss: 0.865316\n",
      "(Iteration 10303 / 14700) loss: 0.813503\n",
      "(Iteration 10304 / 14700) loss: 0.680155\n",
      "(Iteration 10305 / 14700) loss: 0.747123\n",
      "(Iteration 10306 / 14700) loss: 0.569481\n",
      "(Iteration 10307 / 14700) loss: 0.878381\n",
      "(Iteration 10308 / 14700) loss: 0.661833\n",
      "(Iteration 10309 / 14700) loss: 0.847620\n",
      "(Iteration 10310 / 14700) loss: 0.662822\n",
      "(Iteration 10311 / 14700) loss: 0.705408\n",
      "(Iteration 10312 / 14700) loss: 0.784607\n",
      "(Iteration 10313 / 14700) loss: 0.769188\n",
      "(Iteration 10314 / 14700) loss: 0.792442\n",
      "(Iteration 10315 / 14700) loss: 0.482946\n",
      "(Iteration 10316 / 14700) loss: 0.743532\n",
      "(Iteration 10317 / 14700) loss: 0.734698\n",
      "(Iteration 10318 / 14700) loss: 0.594364\n",
      "(Iteration 10319 / 14700) loss: 0.904256\n",
      "(Iteration 10320 / 14700) loss: 0.811983\n",
      "(Iteration 10321 / 14700) loss: 0.756706\n",
      "(Iteration 10322 / 14700) loss: 0.926860\n",
      "(Iteration 10323 / 14700) loss: 0.657994\n",
      "(Iteration 10324 / 14700) loss: 0.647451\n",
      "(Iteration 10325 / 14700) loss: 0.727950\n",
      "(Iteration 10326 / 14700) loss: 0.862042\n",
      "(Iteration 10327 / 14700) loss: 0.551458\n",
      "(Iteration 10328 / 14700) loss: 0.602000\n",
      "(Iteration 10329 / 14700) loss: 0.577517\n",
      "(Iteration 10330 / 14700) loss: 0.709950\n",
      "(Iteration 10331 / 14700) loss: 0.717648\n",
      "(Iteration 10332 / 14700) loss: 0.773911\n",
      "(Iteration 10333 / 14700) loss: 0.866931\n",
      "(Iteration 10334 / 14700) loss: 0.694229\n",
      "(Iteration 10335 / 14700) loss: 0.569493\n",
      "(Iteration 10336 / 14700) loss: 0.987980\n",
      "(Iteration 10337 / 14700) loss: 0.690258\n",
      "(Iteration 10338 / 14700) loss: 0.783133\n",
      "(Iteration 10339 / 14700) loss: 0.623148\n",
      "(Iteration 10340 / 14700) loss: 0.467620\n",
      "(Iteration 10341 / 14700) loss: 0.957534\n",
      "(Iteration 10342 / 14700) loss: 0.722543\n",
      "(Iteration 10343 / 14700) loss: 0.751757\n",
      "(Iteration 10344 / 14700) loss: 0.779533\n",
      "(Iteration 10345 / 14700) loss: 0.614076\n",
      "(Iteration 10346 / 14700) loss: 0.916174\n",
      "(Iteration 10347 / 14700) loss: 0.830665\n",
      "(Iteration 10348 / 14700) loss: 0.670628\n",
      "(Iteration 10349 / 14700) loss: 0.646489\n",
      "(Iteration 10350 / 14700) loss: 0.659723\n",
      "(Iteration 10351 / 14700) loss: 0.909521\n",
      "(Iteration 10352 / 14700) loss: 0.603014\n",
      "(Iteration 10353 / 14700) loss: 0.651949\n",
      "(Iteration 10354 / 14700) loss: 0.848799\n",
      "(Iteration 10355 / 14700) loss: 0.762551\n",
      "(Iteration 10356 / 14700) loss: 0.804572\n",
      "(Iteration 10357 / 14700) loss: 0.924719\n",
      "(Iteration 10358 / 14700) loss: 0.973703\n",
      "(Iteration 10359 / 14700) loss: 0.623886\n",
      "(Iteration 10360 / 14700) loss: 0.474997\n",
      "(Iteration 10361 / 14700) loss: 0.746583\n",
      "(Iteration 10362 / 14700) loss: 0.684896\n",
      "(Iteration 10363 / 14700) loss: 0.606217\n",
      "(Iteration 10364 / 14700) loss: 0.725918\n",
      "(Iteration 10365 / 14700) loss: 0.693599\n",
      "(Iteration 10366 / 14700) loss: 0.826150\n",
      "(Iteration 10367 / 14700) loss: 0.791083\n",
      "(Iteration 10368 / 14700) loss: 0.824336\n",
      "(Iteration 10369 / 14700) loss: 0.730855\n",
      "(Iteration 10370 / 14700) loss: 0.516771\n",
      "(Iteration 10371 / 14700) loss: 0.602053\n",
      "(Iteration 10372 / 14700) loss: 0.844607\n",
      "(Iteration 10373 / 14700) loss: 0.757063\n",
      "(Iteration 10374 / 14700) loss: 0.924905\n",
      "(Iteration 10375 / 14700) loss: 0.899517\n",
      "(Iteration 10376 / 14700) loss: 0.717802\n",
      "(Iteration 10377 / 14700) loss: 0.679237\n",
      "(Iteration 10378 / 14700) loss: 0.500144\n",
      "(Iteration 10379 / 14700) loss: 0.632301\n",
      "(Iteration 10380 / 14700) loss: 0.863085\n",
      "(Iteration 10381 / 14700) loss: 0.525218\n",
      "(Iteration 10382 / 14700) loss: 0.646656\n",
      "(Iteration 10383 / 14700) loss: 0.723665\n",
      "(Iteration 10384 / 14700) loss: 0.683088\n",
      "(Iteration 10385 / 14700) loss: 0.629222\n",
      "(Iteration 10386 / 14700) loss: 0.576356\n",
      "(Iteration 10387 / 14700) loss: 0.876318\n",
      "(Iteration 10388 / 14700) loss: 0.686200\n",
      "(Iteration 10389 / 14700) loss: 0.510761\n",
      "(Iteration 10390 / 14700) loss: 0.870997\n",
      "(Iteration 10391 / 14700) loss: 0.798237\n",
      "(Iteration 10392 / 14700) loss: 0.904918\n",
      "(Iteration 10393 / 14700) loss: 1.006046\n",
      "(Iteration 10394 / 14700) loss: 0.613049\n",
      "(Iteration 10395 / 14700) loss: 0.715240\n",
      "(Iteration 10396 / 14700) loss: 0.676381\n",
      "(Iteration 10397 / 14700) loss: 0.602958\n",
      "(Iteration 10398 / 14700) loss: 0.564997\n",
      "(Iteration 10399 / 14700) loss: 0.630102\n",
      "(Iteration 10400 / 14700) loss: 1.013089\n",
      "(Iteration 10401 / 14700) loss: 0.815493\n",
      "(Iteration 10402 / 14700) loss: 0.628228\n",
      "(Iteration 10403 / 14700) loss: 0.880054\n",
      "(Iteration 10404 / 14700) loss: 0.723463\n",
      "(Iteration 10405 / 14700) loss: 0.731098\n",
      "(Iteration 10406 / 14700) loss: 0.887589\n",
      "(Iteration 10407 / 14700) loss: 0.878589\n",
      "(Iteration 10408 / 14700) loss: 0.739086\n",
      "(Iteration 10409 / 14700) loss: 0.697480\n",
      "(Iteration 10410 / 14700) loss: 0.547127\n",
      "(Iteration 10411 / 14700) loss: 0.692002\n",
      "(Iteration 10412 / 14700) loss: 0.715271\n",
      "(Iteration 10413 / 14700) loss: 0.671276\n",
      "(Iteration 10414 / 14700) loss: 0.602422\n",
      "(Iteration 10415 / 14700) loss: 0.632498\n",
      "(Iteration 10416 / 14700) loss: 0.686640\n",
      "(Iteration 10417 / 14700) loss: 0.829349\n",
      "(Iteration 10418 / 14700) loss: 0.653031\n",
      "(Iteration 10419 / 14700) loss: 0.890625\n",
      "(Iteration 10420 / 14700) loss: 0.719338\n",
      "(Iteration 10421 / 14700) loss: 0.661974\n",
      "(Iteration 10422 / 14700) loss: 0.944887\n",
      "(Iteration 10423 / 14700) loss: 0.995799\n",
      "(Iteration 10424 / 14700) loss: 0.608591\n",
      "(Iteration 10425 / 14700) loss: 0.646494\n",
      "(Iteration 10426 / 14700) loss: 0.793705\n",
      "(Iteration 10427 / 14700) loss: 0.702678\n",
      "(Iteration 10428 / 14700) loss: 0.723619\n",
      "(Iteration 10429 / 14700) loss: 0.759034\n",
      "(Iteration 10430 / 14700) loss: 0.749728\n",
      "(Iteration 10431 / 14700) loss: 0.548631\n",
      "(Iteration 10432 / 14700) loss: 0.590924\n",
      "(Iteration 10433 / 14700) loss: 0.615889\n",
      "(Iteration 10434 / 14700) loss: 0.576496\n",
      "(Iteration 10435 / 14700) loss: 0.963134\n",
      "(Iteration 10436 / 14700) loss: 0.778653\n",
      "(Iteration 10437 / 14700) loss: 0.683608\n",
      "(Iteration 10438 / 14700) loss: 0.673345\n",
      "(Iteration 10439 / 14700) loss: 0.673981\n",
      "(Iteration 10440 / 14700) loss: 0.531378\n",
      "(Iteration 10441 / 14700) loss: 0.710500\n",
      "(Iteration 10442 / 14700) loss: 1.138316\n",
      "(Iteration 10443 / 14700) loss: 0.971193\n",
      "(Iteration 10444 / 14700) loss: 0.563876\n",
      "(Iteration 10445 / 14700) loss: 0.879184\n",
      "(Iteration 10446 / 14700) loss: 0.965325\n",
      "(Iteration 10447 / 14700) loss: 0.892516\n",
      "(Iteration 10448 / 14700) loss: 0.663626\n",
      "(Iteration 10449 / 14700) loss: 0.477954\n",
      "(Iteration 10450 / 14700) loss: 0.518008\n",
      "(Iteration 10451 / 14700) loss: 0.789341\n",
      "(Iteration 10452 / 14700) loss: 0.596977\n",
      "(Iteration 10453 / 14700) loss: 0.886257\n",
      "(Iteration 10454 / 14700) loss: 0.810509\n",
      "(Iteration 10455 / 14700) loss: 0.808898\n",
      "(Iteration 10456 / 14700) loss: 0.828691\n",
      "(Iteration 10457 / 14700) loss: 0.799237\n",
      "(Iteration 10458 / 14700) loss: 0.955052\n",
      "(Iteration 10459 / 14700) loss: 0.581106\n",
      "(Iteration 10460 / 14700) loss: 0.926620\n",
      "(Iteration 10461 / 14700) loss: 0.580473\n",
      "(Iteration 10462 / 14700) loss: 0.663185\n",
      "(Iteration 10463 / 14700) loss: 0.602002\n",
      "(Iteration 10464 / 14700) loss: 0.671677\n",
      "(Iteration 10465 / 14700) loss: 0.812409\n",
      "(Iteration 10466 / 14700) loss: 0.649714\n",
      "(Iteration 10467 / 14700) loss: 0.559068\n",
      "(Iteration 10468 / 14700) loss: 0.589865\n",
      "(Iteration 10469 / 14700) loss: 0.722304\n",
      "(Iteration 10470 / 14700) loss: 0.639586\n",
      "(Iteration 10471 / 14700) loss: 0.549343\n",
      "(Iteration 10472 / 14700) loss: 0.982684\n",
      "(Iteration 10473 / 14700) loss: 0.672962\n",
      "(Iteration 10474 / 14700) loss: 0.930060\n",
      "(Iteration 10475 / 14700) loss: 0.756821\n",
      "(Iteration 10476 / 14700) loss: 0.834542\n",
      "(Iteration 10477 / 14700) loss: 0.684147\n",
      "(Iteration 10478 / 14700) loss: 0.695953\n",
      "(Iteration 10479 / 14700) loss: 0.768932\n",
      "(Iteration 10480 / 14700) loss: 0.700302\n",
      "(Iteration 10481 / 14700) loss: 0.667841\n",
      "(Iteration 10482 / 14700) loss: 0.828888\n",
      "(Iteration 10483 / 14700) loss: 0.677119\n",
      "(Iteration 10484 / 14700) loss: 1.012485\n",
      "(Iteration 10485 / 14700) loss: 0.884532\n",
      "(Iteration 10486 / 14700) loss: 0.886806\n",
      "(Iteration 10487 / 14700) loss: 0.631041\n",
      "(Iteration 10488 / 14700) loss: 0.850737\n",
      "(Iteration 10489 / 14700) loss: 0.692327\n",
      "(Iteration 10490 / 14700) loss: 0.698290\n",
      "(Iteration 10491 / 14700) loss: 0.653825\n",
      "(Iteration 10492 / 14700) loss: 0.681310\n",
      "(Iteration 10493 / 14700) loss: 0.743073\n",
      "(Iteration 10494 / 14700) loss: 0.681696\n",
      "(Iteration 10495 / 14700) loss: 0.828334\n",
      "(Iteration 10496 / 14700) loss: 0.729196\n",
      "(Iteration 10497 / 14700) loss: 0.736992\n",
      "(Iteration 10498 / 14700) loss: 0.763052\n",
      "(Iteration 10499 / 14700) loss: 0.523678\n",
      "(Iteration 10500 / 14700) loss: 0.655044\n",
      "(Iteration 10501 / 14700) loss: 0.843599\n",
      "(Iteration 10502 / 14700) loss: 1.033802\n",
      "(Iteration 10503 / 14700) loss: 0.715635\n",
      "(Iteration 10504 / 14700) loss: 0.774659\n",
      "(Iteration 10505 / 14700) loss: 0.768833\n",
      "(Iteration 10506 / 14700) loss: 0.812396\n",
      "(Iteration 10507 / 14700) loss: 0.865701\n",
      "(Iteration 10508 / 14700) loss: 0.514244\n",
      "(Iteration 10509 / 14700) loss: 0.610010\n",
      "(Iteration 10510 / 14700) loss: 0.719333\n",
      "(Iteration 10511 / 14700) loss: 0.695947\n",
      "(Iteration 10512 / 14700) loss: 0.980491\n",
      "(Iteration 10513 / 14700) loss: 0.509949\n",
      "(Iteration 10514 / 14700) loss: 1.132980\n",
      "(Iteration 10515 / 14700) loss: 1.115080\n",
      "(Iteration 10516 / 14700) loss: 0.695981\n",
      "(Iteration 10517 / 14700) loss: 0.697756\n",
      "(Iteration 10518 / 14700) loss: 0.720327\n",
      "(Iteration 10519 / 14700) loss: 0.557288\n",
      "(Iteration 10520 / 14700) loss: 0.875350\n",
      "(Iteration 10521 / 14700) loss: 0.605393\n",
      "(Iteration 10522 / 14700) loss: 0.788748\n",
      "(Iteration 10523 / 14700) loss: 0.791140\n",
      "(Iteration 10524 / 14700) loss: 0.851169\n",
      "(Iteration 10525 / 14700) loss: 0.765945\n",
      "(Iteration 10526 / 14700) loss: 0.684662\n",
      "(Iteration 10527 / 14700) loss: 0.647278\n",
      "(Iteration 10528 / 14700) loss: 0.634252\n",
      "(Iteration 10529 / 14700) loss: 0.684636\n",
      "(Iteration 10530 / 14700) loss: 0.811720\n",
      "(Iteration 10531 / 14700) loss: 0.608675\n",
      "(Iteration 10532 / 14700) loss: 0.659192\n",
      "(Iteration 10533 / 14700) loss: 0.805351\n",
      "(Iteration 10534 / 14700) loss: 0.684916\n",
      "(Iteration 10535 / 14700) loss: 0.887491\n",
      "(Iteration 10536 / 14700) loss: 0.882108\n",
      "(Iteration 10537 / 14700) loss: 0.823274\n",
      "(Iteration 10538 / 14700) loss: 0.543494\n",
      "(Iteration 10539 / 14700) loss: 0.662642\n",
      "(Iteration 10540 / 14700) loss: 0.740617\n",
      "(Iteration 10541 / 14700) loss: 0.662724\n",
      "(Iteration 10542 / 14700) loss: 0.735416\n",
      "(Iteration 10543 / 14700) loss: 0.795769\n",
      "(Iteration 10544 / 14700) loss: 0.683104\n",
      "(Iteration 10545 / 14700) loss: 0.557792\n",
      "(Iteration 10546 / 14700) loss: 0.751453\n",
      "(Iteration 10547 / 14700) loss: 0.761331\n",
      "(Iteration 10548 / 14700) loss: 0.698392\n",
      "(Iteration 10549 / 14700) loss: 0.862365\n",
      "(Iteration 10550 / 14700) loss: 0.587222\n",
      "(Iteration 10551 / 14700) loss: 0.675923\n",
      "(Iteration 10552 / 14700) loss: 0.604516\n",
      "(Iteration 10553 / 14700) loss: 0.778674\n",
      "(Iteration 10554 / 14700) loss: 0.850334\n",
      "(Iteration 10555 / 14700) loss: 0.748927\n",
      "(Iteration 10556 / 14700) loss: 0.535911\n",
      "(Iteration 10557 / 14700) loss: 0.738206\n",
      "(Iteration 10558 / 14700) loss: 0.719411\n",
      "(Iteration 10559 / 14700) loss: 0.760756\n",
      "(Iteration 10560 / 14700) loss: 0.522750\n",
      "(Iteration 10561 / 14700) loss: 0.899392\n",
      "(Iteration 10562 / 14700) loss: 0.914885\n",
      "(Iteration 10563 / 14700) loss: 0.676850\n",
      "(Iteration 10564 / 14700) loss: 0.961122\n",
      "(Iteration 10565 / 14700) loss: 0.695236\n",
      "(Iteration 10566 / 14700) loss: 0.855296\n",
      "(Iteration 10567 / 14700) loss: 0.659104\n",
      "(Iteration 10568 / 14700) loss: 0.717117\n",
      "(Iteration 10569 / 14700) loss: 0.651013\n",
      "(Iteration 10570 / 14700) loss: 0.642328\n",
      "(Iteration 10571 / 14700) loss: 0.789185\n",
      "(Iteration 10572 / 14700) loss: 0.575282\n",
      "(Iteration 10573 / 14700) loss: 0.612202\n",
      "(Iteration 10574 / 14700) loss: 0.642466\n",
      "(Iteration 10575 / 14700) loss: 0.860472\n",
      "(Iteration 10576 / 14700) loss: 0.863957\n",
      "(Iteration 10577 / 14700) loss: 0.770260\n",
      "(Iteration 10578 / 14700) loss: 0.649629\n",
      "(Iteration 10579 / 14700) loss: 0.627038\n",
      "(Iteration 10580 / 14700) loss: 0.745190\n",
      "(Iteration 10581 / 14700) loss: 0.536346\n",
      "(Iteration 10582 / 14700) loss: 0.636628\n",
      "(Iteration 10583 / 14700) loss: 0.728119\n",
      "(Iteration 10584 / 14700) loss: 0.905778\n",
      "(Iteration 10585 / 14700) loss: 0.641424\n",
      "(Iteration 10586 / 14700) loss: 0.543215\n",
      "(Iteration 10587 / 14700) loss: 0.559426\n",
      "(Iteration 10588 / 14700) loss: 0.625772\n",
      "(Iteration 10589 / 14700) loss: 0.687167\n",
      "(Iteration 10590 / 14700) loss: 0.698975\n",
      "(Iteration 10591 / 14700) loss: 0.733273\n",
      "(Iteration 10592 / 14700) loss: 0.719965\n",
      "(Iteration 10593 / 14700) loss: 0.665277\n",
      "(Iteration 10594 / 14700) loss: 0.762553\n",
      "(Iteration 10595 / 14700) loss: 0.600703\n",
      "(Iteration 10596 / 14700) loss: 0.787715\n",
      "(Iteration 10597 / 14700) loss: 0.611643\n",
      "(Iteration 10598 / 14700) loss: 0.563692\n",
      "(Iteration 10599 / 14700) loss: 0.563726\n",
      "(Iteration 10600 / 14700) loss: 0.679581\n",
      "(Iteration 10601 / 14700) loss: 0.970230\n",
      "(Iteration 10602 / 14700) loss: 0.626725\n",
      "(Iteration 10603 / 14700) loss: 0.652058\n",
      "(Iteration 10604 / 14700) loss: 0.765506\n",
      "(Iteration 10605 / 14700) loss: 0.691949\n",
      "(Iteration 10606 / 14700) loss: 0.753785\n",
      "(Iteration 10607 / 14700) loss: 0.865272\n",
      "(Iteration 10608 / 14700) loss: 0.547261\n",
      "(Iteration 10609 / 14700) loss: 0.812175\n",
      "(Iteration 10610 / 14700) loss: 0.922545\n",
      "(Iteration 10611 / 14700) loss: 0.595340\n",
      "(Iteration 10612 / 14700) loss: 0.736111\n",
      "(Iteration 10613 / 14700) loss: 0.675934\n",
      "(Iteration 10614 / 14700) loss: 0.833257\n",
      "(Iteration 10615 / 14700) loss: 0.499766\n",
      "(Iteration 10616 / 14700) loss: 0.809092\n",
      "(Iteration 10617 / 14700) loss: 0.768812\n",
      "(Iteration 10618 / 14700) loss: 0.611765\n",
      "(Iteration 10619 / 14700) loss: 0.647173\n",
      "(Iteration 10620 / 14700) loss: 0.687165\n",
      "(Iteration 10621 / 14700) loss: 0.850147\n",
      "(Iteration 10622 / 14700) loss: 0.670618\n",
      "(Iteration 10623 / 14700) loss: 0.889911\n",
      "(Iteration 10624 / 14700) loss: 0.770153\n",
      "(Iteration 10625 / 14700) loss: 0.610912\n",
      "(Iteration 10626 / 14700) loss: 0.728705\n",
      "(Iteration 10627 / 14700) loss: 0.784774\n",
      "(Iteration 10628 / 14700) loss: 0.700209\n",
      "(Iteration 10629 / 14700) loss: 0.918590\n",
      "(Iteration 10630 / 14700) loss: 0.667306\n",
      "(Iteration 10631 / 14700) loss: 0.762980\n",
      "(Iteration 10632 / 14700) loss: 0.662515\n",
      "(Iteration 10633 / 14700) loss: 0.479046\n",
      "(Iteration 10634 / 14700) loss: 0.561305\n",
      "(Iteration 10635 / 14700) loss: 0.929737\n",
      "(Iteration 10636 / 14700) loss: 0.618325\n",
      "(Iteration 10637 / 14700) loss: 0.871294\n",
      "(Iteration 10638 / 14700) loss: 0.642509\n",
      "(Iteration 10639 / 14700) loss: 0.608728\n",
      "(Iteration 10640 / 14700) loss: 0.852179\n",
      "(Iteration 10641 / 14700) loss: 0.579350\n",
      "(Iteration 10642 / 14700) loss: 0.771933\n",
      "(Iteration 10643 / 14700) loss: 0.672180\n",
      "(Iteration 10644 / 14700) loss: 0.542585\n",
      "(Iteration 10645 / 14700) loss: 0.918221\n",
      "(Iteration 10646 / 14700) loss: 0.839027\n",
      "(Iteration 10647 / 14700) loss: 0.559558\n",
      "(Iteration 10648 / 14700) loss: 0.671064\n",
      "(Iteration 10649 / 14700) loss: 0.641301\n",
      "(Iteration 10650 / 14700) loss: 0.668231\n",
      "(Iteration 10651 / 14700) loss: 0.783762\n",
      "(Iteration 10652 / 14700) loss: 0.702452\n",
      "(Iteration 10653 / 14700) loss: 0.973634\n",
      "(Iteration 10654 / 14700) loss: 0.554749\n",
      "(Iteration 10655 / 14700) loss: 0.680360\n",
      "(Iteration 10656 / 14700) loss: 0.610044\n",
      "(Iteration 10657 / 14700) loss: 0.983314\n",
      "(Iteration 10658 / 14700) loss: 0.568967\n",
      "(Iteration 10659 / 14700) loss: 0.644154\n",
      "(Iteration 10660 / 14700) loss: 0.934623\n",
      "(Iteration 10661 / 14700) loss: 0.808356\n",
      "(Iteration 10662 / 14700) loss: 0.810037\n",
      "(Iteration 10663 / 14700) loss: 0.932195\n",
      "(Iteration 10664 / 14700) loss: 0.817020\n",
      "(Iteration 10665 / 14700) loss: 0.964960\n",
      "(Iteration 10666 / 14700) loss: 0.749742\n",
      "(Iteration 10667 / 14700) loss: 0.801330\n",
      "(Iteration 10668 / 14700) loss: 0.773409\n",
      "(Iteration 10669 / 14700) loss: 0.681488\n",
      "(Iteration 10670 / 14700) loss: 0.794108\n",
      "(Iteration 10671 / 14700) loss: 0.425477\n",
      "(Iteration 10672 / 14700) loss: 0.832270\n",
      "(Iteration 10673 / 14700) loss: 0.747827\n",
      "(Iteration 10674 / 14700) loss: 0.889634\n",
      "(Iteration 10675 / 14700) loss: 0.878586\n",
      "(Iteration 10676 / 14700) loss: 0.944312\n",
      "(Iteration 10677 / 14700) loss: 0.527709\n",
      "(Iteration 10678 / 14700) loss: 0.799421\n",
      "(Iteration 10679 / 14700) loss: 0.874861\n",
      "(Iteration 10680 / 14700) loss: 0.807679\n",
      "(Iteration 10681 / 14700) loss: 0.697259\n",
      "(Iteration 10682 / 14700) loss: 0.662052\n",
      "(Iteration 10683 / 14700) loss: 0.664361\n",
      "(Iteration 10684 / 14700) loss: 0.909454\n",
      "(Iteration 10685 / 14700) loss: 0.581374\n",
      "(Iteration 10686 / 14700) loss: 0.949096\n",
      "(Iteration 10687 / 14700) loss: 0.710046\n",
      "(Iteration 10688 / 14700) loss: 0.716541\n",
      "(Iteration 10689 / 14700) loss: 1.013833\n",
      "(Iteration 10690 / 14700) loss: 0.772215\n",
      "(Iteration 10691 / 14700) loss: 0.755137\n",
      "(Iteration 10692 / 14700) loss: 0.669085\n",
      "(Iteration 10693 / 14700) loss: 0.708605\n",
      "(Iteration 10694 / 14700) loss: 0.777935\n",
      "(Iteration 10695 / 14700) loss: 0.620871\n",
      "(Iteration 10696 / 14700) loss: 0.845315\n",
      "(Iteration 10697 / 14700) loss: 0.614045\n",
      "(Iteration 10698 / 14700) loss: 0.998859\n",
      "(Iteration 10699 / 14700) loss: 0.585081\n",
      "(Iteration 10700 / 14700) loss: 0.634995\n",
      "(Iteration 10701 / 14700) loss: 1.056472\n",
      "(Iteration 10702 / 14700) loss: 0.689542\n",
      "(Iteration 10703 / 14700) loss: 0.941581\n",
      "(Iteration 10704 / 14700) loss: 0.688391\n",
      "(Iteration 10705 / 14700) loss: 0.934083\n",
      "(Iteration 10706 / 14700) loss: 0.771224\n",
      "(Iteration 10707 / 14700) loss: 0.648535\n",
      "(Iteration 10708 / 14700) loss: 0.741141\n",
      "(Iteration 10709 / 14700) loss: 0.721282\n",
      "(Iteration 10710 / 14700) loss: 0.714146\n",
      "(Iteration 10711 / 14700) loss: 0.601770\n",
      "(Iteration 10712 / 14700) loss: 0.614157\n",
      "(Iteration 10713 / 14700) loss: 0.668803\n",
      "(Iteration 10714 / 14700) loss: 0.543577\n",
      "(Iteration 10715 / 14700) loss: 0.645610\n",
      "(Iteration 10716 / 14700) loss: 0.757059\n",
      "(Iteration 10717 / 14700) loss: 0.734459\n",
      "(Iteration 10718 / 14700) loss: 1.088551\n",
      "(Iteration 10719 / 14700) loss: 0.591396\n",
      "(Iteration 10720 / 14700) loss: 0.539234\n",
      "(Iteration 10721 / 14700) loss: 0.685952\n",
      "(Iteration 10722 / 14700) loss: 0.661439\n",
      "(Iteration 10723 / 14700) loss: 0.632361\n",
      "(Iteration 10724 / 14700) loss: 0.821229\n",
      "(Iteration 10725 / 14700) loss: 0.646815\n",
      "(Iteration 10726 / 14700) loss: 0.782501\n",
      "(Iteration 10727 / 14700) loss: 0.610003\n",
      "(Iteration 10728 / 14700) loss: 0.845349\n",
      "(Iteration 10729 / 14700) loss: 0.713578\n",
      "(Iteration 10730 / 14700) loss: 0.565413\n",
      "(Iteration 10731 / 14700) loss: 0.767063\n",
      "(Iteration 10732 / 14700) loss: 0.583655\n",
      "(Iteration 10733 / 14700) loss: 0.774307\n",
      "(Iteration 10734 / 14700) loss: 0.678622\n",
      "(Iteration 10735 / 14700) loss: 0.832635\n",
      "(Iteration 10736 / 14700) loss: 0.803234\n",
      "(Iteration 10737 / 14700) loss: 0.882747\n",
      "(Iteration 10738 / 14700) loss: 0.625368\n",
      "(Iteration 10739 / 14700) loss: 0.694617\n",
      "(Iteration 10740 / 14700) loss: 0.733556\n",
      "(Iteration 10741 / 14700) loss: 0.944316\n",
      "(Iteration 10742 / 14700) loss: 0.639939\n",
      "(Iteration 10743 / 14700) loss: 0.911166\n",
      "(Iteration 10744 / 14700) loss: 0.791541\n",
      "(Iteration 10745 / 14700) loss: 0.696140\n",
      "(Iteration 10746 / 14700) loss: 0.859893\n",
      "(Iteration 10747 / 14700) loss: 0.956187\n",
      "(Iteration 10748 / 14700) loss: 0.696043\n",
      "(Iteration 10749 / 14700) loss: 0.649956\n",
      "(Iteration 10750 / 14700) loss: 0.892593\n",
      "(Iteration 10751 / 14700) loss: 0.813973\n",
      "(Iteration 10752 / 14700) loss: 0.730305\n",
      "(Iteration 10753 / 14700) loss: 0.806178\n",
      "(Iteration 10754 / 14700) loss: 0.694470\n",
      "(Iteration 10755 / 14700) loss: 0.795189\n",
      "(Iteration 10756 / 14700) loss: 0.539242\n",
      "(Iteration 10757 / 14700) loss: 0.781934\n",
      "(Iteration 10758 / 14700) loss: 0.576939\n",
      "(Iteration 10759 / 14700) loss: 0.704782\n",
      "(Iteration 10760 / 14700) loss: 0.765194\n",
      "(Iteration 10761 / 14700) loss: 0.568663\n",
      "(Iteration 10762 / 14700) loss: 0.694603\n",
      "(Iteration 10763 / 14700) loss: 0.646190\n",
      "(Iteration 10764 / 14700) loss: 0.963963\n",
      "(Iteration 10765 / 14700) loss: 0.797009\n",
      "(Iteration 10766 / 14700) loss: 0.788047\n",
      "(Iteration 10767 / 14700) loss: 0.524841\n",
      "(Iteration 10768 / 14700) loss: 0.634655\n",
      "(Iteration 10769 / 14700) loss: 0.774359\n",
      "(Iteration 10770 / 14700) loss: 0.917457\n",
      "(Iteration 10771 / 14700) loss: 0.621234\n",
      "(Iteration 10772 / 14700) loss: 0.659868\n",
      "(Iteration 10773 / 14700) loss: 0.920928\n",
      "(Iteration 10774 / 14700) loss: 0.672795\n",
      "(Iteration 10775 / 14700) loss: 0.903790\n",
      "(Iteration 10776 / 14700) loss: 0.716603\n",
      "(Iteration 10777 / 14700) loss: 0.543614\n",
      "(Iteration 10778 / 14700) loss: 0.782480\n",
      "(Iteration 10779 / 14700) loss: 0.578746\n",
      "(Iteration 10780 / 14700) loss: 0.734362\n",
      "(Epoch 11 / 15) train acc: 0.799000; val_acc: 0.773000\n",
      "(Iteration 10781 / 14700) loss: 0.544471\n",
      "(Iteration 10782 / 14700) loss: 0.680417\n",
      "(Iteration 10783 / 14700) loss: 0.651014\n",
      "(Iteration 10784 / 14700) loss: 0.940422\n",
      "(Iteration 10785 / 14700) loss: 0.759707\n",
      "(Iteration 10786 / 14700) loss: 0.825591\n",
      "(Iteration 10787 / 14700) loss: 0.766483\n",
      "(Iteration 10788 / 14700) loss: 0.722175\n",
      "(Iteration 10789 / 14700) loss: 0.686099\n",
      "(Iteration 10790 / 14700) loss: 0.668815\n",
      "(Iteration 10791 / 14700) loss: 0.723354\n",
      "(Iteration 10792 / 14700) loss: 0.640626\n",
      "(Iteration 10793 / 14700) loss: 0.830794\n",
      "(Iteration 10794 / 14700) loss: 0.603115\n",
      "(Iteration 10795 / 14700) loss: 0.634938\n",
      "(Iteration 10796 / 14700) loss: 0.554166\n",
      "(Iteration 10797 / 14700) loss: 0.736012\n",
      "(Iteration 10798 / 14700) loss: 0.861821\n",
      "(Iteration 10799 / 14700) loss: 0.890895\n",
      "(Iteration 10800 / 14700) loss: 0.672824\n",
      "(Iteration 10801 / 14700) loss: 0.581136\n",
      "(Iteration 10802 / 14700) loss: 0.825205\n",
      "(Iteration 10803 / 14700) loss: 0.493812\n",
      "(Iteration 10804 / 14700) loss: 0.747117\n",
      "(Iteration 10805 / 14700) loss: 0.803539\n",
      "(Iteration 10806 / 14700) loss: 0.745793\n",
      "(Iteration 10807 / 14700) loss: 0.714279\n",
      "(Iteration 10808 / 14700) loss: 0.842316\n",
      "(Iteration 10809 / 14700) loss: 0.825627\n",
      "(Iteration 10810 / 14700) loss: 0.759523\n",
      "(Iteration 10811 / 14700) loss: 0.577779\n",
      "(Iteration 10812 / 14700) loss: 0.566648\n",
      "(Iteration 10813 / 14700) loss: 0.677178\n",
      "(Iteration 10814 / 14700) loss: 0.762704\n",
      "(Iteration 10815 / 14700) loss: 0.940942\n",
      "(Iteration 10816 / 14700) loss: 0.775498\n",
      "(Iteration 10817 / 14700) loss: 0.823133\n",
      "(Iteration 10818 / 14700) loss: 0.656214\n",
      "(Iteration 10819 / 14700) loss: 0.785296\n",
      "(Iteration 10820 / 14700) loss: 0.810712\n",
      "(Iteration 10821 / 14700) loss: 0.817211\n",
      "(Iteration 10822 / 14700) loss: 0.771015\n",
      "(Iteration 10823 / 14700) loss: 0.600536\n",
      "(Iteration 10824 / 14700) loss: 0.666911\n",
      "(Iteration 10825 / 14700) loss: 0.836755\n",
      "(Iteration 10826 / 14700) loss: 0.686363\n",
      "(Iteration 10827 / 14700) loss: 0.799311\n",
      "(Iteration 10828 / 14700) loss: 0.867108\n",
      "(Iteration 10829 / 14700) loss: 0.739218\n",
      "(Iteration 10830 / 14700) loss: 0.763185\n",
      "(Iteration 10831 / 14700) loss: 0.732976\n",
      "(Iteration 10832 / 14700) loss: 0.791332\n",
      "(Iteration 10833 / 14700) loss: 0.541136\n",
      "(Iteration 10834 / 14700) loss: 0.645977\n",
      "(Iteration 10835 / 14700) loss: 0.681285\n",
      "(Iteration 10836 / 14700) loss: 0.959878\n",
      "(Iteration 10837 / 14700) loss: 0.791171\n",
      "(Iteration 10838 / 14700) loss: 0.563818\n",
      "(Iteration 10839 / 14700) loss: 0.892400\n",
      "(Iteration 10840 / 14700) loss: 0.617152\n",
      "(Iteration 10841 / 14700) loss: 0.774221\n",
      "(Iteration 10842 / 14700) loss: 0.947375\n",
      "(Iteration 10843 / 14700) loss: 0.632937\n",
      "(Iteration 10844 / 14700) loss: 0.932134\n",
      "(Iteration 10845 / 14700) loss: 0.822125\n",
      "(Iteration 10846 / 14700) loss: 0.818680\n",
      "(Iteration 10847 / 14700) loss: 0.585557\n",
      "(Iteration 10848 / 14700) loss: 0.621932\n",
      "(Iteration 10849 / 14700) loss: 0.913578\n",
      "(Iteration 10850 / 14700) loss: 0.777158\n",
      "(Iteration 10851 / 14700) loss: 0.899063\n",
      "(Iteration 10852 / 14700) loss: 0.678702\n",
      "(Iteration 10853 / 14700) loss: 0.514213\n",
      "(Iteration 10854 / 14700) loss: 0.749626\n",
      "(Iteration 10855 / 14700) loss: 0.696433\n",
      "(Iteration 10856 / 14700) loss: 0.574384\n",
      "(Iteration 10857 / 14700) loss: 0.805066\n",
      "(Iteration 10858 / 14700) loss: 0.880000\n",
      "(Iteration 10859 / 14700) loss: 0.651488\n",
      "(Iteration 10860 / 14700) loss: 0.637270\n",
      "(Iteration 10861 / 14700) loss: 0.539866\n",
      "(Iteration 10862 / 14700) loss: 0.876774\n",
      "(Iteration 10863 / 14700) loss: 0.576226\n",
      "(Iteration 10864 / 14700) loss: 0.747082\n",
      "(Iteration 10865 / 14700) loss: 0.731286\n",
      "(Iteration 10866 / 14700) loss: 0.711009\n",
      "(Iteration 10867 / 14700) loss: 0.650998\n",
      "(Iteration 10868 / 14700) loss: 0.685233\n",
      "(Iteration 10869 / 14700) loss: 0.884144\n",
      "(Iteration 10870 / 14700) loss: 0.869654\n",
      "(Iteration 10871 / 14700) loss: 0.826123\n",
      "(Iteration 10872 / 14700) loss: 0.739763\n",
      "(Iteration 10873 / 14700) loss: 0.746066\n",
      "(Iteration 10874 / 14700) loss: 0.977949\n",
      "(Iteration 10875 / 14700) loss: 0.912537\n",
      "(Iteration 10876 / 14700) loss: 0.807735\n",
      "(Iteration 10877 / 14700) loss: 0.884194\n",
      "(Iteration 10878 / 14700) loss: 0.566461\n",
      "(Iteration 10879 / 14700) loss: 0.855795\n",
      "(Iteration 10880 / 14700) loss: 1.157133\n",
      "(Iteration 10881 / 14700) loss: 0.679324\n",
      "(Iteration 10882 / 14700) loss: 0.679947\n",
      "(Iteration 10883 / 14700) loss: 0.730092\n",
      "(Iteration 10884 / 14700) loss: 0.681021\n",
      "(Iteration 10885 / 14700) loss: 0.785449\n",
      "(Iteration 10886 / 14700) loss: 0.751469\n",
      "(Iteration 10887 / 14700) loss: 0.849425\n",
      "(Iteration 10888 / 14700) loss: 0.630804\n",
      "(Iteration 10889 / 14700) loss: 0.757330\n",
      "(Iteration 10890 / 14700) loss: 0.694306\n",
      "(Iteration 10891 / 14700) loss: 0.593595\n",
      "(Iteration 10892 / 14700) loss: 0.552786\n",
      "(Iteration 10893 / 14700) loss: 0.945510\n",
      "(Iteration 10894 / 14700) loss: 0.620381\n",
      "(Iteration 10895 / 14700) loss: 0.931050\n",
      "(Iteration 10896 / 14700) loss: 0.705384\n",
      "(Iteration 10897 / 14700) loss: 0.676130\n",
      "(Iteration 10898 / 14700) loss: 0.712715\n",
      "(Iteration 10899 / 14700) loss: 0.815666\n",
      "(Iteration 10900 / 14700) loss: 0.978954\n",
      "(Iteration 10901 / 14700) loss: 0.742000\n",
      "(Iteration 10902 / 14700) loss: 0.619123\n",
      "(Iteration 10903 / 14700) loss: 0.692640\n",
      "(Iteration 10904 / 14700) loss: 0.652660\n",
      "(Iteration 10905 / 14700) loss: 0.699578\n",
      "(Iteration 10906 / 14700) loss: 0.675295\n",
      "(Iteration 10907 / 14700) loss: 0.591552\n",
      "(Iteration 10908 / 14700) loss: 0.732324\n",
      "(Iteration 10909 / 14700) loss: 0.671222\n",
      "(Iteration 10910 / 14700) loss: 0.780024\n",
      "(Iteration 10911 / 14700) loss: 0.593589\n",
      "(Iteration 10912 / 14700) loss: 0.868254\n",
      "(Iteration 10913 / 14700) loss: 1.060739\n",
      "(Iteration 10914 / 14700) loss: 0.853929\n",
      "(Iteration 10915 / 14700) loss: 0.824794\n",
      "(Iteration 10916 / 14700) loss: 0.611673\n",
      "(Iteration 10917 / 14700) loss: 0.577725\n",
      "(Iteration 10918 / 14700) loss: 0.966546\n",
      "(Iteration 10919 / 14700) loss: 0.801042\n",
      "(Iteration 10920 / 14700) loss: 0.586533\n",
      "(Iteration 10921 / 14700) loss: 0.725682\n",
      "(Iteration 10922 / 14700) loss: 0.727755\n",
      "(Iteration 10923 / 14700) loss: 0.752733\n",
      "(Iteration 10924 / 14700) loss: 0.957128\n",
      "(Iteration 10925 / 14700) loss: 0.748145\n",
      "(Iteration 10926 / 14700) loss: 0.715948\n",
      "(Iteration 10927 / 14700) loss: 0.711643\n",
      "(Iteration 10928 / 14700) loss: 0.552157\n",
      "(Iteration 10929 / 14700) loss: 0.914340\n",
      "(Iteration 10930 / 14700) loss: 1.003733\n",
      "(Iteration 10931 / 14700) loss: 0.675944\n",
      "(Iteration 10932 / 14700) loss: 0.769251\n",
      "(Iteration 10933 / 14700) loss: 0.703046\n",
      "(Iteration 10934 / 14700) loss: 0.800004\n",
      "(Iteration 10935 / 14700) loss: 0.543369\n",
      "(Iteration 10936 / 14700) loss: 0.819541\n",
      "(Iteration 10937 / 14700) loss: 0.574345\n",
      "(Iteration 10938 / 14700) loss: 0.468180\n",
      "(Iteration 10939 / 14700) loss: 0.843714\n",
      "(Iteration 10940 / 14700) loss: 0.588908\n",
      "(Iteration 10941 / 14700) loss: 0.842527\n",
      "(Iteration 10942 / 14700) loss: 0.645455\n",
      "(Iteration 10943 / 14700) loss: 0.597542\n",
      "(Iteration 10944 / 14700) loss: 0.628314\n",
      "(Iteration 10945 / 14700) loss: 0.614388\n",
      "(Iteration 10946 / 14700) loss: 0.483491\n",
      "(Iteration 10947 / 14700) loss: 0.546737\n",
      "(Iteration 10948 / 14700) loss: 0.590824\n",
      "(Iteration 10949 / 14700) loss: 0.759591\n",
      "(Iteration 10950 / 14700) loss: 0.737605\n",
      "(Iteration 10951 / 14700) loss: 0.760176\n",
      "(Iteration 10952 / 14700) loss: 0.660754\n",
      "(Iteration 10953 / 14700) loss: 0.901695\n",
      "(Iteration 10954 / 14700) loss: 0.673727\n",
      "(Iteration 10955 / 14700) loss: 0.753009\n",
      "(Iteration 10956 / 14700) loss: 0.600147\n",
      "(Iteration 10957 / 14700) loss: 0.789448\n",
      "(Iteration 10958 / 14700) loss: 0.597187\n",
      "(Iteration 10959 / 14700) loss: 0.635715\n",
      "(Iteration 10960 / 14700) loss: 0.800860\n",
      "(Iteration 10961 / 14700) loss: 0.851728\n",
      "(Iteration 10962 / 14700) loss: 0.791367\n",
      "(Iteration 10963 / 14700) loss: 0.749130\n",
      "(Iteration 10964 / 14700) loss: 0.478396\n",
      "(Iteration 10965 / 14700) loss: 0.623681\n",
      "(Iteration 10966 / 14700) loss: 0.646873\n",
      "(Iteration 10967 / 14700) loss: 0.987663\n",
      "(Iteration 10968 / 14700) loss: 0.881781\n",
      "(Iteration 10969 / 14700) loss: 0.671241\n",
      "(Iteration 10970 / 14700) loss: 1.006052\n",
      "(Iteration 10971 / 14700) loss: 0.841379\n",
      "(Iteration 10972 / 14700) loss: 0.660118\n",
      "(Iteration 10973 / 14700) loss: 0.815238\n",
      "(Iteration 10974 / 14700) loss: 0.656737\n",
      "(Iteration 10975 / 14700) loss: 0.703511\n",
      "(Iteration 10976 / 14700) loss: 0.914638\n",
      "(Iteration 10977 / 14700) loss: 0.796448\n",
      "(Iteration 10978 / 14700) loss: 0.644738\n",
      "(Iteration 10979 / 14700) loss: 0.811270\n",
      "(Iteration 10980 / 14700) loss: 0.681360\n",
      "(Iteration 10981 / 14700) loss: 0.731822\n",
      "(Iteration 10982 / 14700) loss: 0.869133\n",
      "(Iteration 10983 / 14700) loss: 0.892594\n",
      "(Iteration 10984 / 14700) loss: 0.973234\n",
      "(Iteration 10985 / 14700) loss: 0.689213\n",
      "(Iteration 10986 / 14700) loss: 0.778864\n",
      "(Iteration 10987 / 14700) loss: 0.771411\n",
      "(Iteration 10988 / 14700) loss: 0.897562\n",
      "(Iteration 10989 / 14700) loss: 0.773159\n",
      "(Iteration 10990 / 14700) loss: 0.791778\n",
      "(Iteration 10991 / 14700) loss: 0.930978\n",
      "(Iteration 10992 / 14700) loss: 0.667973\n",
      "(Iteration 10993 / 14700) loss: 0.586416\n",
      "(Iteration 10994 / 14700) loss: 0.912151\n",
      "(Iteration 10995 / 14700) loss: 0.831967\n",
      "(Iteration 10996 / 14700) loss: 0.625548\n",
      "(Iteration 10997 / 14700) loss: 0.795673\n",
      "(Iteration 10998 / 14700) loss: 0.576205\n",
      "(Iteration 10999 / 14700) loss: 0.615108\n",
      "(Iteration 11000 / 14700) loss: 0.642811\n",
      "(Iteration 11001 / 14700) loss: 0.719612\n",
      "(Iteration 11002 / 14700) loss: 0.686259\n",
      "(Iteration 11003 / 14700) loss: 0.672628\n",
      "(Iteration 11004 / 14700) loss: 0.818985\n",
      "(Iteration 11005 / 14700) loss: 1.009705\n",
      "(Iteration 11006 / 14700) loss: 0.630503\n",
      "(Iteration 11007 / 14700) loss: 0.505234\n",
      "(Iteration 11008 / 14700) loss: 0.570205\n",
      "(Iteration 11009 / 14700) loss: 0.620582\n",
      "(Iteration 11010 / 14700) loss: 0.949909\n",
      "(Iteration 11011 / 14700) loss: 0.621560\n",
      "(Iteration 11012 / 14700) loss: 0.786768\n",
      "(Iteration 11013 / 14700) loss: 0.875920\n",
      "(Iteration 11014 / 14700) loss: 0.888975\n",
      "(Iteration 11015 / 14700) loss: 0.649591\n",
      "(Iteration 11016 / 14700) loss: 0.720219\n",
      "(Iteration 11017 / 14700) loss: 0.752563\n",
      "(Iteration 11018 / 14700) loss: 0.673603\n",
      "(Iteration 11019 / 14700) loss: 0.730878\n",
      "(Iteration 11020 / 14700) loss: 0.580887\n",
      "(Iteration 11021 / 14700) loss: 0.744613\n",
      "(Iteration 11022 / 14700) loss: 0.567233\n",
      "(Iteration 11023 / 14700) loss: 1.048537\n",
      "(Iteration 11024 / 14700) loss: 0.561744\n",
      "(Iteration 11025 / 14700) loss: 0.634657\n",
      "(Iteration 11026 / 14700) loss: 0.955089\n",
      "(Iteration 11027 / 14700) loss: 0.672887\n",
      "(Iteration 11028 / 14700) loss: 0.968227\n",
      "(Iteration 11029 / 14700) loss: 0.576231\n",
      "(Iteration 11030 / 14700) loss: 0.705203\n",
      "(Iteration 11031 / 14700) loss: 0.540353\n",
      "(Iteration 11032 / 14700) loss: 0.506619\n",
      "(Iteration 11033 / 14700) loss: 0.821003\n",
      "(Iteration 11034 / 14700) loss: 0.859002\n",
      "(Iteration 11035 / 14700) loss: 0.686950\n",
      "(Iteration 11036 / 14700) loss: 0.665089\n",
      "(Iteration 11037 / 14700) loss: 0.601139\n",
      "(Iteration 11038 / 14700) loss: 0.752824\n",
      "(Iteration 11039 / 14700) loss: 0.751348\n",
      "(Iteration 11040 / 14700) loss: 0.917537\n",
      "(Iteration 11041 / 14700) loss: 0.779455\n",
      "(Iteration 11042 / 14700) loss: 0.856838\n",
      "(Iteration 11043 / 14700) loss: 0.824043\n",
      "(Iteration 11044 / 14700) loss: 0.690172\n",
      "(Iteration 11045 / 14700) loss: 0.780597\n",
      "(Iteration 11046 / 14700) loss: 1.028275\n",
      "(Iteration 11047 / 14700) loss: 0.854286\n",
      "(Iteration 11048 / 14700) loss: 0.955480\n",
      "(Iteration 11049 / 14700) loss: 0.569129\n",
      "(Iteration 11050 / 14700) loss: 0.546906\n",
      "(Iteration 11051 / 14700) loss: 0.641396\n",
      "(Iteration 11052 / 14700) loss: 0.770135\n",
      "(Iteration 11053 / 14700) loss: 0.859227\n",
      "(Iteration 11054 / 14700) loss: 0.528007\n",
      "(Iteration 11055 / 14700) loss: 0.637664\n",
      "(Iteration 11056 / 14700) loss: 0.778118\n",
      "(Iteration 11057 / 14700) loss: 0.726415\n",
      "(Iteration 11058 / 14700) loss: 0.863047\n",
      "(Iteration 11059 / 14700) loss: 0.566017\n",
      "(Iteration 11060 / 14700) loss: 0.876684\n",
      "(Iteration 11061 / 14700) loss: 0.745930\n",
      "(Iteration 11062 / 14700) loss: 0.780518\n",
      "(Iteration 11063 / 14700) loss: 0.661990\n",
      "(Iteration 11064 / 14700) loss: 0.891511\n",
      "(Iteration 11065 / 14700) loss: 0.700830\n",
      "(Iteration 11066 / 14700) loss: 0.785492\n",
      "(Iteration 11067 / 14700) loss: 0.650402\n",
      "(Iteration 11068 / 14700) loss: 0.869185\n",
      "(Iteration 11069 / 14700) loss: 0.813923\n",
      "(Iteration 11070 / 14700) loss: 0.561923\n",
      "(Iteration 11071 / 14700) loss: 0.618104\n",
      "(Iteration 11072 / 14700) loss: 0.777708\n",
      "(Iteration 11073 / 14700) loss: 0.894979\n",
      "(Iteration 11074 / 14700) loss: 0.679852\n",
      "(Iteration 11075 / 14700) loss: 0.611347\n",
      "(Iteration 11076 / 14700) loss: 0.616352\n",
      "(Iteration 11077 / 14700) loss: 0.759500\n",
      "(Iteration 11078 / 14700) loss: 0.850591\n",
      "(Iteration 11079 / 14700) loss: 0.748836\n",
      "(Iteration 11080 / 14700) loss: 0.677393\n",
      "(Iteration 11081 / 14700) loss: 0.844283\n",
      "(Iteration 11082 / 14700) loss: 0.947620\n",
      "(Iteration 11083 / 14700) loss: 0.644694\n",
      "(Iteration 11084 / 14700) loss: 0.538879\n",
      "(Iteration 11085 / 14700) loss: 0.812957\n",
      "(Iteration 11086 / 14700) loss: 0.782332\n",
      "(Iteration 11087 / 14700) loss: 0.595946\n",
      "(Iteration 11088 / 14700) loss: 0.911465\n",
      "(Iteration 11089 / 14700) loss: 0.582820\n",
      "(Iteration 11090 / 14700) loss: 0.784713\n",
      "(Iteration 11091 / 14700) loss: 0.826929\n",
      "(Iteration 11092 / 14700) loss: 0.887640\n",
      "(Iteration 11093 / 14700) loss: 0.964668\n",
      "(Iteration 11094 / 14700) loss: 0.712948\n",
      "(Iteration 11095 / 14700) loss: 0.735965\n",
      "(Iteration 11096 / 14700) loss: 0.652211\n",
      "(Iteration 11097 / 14700) loss: 0.874051\n",
      "(Iteration 11098 / 14700) loss: 0.907753\n",
      "(Iteration 11099 / 14700) loss: 0.702743\n",
      "(Iteration 11100 / 14700) loss: 0.498873\n",
      "(Iteration 11101 / 14700) loss: 0.942718\n",
      "(Iteration 11102 / 14700) loss: 0.591155\n",
      "(Iteration 11103 / 14700) loss: 0.843096\n",
      "(Iteration 11104 / 14700) loss: 0.722589\n",
      "(Iteration 11105 / 14700) loss: 0.795438\n",
      "(Iteration 11106 / 14700) loss: 0.726550\n",
      "(Iteration 11107 / 14700) loss: 0.772409\n",
      "(Iteration 11108 / 14700) loss: 0.867487\n",
      "(Iteration 11109 / 14700) loss: 0.595632\n",
      "(Iteration 11110 / 14700) loss: 0.820984\n",
      "(Iteration 11111 / 14700) loss: 0.624317\n",
      "(Iteration 11112 / 14700) loss: 0.703293\n",
      "(Iteration 11113 / 14700) loss: 0.595457\n",
      "(Iteration 11114 / 14700) loss: 0.827069\n",
      "(Iteration 11115 / 14700) loss: 0.652421\n",
      "(Iteration 11116 / 14700) loss: 0.905163\n",
      "(Iteration 11117 / 14700) loss: 0.741364\n",
      "(Iteration 11118 / 14700) loss: 0.656349\n",
      "(Iteration 11119 / 14700) loss: 0.655755\n",
      "(Iteration 11120 / 14700) loss: 1.017611\n",
      "(Iteration 11121 / 14700) loss: 0.779409\n",
      "(Iteration 11122 / 14700) loss: 0.732496\n",
      "(Iteration 11123 / 14700) loss: 0.729756\n",
      "(Iteration 11124 / 14700) loss: 0.697112\n",
      "(Iteration 11125 / 14700) loss: 0.708854\n",
      "(Iteration 11126 / 14700) loss: 0.756640\n",
      "(Iteration 11127 / 14700) loss: 0.814356\n",
      "(Iteration 11128 / 14700) loss: 0.726631\n",
      "(Iteration 11129 / 14700) loss: 0.388183\n",
      "(Iteration 11130 / 14700) loss: 0.861844\n",
      "(Iteration 11131 / 14700) loss: 0.944422\n",
      "(Iteration 11132 / 14700) loss: 0.536753\n",
      "(Iteration 11133 / 14700) loss: 0.526406\n",
      "(Iteration 11134 / 14700) loss: 0.668337\n",
      "(Iteration 11135 / 14700) loss: 0.586096\n",
      "(Iteration 11136 / 14700) loss: 0.692080\n",
      "(Iteration 11137 / 14700) loss: 0.930342\n",
      "(Iteration 11138 / 14700) loss: 1.059068\n",
      "(Iteration 11139 / 14700) loss: 0.731479\n",
      "(Iteration 11140 / 14700) loss: 0.644759\n",
      "(Iteration 11141 / 14700) loss: 0.701917\n",
      "(Iteration 11142 / 14700) loss: 0.817618\n",
      "(Iteration 11143 / 14700) loss: 0.669649\n",
      "(Iteration 11144 / 14700) loss: 0.600163\n",
      "(Iteration 11145 / 14700) loss: 0.913200\n",
      "(Iteration 11146 / 14700) loss: 0.686295\n",
      "(Iteration 11147 / 14700) loss: 0.665766\n",
      "(Iteration 11148 / 14700) loss: 0.744422\n",
      "(Iteration 11149 / 14700) loss: 0.629559\n",
      "(Iteration 11150 / 14700) loss: 0.702389\n",
      "(Iteration 11151 / 14700) loss: 1.013577\n",
      "(Iteration 11152 / 14700) loss: 0.743390\n",
      "(Iteration 11153 / 14700) loss: 0.544730\n",
      "(Iteration 11154 / 14700) loss: 0.667266\n",
      "(Iteration 11155 / 14700) loss: 0.624366\n",
      "(Iteration 11156 / 14700) loss: 0.877886\n",
      "(Iteration 11157 / 14700) loss: 0.771992\n",
      "(Iteration 11158 / 14700) loss: 0.772637\n",
      "(Iteration 11159 / 14700) loss: 0.772355\n",
      "(Iteration 11160 / 14700) loss: 0.897803\n",
      "(Iteration 11161 / 14700) loss: 0.701631\n",
      "(Iteration 11162 / 14700) loss: 0.847310\n",
      "(Iteration 11163 / 14700) loss: 0.781261\n",
      "(Iteration 11164 / 14700) loss: 0.776935\n",
      "(Iteration 11165 / 14700) loss: 0.644675\n",
      "(Iteration 11166 / 14700) loss: 0.646006\n",
      "(Iteration 11167 / 14700) loss: 0.784849\n",
      "(Iteration 11168 / 14700) loss: 0.783646\n",
      "(Iteration 11169 / 14700) loss: 0.684187\n",
      "(Iteration 11170 / 14700) loss: 0.739037\n",
      "(Iteration 11171 / 14700) loss: 0.720721\n",
      "(Iteration 11172 / 14700) loss: 0.622592\n",
      "(Iteration 11173 / 14700) loss: 0.735862\n",
      "(Iteration 11174 / 14700) loss: 0.654162\n",
      "(Iteration 11175 / 14700) loss: 0.622080\n",
      "(Iteration 11176 / 14700) loss: 0.633538\n",
      "(Iteration 11177 / 14700) loss: 0.667335\n",
      "(Iteration 11178 / 14700) loss: 0.672686\n",
      "(Iteration 11179 / 14700) loss: 0.722371\n",
      "(Iteration 11180 / 14700) loss: 0.605928\n",
      "(Iteration 11181 / 14700) loss: 0.716393\n",
      "(Iteration 11182 / 14700) loss: 0.903348\n",
      "(Iteration 11183 / 14700) loss: 0.750618\n",
      "(Iteration 11184 / 14700) loss: 0.685210\n",
      "(Iteration 11185 / 14700) loss: 0.671057\n",
      "(Iteration 11186 / 14700) loss: 1.068203\n",
      "(Iteration 11187 / 14700) loss: 0.486725\n",
      "(Iteration 11188 / 14700) loss: 0.811967\n",
      "(Iteration 11189 / 14700) loss: 1.038729\n",
      "(Iteration 11190 / 14700) loss: 0.888009\n",
      "(Iteration 11191 / 14700) loss: 0.776628\n",
      "(Iteration 11192 / 14700) loss: 0.805598\n",
      "(Iteration 11193 / 14700) loss: 0.616500\n",
      "(Iteration 11194 / 14700) loss: 0.693212\n",
      "(Iteration 11195 / 14700) loss: 0.833824\n",
      "(Iteration 11196 / 14700) loss: 0.642286\n",
      "(Iteration 11197 / 14700) loss: 0.842912\n",
      "(Iteration 11198 / 14700) loss: 0.564310\n",
      "(Iteration 11199 / 14700) loss: 0.828901\n",
      "(Iteration 11200 / 14700) loss: 0.857490\n",
      "(Iteration 11201 / 14700) loss: 0.713438\n",
      "(Iteration 11202 / 14700) loss: 0.726823\n",
      "(Iteration 11203 / 14700) loss: 0.748381\n",
      "(Iteration 11204 / 14700) loss: 0.793625\n",
      "(Iteration 11205 / 14700) loss: 0.573038\n",
      "(Iteration 11206 / 14700) loss: 0.628291\n",
      "(Iteration 11207 / 14700) loss: 0.807010\n",
      "(Iteration 11208 / 14700) loss: 0.802838\n",
      "(Iteration 11209 / 14700) loss: 0.854584\n",
      "(Iteration 11210 / 14700) loss: 0.727793\n",
      "(Iteration 11211 / 14700) loss: 0.556635\n",
      "(Iteration 11212 / 14700) loss: 0.853900\n",
      "(Iteration 11213 / 14700) loss: 0.671847\n",
      "(Iteration 11214 / 14700) loss: 0.511475\n",
      "(Iteration 11215 / 14700) loss: 0.761569\n",
      "(Iteration 11216 / 14700) loss: 0.672496\n",
      "(Iteration 11217 / 14700) loss: 0.830463\n",
      "(Iteration 11218 / 14700) loss: 0.651594\n",
      "(Iteration 11219 / 14700) loss: 0.487295\n",
      "(Iteration 11220 / 14700) loss: 0.684883\n",
      "(Iteration 11221 / 14700) loss: 0.581377\n",
      "(Iteration 11222 / 14700) loss: 0.760721\n",
      "(Iteration 11223 / 14700) loss: 0.814494\n",
      "(Iteration 11224 / 14700) loss: 0.745742\n",
      "(Iteration 11225 / 14700) loss: 0.803588\n",
      "(Iteration 11226 / 14700) loss: 0.467239\n",
      "(Iteration 11227 / 14700) loss: 0.726368\n",
      "(Iteration 11228 / 14700) loss: 0.498162\n",
      "(Iteration 11229 / 14700) loss: 0.721248\n",
      "(Iteration 11230 / 14700) loss: 1.141229\n",
      "(Iteration 11231 / 14700) loss: 0.668032\n",
      "(Iteration 11232 / 14700) loss: 0.584058\n",
      "(Iteration 11233 / 14700) loss: 0.621949\n",
      "(Iteration 11234 / 14700) loss: 0.929522\n",
      "(Iteration 11235 / 14700) loss: 0.841612\n",
      "(Iteration 11236 / 14700) loss: 0.941333\n",
      "(Iteration 11237 / 14700) loss: 0.702232\n",
      "(Iteration 11238 / 14700) loss: 0.658240\n",
      "(Iteration 11239 / 14700) loss: 0.750513\n",
      "(Iteration 11240 / 14700) loss: 0.915801\n",
      "(Iteration 11241 / 14700) loss: 0.671437\n",
      "(Iteration 11242 / 14700) loss: 0.928749\n",
      "(Iteration 11243 / 14700) loss: 0.746867\n",
      "(Iteration 11244 / 14700) loss: 0.836753\n",
      "(Iteration 11245 / 14700) loss: 0.769313\n",
      "(Iteration 11246 / 14700) loss: 0.654184\n",
      "(Iteration 11247 / 14700) loss: 0.691857\n",
      "(Iteration 11248 / 14700) loss: 0.639380\n",
      "(Iteration 11249 / 14700) loss: 0.983675\n",
      "(Iteration 11250 / 14700) loss: 0.842392\n",
      "(Iteration 11251 / 14700) loss: 0.703768\n",
      "(Iteration 11252 / 14700) loss: 0.551298\n",
      "(Iteration 11253 / 14700) loss: 0.813282\n",
      "(Iteration 11254 / 14700) loss: 0.830259\n",
      "(Iteration 11255 / 14700) loss: 0.715471\n",
      "(Iteration 11256 / 14700) loss: 0.663290\n",
      "(Iteration 11257 / 14700) loss: 0.491463\n",
      "(Iteration 11258 / 14700) loss: 0.900534\n",
      "(Iteration 11259 / 14700) loss: 0.527738\n",
      "(Iteration 11260 / 14700) loss: 0.537863\n",
      "(Iteration 11261 / 14700) loss: 0.747793\n",
      "(Iteration 11262 / 14700) loss: 0.823826\n",
      "(Iteration 11263 / 14700) loss: 0.828948\n",
      "(Iteration 11264 / 14700) loss: 0.851971\n",
      "(Iteration 11265 / 14700) loss: 0.612948\n",
      "(Iteration 11266 / 14700) loss: 0.815070\n",
      "(Iteration 11267 / 14700) loss: 0.607817\n",
      "(Iteration 11268 / 14700) loss: 0.816851\n",
      "(Iteration 11269 / 14700) loss: 0.867117\n",
      "(Iteration 11270 / 14700) loss: 0.545134\n",
      "(Iteration 11271 / 14700) loss: 0.549181\n",
      "(Iteration 11272 / 14700) loss: 0.640469\n",
      "(Iteration 11273 / 14700) loss: 0.725895\n",
      "(Iteration 11274 / 14700) loss: 0.618215\n",
      "(Iteration 11275 / 14700) loss: 0.712749\n",
      "(Iteration 11276 / 14700) loss: 0.565713\n",
      "(Iteration 11277 / 14700) loss: 0.591531\n",
      "(Iteration 11278 / 14700) loss: 0.724684\n",
      "(Iteration 11279 / 14700) loss: 0.609295\n",
      "(Iteration 11280 / 14700) loss: 0.548727\n",
      "(Iteration 11281 / 14700) loss: 0.587018\n",
      "(Iteration 11282 / 14700) loss: 0.610489\n",
      "(Iteration 11283 / 14700) loss: 0.647377\n",
      "(Iteration 11284 / 14700) loss: 0.667156\n",
      "(Iteration 11285 / 14700) loss: 0.694322\n",
      "(Iteration 11286 / 14700) loss: 0.634886\n",
      "(Iteration 11287 / 14700) loss: 0.905210\n",
      "(Iteration 11288 / 14700) loss: 0.780121\n",
      "(Iteration 11289 / 14700) loss: 0.583359\n",
      "(Iteration 11290 / 14700) loss: 0.899663\n",
      "(Iteration 11291 / 14700) loss: 0.750781\n",
      "(Iteration 11292 / 14700) loss: 0.785998\n",
      "(Iteration 11293 / 14700) loss: 0.861347\n",
      "(Iteration 11294 / 14700) loss: 0.816003\n",
      "(Iteration 11295 / 14700) loss: 0.877482\n",
      "(Iteration 11296 / 14700) loss: 0.991673\n",
      "(Iteration 11297 / 14700) loss: 0.681414\n",
      "(Iteration 11298 / 14700) loss: 0.688153\n",
      "(Iteration 11299 / 14700) loss: 1.205848\n",
      "(Iteration 11300 / 14700) loss: 0.841180\n",
      "(Iteration 11301 / 14700) loss: 0.860349\n",
      "(Iteration 11302 / 14700) loss: 0.780850\n",
      "(Iteration 11303 / 14700) loss: 0.822225\n",
      "(Iteration 11304 / 14700) loss: 0.621273\n",
      "(Iteration 11305 / 14700) loss: 0.992799\n",
      "(Iteration 11306 / 14700) loss: 0.883372\n",
      "(Iteration 11307 / 14700) loss: 0.636732\n",
      "(Iteration 11308 / 14700) loss: 1.134197\n",
      "(Iteration 11309 / 14700) loss: 0.902378\n",
      "(Iteration 11310 / 14700) loss: 0.742470\n",
      "(Iteration 11311 / 14700) loss: 0.636929\n",
      "(Iteration 11312 / 14700) loss: 0.698026\n",
      "(Iteration 11313 / 14700) loss: 0.905357\n",
      "(Iteration 11314 / 14700) loss: 0.720073\n",
      "(Iteration 11315 / 14700) loss: 0.567856\n",
      "(Iteration 11316 / 14700) loss: 0.643900\n",
      "(Iteration 11317 / 14700) loss: 0.843202\n",
      "(Iteration 11318 / 14700) loss: 0.606068\n",
      "(Iteration 11319 / 14700) loss: 0.736917\n",
      "(Iteration 11320 / 14700) loss: 1.027334\n",
      "(Iteration 11321 / 14700) loss: 0.766759\n",
      "(Iteration 11322 / 14700) loss: 0.539824\n",
      "(Iteration 11323 / 14700) loss: 0.753788\n",
      "(Iteration 11324 / 14700) loss: 0.856028\n",
      "(Iteration 11325 / 14700) loss: 0.717300\n",
      "(Iteration 11326 / 14700) loss: 0.702937\n",
      "(Iteration 11327 / 14700) loss: 0.734724\n",
      "(Iteration 11328 / 14700) loss: 0.720183\n",
      "(Iteration 11329 / 14700) loss: 0.757520\n",
      "(Iteration 11330 / 14700) loss: 1.071545\n",
      "(Iteration 11331 / 14700) loss: 0.692751\n",
      "(Iteration 11332 / 14700) loss: 0.971097\n",
      "(Iteration 11333 / 14700) loss: 0.581120\n",
      "(Iteration 11334 / 14700) loss: 0.676265\n",
      "(Iteration 11335 / 14700) loss: 0.664126\n",
      "(Iteration 11336 / 14700) loss: 0.658665\n",
      "(Iteration 11337 / 14700) loss: 0.692603\n",
      "(Iteration 11338 / 14700) loss: 0.606268\n",
      "(Iteration 11339 / 14700) loss: 0.847066\n",
      "(Iteration 11340 / 14700) loss: 0.653623\n",
      "(Iteration 11341 / 14700) loss: 0.717726\n",
      "(Iteration 11342 / 14700) loss: 0.635607\n",
      "(Iteration 11343 / 14700) loss: 0.731127\n",
      "(Iteration 11344 / 14700) loss: 0.569817\n",
      "(Iteration 11345 / 14700) loss: 0.740707\n",
      "(Iteration 11346 / 14700) loss: 0.673049\n",
      "(Iteration 11347 / 14700) loss: 0.936976\n",
      "(Iteration 11348 / 14700) loss: 0.552101\n",
      "(Iteration 11349 / 14700) loss: 0.766938\n",
      "(Iteration 11350 / 14700) loss: 0.476063\n",
      "(Iteration 11351 / 14700) loss: 0.816575\n",
      "(Iteration 11352 / 14700) loss: 0.975522\n",
      "(Iteration 11353 / 14700) loss: 0.994691\n",
      "(Iteration 11354 / 14700) loss: 0.680428\n",
      "(Iteration 11355 / 14700) loss: 0.647908\n",
      "(Iteration 11356 / 14700) loss: 0.852014\n",
      "(Iteration 11357 / 14700) loss: 0.802236\n",
      "(Iteration 11358 / 14700) loss: 0.512777\n",
      "(Iteration 11359 / 14700) loss: 0.651477\n",
      "(Iteration 11360 / 14700) loss: 0.592147\n",
      "(Iteration 11361 / 14700) loss: 0.959320\n",
      "(Iteration 11362 / 14700) loss: 0.664694\n",
      "(Iteration 11363 / 14700) loss: 0.721560\n",
      "(Iteration 11364 / 14700) loss: 0.736646\n",
      "(Iteration 11365 / 14700) loss: 0.694425\n",
      "(Iteration 11366 / 14700) loss: 0.813618\n",
      "(Iteration 11367 / 14700) loss: 0.661733\n",
      "(Iteration 11368 / 14700) loss: 0.791853\n",
      "(Iteration 11369 / 14700) loss: 0.741065\n",
      "(Iteration 11370 / 14700) loss: 0.814979\n",
      "(Iteration 11371 / 14700) loss: 1.042614\n",
      "(Iteration 11372 / 14700) loss: 0.620069\n",
      "(Iteration 11373 / 14700) loss: 0.638737\n",
      "(Iteration 11374 / 14700) loss: 0.613418\n",
      "(Iteration 11375 / 14700) loss: 0.883584\n",
      "(Iteration 11376 / 14700) loss: 0.935105\n",
      "(Iteration 11377 / 14700) loss: 0.708150\n",
      "(Iteration 11378 / 14700) loss: 0.607921\n",
      "(Iteration 11379 / 14700) loss: 0.631786\n",
      "(Iteration 11380 / 14700) loss: 0.645701\n",
      "(Iteration 11381 / 14700) loss: 0.756420\n",
      "(Iteration 11382 / 14700) loss: 1.026292\n",
      "(Iteration 11383 / 14700) loss: 0.822645\n",
      "(Iteration 11384 / 14700) loss: 0.837207\n",
      "(Iteration 11385 / 14700) loss: 0.938252\n",
      "(Iteration 11386 / 14700) loss: 0.923112\n",
      "(Iteration 11387 / 14700) loss: 0.806640\n",
      "(Iteration 11388 / 14700) loss: 0.753198\n",
      "(Iteration 11389 / 14700) loss: 0.650364\n",
      "(Iteration 11390 / 14700) loss: 0.639544\n",
      "(Iteration 11391 / 14700) loss: 0.601675\n",
      "(Iteration 11392 / 14700) loss: 0.864548\n",
      "(Iteration 11393 / 14700) loss: 0.854827\n",
      "(Iteration 11394 / 14700) loss: 0.768220\n",
      "(Iteration 11395 / 14700) loss: 0.791589\n",
      "(Iteration 11396 / 14700) loss: 0.787125\n",
      "(Iteration 11397 / 14700) loss: 0.730526\n",
      "(Iteration 11398 / 14700) loss: 0.613028\n",
      "(Iteration 11399 / 14700) loss: 0.768627\n",
      "(Iteration 11400 / 14700) loss: 0.657871\n",
      "(Iteration 11401 / 14700) loss: 0.706905\n",
      "(Iteration 11402 / 14700) loss: 0.921633\n",
      "(Iteration 11403 / 14700) loss: 1.040017\n",
      "(Iteration 11404 / 14700) loss: 0.604404\n",
      "(Iteration 11405 / 14700) loss: 0.759959\n",
      "(Iteration 11406 / 14700) loss: 0.711620\n",
      "(Iteration 11407 / 14700) loss: 1.187727\n",
      "(Iteration 11408 / 14700) loss: 0.652006\n",
      "(Iteration 11409 / 14700) loss: 1.004162\n",
      "(Iteration 11410 / 14700) loss: 0.881876\n",
      "(Iteration 11411 / 14700) loss: 0.733022\n",
      "(Iteration 11412 / 14700) loss: 0.892037\n",
      "(Iteration 11413 / 14700) loss: 0.679836\n",
      "(Iteration 11414 / 14700) loss: 0.801016\n",
      "(Iteration 11415 / 14700) loss: 0.725831\n",
      "(Iteration 11416 / 14700) loss: 0.906293\n",
      "(Iteration 11417 / 14700) loss: 0.693480\n",
      "(Iteration 11418 / 14700) loss: 0.867122\n",
      "(Iteration 11419 / 14700) loss: 0.633845\n",
      "(Iteration 11420 / 14700) loss: 0.828980\n",
      "(Iteration 11421 / 14700) loss: 0.670482\n",
      "(Iteration 11422 / 14700) loss: 0.605014\n",
      "(Iteration 11423 / 14700) loss: 0.573285\n",
      "(Iteration 11424 / 14700) loss: 0.649939\n",
      "(Iteration 11425 / 14700) loss: 0.878093\n",
      "(Iteration 11426 / 14700) loss: 0.381493\n",
      "(Iteration 11427 / 14700) loss: 0.920875\n",
      "(Iteration 11428 / 14700) loss: 0.595162\n",
      "(Iteration 11429 / 14700) loss: 0.686382\n",
      "(Iteration 11430 / 14700) loss: 0.889476\n",
      "(Iteration 11431 / 14700) loss: 0.558305\n",
      "(Iteration 11432 / 14700) loss: 0.779564\n",
      "(Iteration 11433 / 14700) loss: 0.723542\n",
      "(Iteration 11434 / 14700) loss: 0.827932\n",
      "(Iteration 11435 / 14700) loss: 0.788002\n",
      "(Iteration 11436 / 14700) loss: 0.738715\n",
      "(Iteration 11437 / 14700) loss: 0.552168\n",
      "(Iteration 11438 / 14700) loss: 0.601684\n",
      "(Iteration 11439 / 14700) loss: 0.763958\n",
      "(Iteration 11440 / 14700) loss: 0.536246\n",
      "(Iteration 11441 / 14700) loss: 0.673981\n",
      "(Iteration 11442 / 14700) loss: 0.782922\n",
      "(Iteration 11443 / 14700) loss: 0.756663\n",
      "(Iteration 11444 / 14700) loss: 0.962558\n",
      "(Iteration 11445 / 14700) loss: 0.763367\n",
      "(Iteration 11446 / 14700) loss: 0.544976\n",
      "(Iteration 11447 / 14700) loss: 0.576788\n",
      "(Iteration 11448 / 14700) loss: 0.752678\n",
      "(Iteration 11449 / 14700) loss: 1.013900\n",
      "(Iteration 11450 / 14700) loss: 0.659285\n",
      "(Iteration 11451 / 14700) loss: 1.006326\n",
      "(Iteration 11452 / 14700) loss: 0.595957\n",
      "(Iteration 11453 / 14700) loss: 0.905938\n",
      "(Iteration 11454 / 14700) loss: 0.588436\n",
      "(Iteration 11455 / 14700) loss: 0.614161\n",
      "(Iteration 11456 / 14700) loss: 0.823738\n",
      "(Iteration 11457 / 14700) loss: 0.680053\n",
      "(Iteration 11458 / 14700) loss: 0.754002\n",
      "(Iteration 11459 / 14700) loss: 0.783783\n",
      "(Iteration 11460 / 14700) loss: 0.664389\n",
      "(Iteration 11461 / 14700) loss: 0.883119\n",
      "(Iteration 11462 / 14700) loss: 0.684935\n",
      "(Iteration 11463 / 14700) loss: 0.758872\n",
      "(Iteration 11464 / 14700) loss: 0.505908\n",
      "(Iteration 11465 / 14700) loss: 0.871917\n",
      "(Iteration 11466 / 14700) loss: 0.530338\n",
      "(Iteration 11467 / 14700) loss: 0.776618\n",
      "(Iteration 11468 / 14700) loss: 0.523619\n",
      "(Iteration 11469 / 14700) loss: 0.789720\n",
      "(Iteration 11470 / 14700) loss: 0.646928\n",
      "(Iteration 11471 / 14700) loss: 1.058106\n",
      "(Iteration 11472 / 14700) loss: 0.895446\n",
      "(Iteration 11473 / 14700) loss: 0.729549\n",
      "(Iteration 11474 / 14700) loss: 0.542789\n",
      "(Iteration 11475 / 14700) loss: 0.700808\n",
      "(Iteration 11476 / 14700) loss: 0.828822\n",
      "(Iteration 11477 / 14700) loss: 0.689406\n",
      "(Iteration 11478 / 14700) loss: 1.044563\n",
      "(Iteration 11479 / 14700) loss: 0.839204\n",
      "(Iteration 11480 / 14700) loss: 0.753763\n",
      "(Iteration 11481 / 14700) loss: 0.638840\n",
      "(Iteration 11482 / 14700) loss: 0.893340\n",
      "(Iteration 11483 / 14700) loss: 0.701068\n",
      "(Iteration 11484 / 14700) loss: 0.520558\n",
      "(Iteration 11485 / 14700) loss: 1.014512\n",
      "(Iteration 11486 / 14700) loss: 0.702179\n",
      "(Iteration 11487 / 14700) loss: 0.744335\n",
      "(Iteration 11488 / 14700) loss: 0.777820\n",
      "(Iteration 11489 / 14700) loss: 0.659169\n",
      "(Iteration 11490 / 14700) loss: 0.828607\n",
      "(Iteration 11491 / 14700) loss: 0.632936\n",
      "(Iteration 11492 / 14700) loss: 0.681929\n",
      "(Iteration 11493 / 14700) loss: 0.798634\n",
      "(Iteration 11494 / 14700) loss: 0.639657\n",
      "(Iteration 11495 / 14700) loss: 0.995994\n",
      "(Iteration 11496 / 14700) loss: 0.476647\n",
      "(Iteration 11497 / 14700) loss: 0.720734\n",
      "(Iteration 11498 / 14700) loss: 0.716703\n",
      "(Iteration 11499 / 14700) loss: 0.589412\n",
      "(Iteration 11500 / 14700) loss: 0.632071\n",
      "(Iteration 11501 / 14700) loss: 0.818144\n",
      "(Iteration 11502 / 14700) loss: 0.749617\n",
      "(Iteration 11503 / 14700) loss: 0.698638\n",
      "(Iteration 11504 / 14700) loss: 0.691648\n",
      "(Iteration 11505 / 14700) loss: 0.598372\n",
      "(Iteration 11506 / 14700) loss: 0.660953\n",
      "(Iteration 11507 / 14700) loss: 0.783933\n",
      "(Iteration 11508 / 14700) loss: 0.789082\n",
      "(Iteration 11509 / 14700) loss: 0.625234\n",
      "(Iteration 11510 / 14700) loss: 1.006289\n",
      "(Iteration 11511 / 14700) loss: 0.874339\n",
      "(Iteration 11512 / 14700) loss: 0.973230\n",
      "(Iteration 11513 / 14700) loss: 0.580610\n",
      "(Iteration 11514 / 14700) loss: 0.583055\n",
      "(Iteration 11515 / 14700) loss: 0.745518\n",
      "(Iteration 11516 / 14700) loss: 0.901156\n",
      "(Iteration 11517 / 14700) loss: 0.729359\n",
      "(Iteration 11518 / 14700) loss: 0.598578\n",
      "(Iteration 11519 / 14700) loss: 0.750320\n",
      "(Iteration 11520 / 14700) loss: 0.534246\n",
      "(Iteration 11521 / 14700) loss: 0.917787\n",
      "(Iteration 11522 / 14700) loss: 0.917670\n",
      "(Iteration 11523 / 14700) loss: 0.832710\n",
      "(Iteration 11524 / 14700) loss: 0.611584\n",
      "(Iteration 11525 / 14700) loss: 0.657012\n",
      "(Iteration 11526 / 14700) loss: 0.746542\n",
      "(Iteration 11527 / 14700) loss: 0.632017\n",
      "(Iteration 11528 / 14700) loss: 0.658232\n",
      "(Iteration 11529 / 14700) loss: 0.737919\n",
      "(Iteration 11530 / 14700) loss: 0.720096\n",
      "(Iteration 11531 / 14700) loss: 0.572184\n",
      "(Iteration 11532 / 14700) loss: 0.728777\n",
      "(Iteration 11533 / 14700) loss: 0.580559\n",
      "(Iteration 11534 / 14700) loss: 0.949636\n",
      "(Iteration 11535 / 14700) loss: 0.867791\n",
      "(Iteration 11536 / 14700) loss: 0.734350\n",
      "(Iteration 11537 / 14700) loss: 0.720910\n",
      "(Iteration 11538 / 14700) loss: 0.557837\n",
      "(Iteration 11539 / 14700) loss: 0.702277\n",
      "(Iteration 11540 / 14700) loss: 0.796956\n",
      "(Iteration 11541 / 14700) loss: 0.941509\n",
      "(Iteration 11542 / 14700) loss: 0.589148\n",
      "(Iteration 11543 / 14700) loss: 0.593432\n",
      "(Iteration 11544 / 14700) loss: 0.533777\n",
      "(Iteration 11545 / 14700) loss: 0.686892\n",
      "(Iteration 11546 / 14700) loss: 0.767556\n",
      "(Iteration 11547 / 14700) loss: 0.650308\n",
      "(Iteration 11548 / 14700) loss: 0.850296\n",
      "(Iteration 11549 / 14700) loss: 0.822784\n",
      "(Iteration 11550 / 14700) loss: 0.622247\n",
      "(Iteration 11551 / 14700) loss: 0.683454\n",
      "(Iteration 11552 / 14700) loss: 0.674373\n",
      "(Iteration 11553 / 14700) loss: 0.653663\n",
      "(Iteration 11554 / 14700) loss: 0.605809\n",
      "(Iteration 11555 / 14700) loss: 0.618667\n",
      "(Iteration 11556 / 14700) loss: 1.140553\n",
      "(Iteration 11557 / 14700) loss: 0.611196\n",
      "(Iteration 11558 / 14700) loss: 0.730585\n",
      "(Iteration 11559 / 14700) loss: 0.673300\n",
      "(Iteration 11560 / 14700) loss: 0.735488\n",
      "(Iteration 11561 / 14700) loss: 0.542934\n",
      "(Iteration 11562 / 14700) loss: 0.586605\n",
      "(Iteration 11563 / 14700) loss: 0.570111\n",
      "(Iteration 11564 / 14700) loss: 0.598541\n",
      "(Iteration 11565 / 14700) loss: 0.638617\n",
      "(Iteration 11566 / 14700) loss: 0.819467\n",
      "(Iteration 11567 / 14700) loss: 0.654244\n",
      "(Iteration 11568 / 14700) loss: 0.588823\n",
      "(Iteration 11569 / 14700) loss: 0.623037\n",
      "(Iteration 11570 / 14700) loss: 0.786234\n",
      "(Iteration 11571 / 14700) loss: 0.614323\n",
      "(Iteration 11572 / 14700) loss: 0.721579\n",
      "(Iteration 11573 / 14700) loss: 0.868808\n",
      "(Iteration 11574 / 14700) loss: 0.804050\n",
      "(Iteration 11575 / 14700) loss: 0.756293\n",
      "(Iteration 11576 / 14700) loss: 0.620123\n",
      "(Iteration 11577 / 14700) loss: 0.629614\n",
      "(Iteration 11578 / 14700) loss: 0.780246\n",
      "(Iteration 11579 / 14700) loss: 1.021738\n",
      "(Iteration 11580 / 14700) loss: 0.715807\n",
      "(Iteration 11581 / 14700) loss: 0.685682\n",
      "(Iteration 11582 / 14700) loss: 0.591304\n",
      "(Iteration 11583 / 14700) loss: 0.731385\n",
      "(Iteration 11584 / 14700) loss: 0.669143\n",
      "(Iteration 11585 / 14700) loss: 0.720257\n",
      "(Iteration 11586 / 14700) loss: 0.785658\n",
      "(Iteration 11587 / 14700) loss: 0.757454\n",
      "(Iteration 11588 / 14700) loss: 1.051438\n",
      "(Iteration 11589 / 14700) loss: 0.826916\n",
      "(Iteration 11590 / 14700) loss: 0.736139\n",
      "(Iteration 11591 / 14700) loss: 0.776817\n",
      "(Iteration 11592 / 14700) loss: 0.837155\n",
      "(Iteration 11593 / 14700) loss: 0.857680\n",
      "(Iteration 11594 / 14700) loss: 1.043886\n",
      "(Iteration 11595 / 14700) loss: 0.647320\n",
      "(Iteration 11596 / 14700) loss: 0.494869\n",
      "(Iteration 11597 / 14700) loss: 0.863427\n",
      "(Iteration 11598 / 14700) loss: 0.630822\n",
      "(Iteration 11599 / 14700) loss: 0.909387\n",
      "(Iteration 11600 / 14700) loss: 0.785418\n",
      "(Iteration 11601 / 14700) loss: 0.659953\n",
      "(Iteration 11602 / 14700) loss: 0.717929\n",
      "(Iteration 11603 / 14700) loss: 0.777276\n",
      "(Iteration 11604 / 14700) loss: 0.739005\n",
      "(Iteration 11605 / 14700) loss: 0.909866\n",
      "(Iteration 11606 / 14700) loss: 1.051922\n",
      "(Iteration 11607 / 14700) loss: 0.688936\n",
      "(Iteration 11608 / 14700) loss: 0.762582\n",
      "(Iteration 11609 / 14700) loss: 0.690857\n",
      "(Iteration 11610 / 14700) loss: 0.790109\n",
      "(Iteration 11611 / 14700) loss: 0.710969\n",
      "(Iteration 11612 / 14700) loss: 0.620018\n",
      "(Iteration 11613 / 14700) loss: 0.978829\n",
      "(Iteration 11614 / 14700) loss: 0.698573\n",
      "(Iteration 11615 / 14700) loss: 0.580147\n",
      "(Iteration 11616 / 14700) loss: 0.767603\n",
      "(Iteration 11617 / 14700) loss: 0.857666\n",
      "(Iteration 11618 / 14700) loss: 0.691730\n",
      "(Iteration 11619 / 14700) loss: 0.770837\n",
      "(Iteration 11620 / 14700) loss: 0.568454\n",
      "(Iteration 11621 / 14700) loss: 0.895011\n",
      "(Iteration 11622 / 14700) loss: 0.798915\n",
      "(Iteration 11623 / 14700) loss: 0.698908\n",
      "(Iteration 11624 / 14700) loss: 0.902598\n",
      "(Iteration 11625 / 14700) loss: 0.459720\n",
      "(Iteration 11626 / 14700) loss: 0.779266\n",
      "(Iteration 11627 / 14700) loss: 0.600168\n",
      "(Iteration 11628 / 14700) loss: 0.599333\n",
      "(Iteration 11629 / 14700) loss: 0.751745\n",
      "(Iteration 11630 / 14700) loss: 0.866541\n",
      "(Iteration 11631 / 14700) loss: 0.752547\n",
      "(Iteration 11632 / 14700) loss: 0.760294\n",
      "(Iteration 11633 / 14700) loss: 0.549923\n",
      "(Iteration 11634 / 14700) loss: 0.672840\n",
      "(Iteration 11635 / 14700) loss: 0.739115\n",
      "(Iteration 11636 / 14700) loss: 0.803494\n",
      "(Iteration 11637 / 14700) loss: 0.579637\n",
      "(Iteration 11638 / 14700) loss: 0.619418\n",
      "(Iteration 11639 / 14700) loss: 0.493117\n",
      "(Iteration 11640 / 14700) loss: 0.987583\n",
      "(Iteration 11641 / 14700) loss: 0.620190\n",
      "(Iteration 11642 / 14700) loss: 0.663007\n",
      "(Iteration 11643 / 14700) loss: 0.804977\n",
      "(Iteration 11644 / 14700) loss: 0.802087\n",
      "(Iteration 11645 / 14700) loss: 0.664629\n",
      "(Iteration 11646 / 14700) loss: 0.626118\n",
      "(Iteration 11647 / 14700) loss: 0.944720\n",
      "(Iteration 11648 / 14700) loss: 0.516839\n",
      "(Iteration 11649 / 14700) loss: 0.651260\n",
      "(Iteration 11650 / 14700) loss: 0.610523\n",
      "(Iteration 11651 / 14700) loss: 0.766944\n",
      "(Iteration 11652 / 14700) loss: 0.626283\n",
      "(Iteration 11653 / 14700) loss: 0.664607\n",
      "(Iteration 11654 / 14700) loss: 0.944965\n",
      "(Iteration 11655 / 14700) loss: 0.541366\n",
      "(Iteration 11656 / 14700) loss: 0.779482\n",
      "(Iteration 11657 / 14700) loss: 0.727929\n",
      "(Iteration 11658 / 14700) loss: 0.771254\n",
      "(Iteration 11659 / 14700) loss: 0.619522\n",
      "(Iteration 11660 / 14700) loss: 0.629637\n",
      "(Iteration 11661 / 14700) loss: 0.569434\n",
      "(Iteration 11662 / 14700) loss: 0.861172\n",
      "(Iteration 11663 / 14700) loss: 0.705651\n",
      "(Iteration 11664 / 14700) loss: 0.945040\n",
      "(Iteration 11665 / 14700) loss: 0.690677\n",
      "(Iteration 11666 / 14700) loss: 0.653278\n",
      "(Iteration 11667 / 14700) loss: 0.688202\n",
      "(Iteration 11668 / 14700) loss: 0.614320\n",
      "(Iteration 11669 / 14700) loss: 0.689707\n",
      "(Iteration 11670 / 14700) loss: 0.650173\n",
      "(Iteration 11671 / 14700) loss: 0.767365\n",
      "(Iteration 11672 / 14700) loss: 0.786456\n",
      "(Iteration 11673 / 14700) loss: 0.631608\n",
      "(Iteration 11674 / 14700) loss: 0.809260\n",
      "(Iteration 11675 / 14700) loss: 0.549124\n",
      "(Iteration 11676 / 14700) loss: 0.649933\n",
      "(Iteration 11677 / 14700) loss: 0.718572\n",
      "(Iteration 11678 / 14700) loss: 0.929936\n",
      "(Iteration 11679 / 14700) loss: 0.581579\n",
      "(Iteration 11680 / 14700) loss: 0.453782\n",
      "(Iteration 11681 / 14700) loss: 0.581982\n",
      "(Iteration 11682 / 14700) loss: 0.736750\n",
      "(Iteration 11683 / 14700) loss: 0.805165\n",
      "(Iteration 11684 / 14700) loss: 0.604858\n",
      "(Iteration 11685 / 14700) loss: 0.864364\n",
      "(Iteration 11686 / 14700) loss: 0.749687\n",
      "(Iteration 11687 / 14700) loss: 0.486492\n",
      "(Iteration 11688 / 14700) loss: 0.686524\n",
      "(Iteration 11689 / 14700) loss: 0.636255\n",
      "(Iteration 11690 / 14700) loss: 0.587018\n",
      "(Iteration 11691 / 14700) loss: 0.690260\n",
      "(Iteration 11692 / 14700) loss: 0.818409\n",
      "(Iteration 11693 / 14700) loss: 0.641737\n",
      "(Iteration 11694 / 14700) loss: 0.632612\n",
      "(Iteration 11695 / 14700) loss: 0.727821\n",
      "(Iteration 11696 / 14700) loss: 0.717269\n",
      "(Iteration 11697 / 14700) loss: 0.824152\n",
      "(Iteration 11698 / 14700) loss: 0.674230\n",
      "(Iteration 11699 / 14700) loss: 0.707445\n",
      "(Iteration 11700 / 14700) loss: 0.637136\n",
      "(Iteration 11701 / 14700) loss: 0.745628\n",
      "(Iteration 11702 / 14700) loss: 0.804785\n",
      "(Iteration 11703 / 14700) loss: 0.575075\n",
      "(Iteration 11704 / 14700) loss: 0.757684\n",
      "(Iteration 11705 / 14700) loss: 1.092150\n",
      "(Iteration 11706 / 14700) loss: 0.658329\n",
      "(Iteration 11707 / 14700) loss: 0.602812\n",
      "(Iteration 11708 / 14700) loss: 0.775190\n",
      "(Iteration 11709 / 14700) loss: 0.655505\n",
      "(Iteration 11710 / 14700) loss: 0.947872\n",
      "(Iteration 11711 / 14700) loss: 0.901504\n",
      "(Iteration 11712 / 14700) loss: 0.938298\n",
      "(Iteration 11713 / 14700) loss: 0.676216\n",
      "(Iteration 11714 / 14700) loss: 0.600054\n",
      "(Iteration 11715 / 14700) loss: 0.793889\n",
      "(Iteration 11716 / 14700) loss: 0.777878\n",
      "(Iteration 11717 / 14700) loss: 0.929305\n",
      "(Iteration 11718 / 14700) loss: 0.683346\n",
      "(Iteration 11719 / 14700) loss: 0.720681\n",
      "(Iteration 11720 / 14700) loss: 0.632119\n",
      "(Iteration 11721 / 14700) loss: 0.586520\n",
      "(Iteration 11722 / 14700) loss: 0.771894\n",
      "(Iteration 11723 / 14700) loss: 0.913202\n",
      "(Iteration 11724 / 14700) loss: 0.895874\n",
      "(Iteration 11725 / 14700) loss: 0.674102\n",
      "(Iteration 11726 / 14700) loss: 0.580228\n",
      "(Iteration 11727 / 14700) loss: 0.737513\n",
      "(Iteration 11728 / 14700) loss: 0.661393\n",
      "(Iteration 11729 / 14700) loss: 0.869566\n",
      "(Iteration 11730 / 14700) loss: 0.667202\n",
      "(Iteration 11731 / 14700) loss: 0.664580\n",
      "(Iteration 11732 / 14700) loss: 0.606972\n",
      "(Iteration 11733 / 14700) loss: 0.665642\n",
      "(Iteration 11734 / 14700) loss: 0.802333\n",
      "(Iteration 11735 / 14700) loss: 0.846551\n",
      "(Iteration 11736 / 14700) loss: 0.512937\n",
      "(Iteration 11737 / 14700) loss: 0.576058\n",
      "(Iteration 11738 / 14700) loss: 0.708902\n",
      "(Iteration 11739 / 14700) loss: 0.721512\n",
      "(Iteration 11740 / 14700) loss: 0.577548\n",
      "(Iteration 11741 / 14700) loss: 0.799346\n",
      "(Iteration 11742 / 14700) loss: 0.638446\n",
      "(Iteration 11743 / 14700) loss: 0.455385\n",
      "(Iteration 11744 / 14700) loss: 0.920637\n",
      "(Iteration 11745 / 14700) loss: 0.915198\n",
      "(Iteration 11746 / 14700) loss: 0.764061\n",
      "(Iteration 11747 / 14700) loss: 0.663729\n",
      "(Iteration 11748 / 14700) loss: 0.662569\n",
      "(Iteration 11749 / 14700) loss: 0.748434\n",
      "(Iteration 11750 / 14700) loss: 0.896086\n",
      "(Iteration 11751 / 14700) loss: 0.898416\n",
      "(Iteration 11752 / 14700) loss: 0.802019\n",
      "(Iteration 11753 / 14700) loss: 0.533144\n",
      "(Iteration 11754 / 14700) loss: 0.617593\n",
      "(Iteration 11755 / 14700) loss: 0.655607\n",
      "(Iteration 11756 / 14700) loss: 0.845797\n",
      "(Iteration 11757 / 14700) loss: 0.982145\n",
      "(Iteration 11758 / 14700) loss: 0.895918\n",
      "(Iteration 11759 / 14700) loss: 0.816091\n",
      "(Iteration 11760 / 14700) loss: 0.787780\n",
      "(Epoch 12 / 15) train acc: 0.810000; val_acc: 0.773000\n",
      "(Iteration 11761 / 14700) loss: 0.718605\n",
      "(Iteration 11762 / 14700) loss: 0.670619\n",
      "(Iteration 11763 / 14700) loss: 0.819326\n",
      "(Iteration 11764 / 14700) loss: 0.653553\n",
      "(Iteration 11765 / 14700) loss: 0.932380\n",
      "(Iteration 11766 / 14700) loss: 0.692695\n",
      "(Iteration 11767 / 14700) loss: 0.834852\n",
      "(Iteration 11768 / 14700) loss: 0.795852\n",
      "(Iteration 11769 / 14700) loss: 0.834931\n",
      "(Iteration 11770 / 14700) loss: 0.793706\n",
      "(Iteration 11771 / 14700) loss: 0.609697\n",
      "(Iteration 11772 / 14700) loss: 0.734021\n",
      "(Iteration 11773 / 14700) loss: 0.859455\n",
      "(Iteration 11774 / 14700) loss: 0.660014\n",
      "(Iteration 11775 / 14700) loss: 0.569477\n",
      "(Iteration 11776 / 14700) loss: 0.609495\n",
      "(Iteration 11777 / 14700) loss: 0.529306\n",
      "(Iteration 11778 / 14700) loss: 0.615846\n",
      "(Iteration 11779 / 14700) loss: 0.993784\n",
      "(Iteration 11780 / 14700) loss: 0.979181\n",
      "(Iteration 11781 / 14700) loss: 0.757873\n",
      "(Iteration 11782 / 14700) loss: 0.889248\n",
      "(Iteration 11783 / 14700) loss: 0.833664\n",
      "(Iteration 11784 / 14700) loss: 0.876972\n",
      "(Iteration 11785 / 14700) loss: 0.785853\n",
      "(Iteration 11786 / 14700) loss: 0.660528\n",
      "(Iteration 11787 / 14700) loss: 0.630818\n",
      "(Iteration 11788 / 14700) loss: 0.803043\n",
      "(Iteration 11789 / 14700) loss: 0.775534\n",
      "(Iteration 11790 / 14700) loss: 0.913420\n",
      "(Iteration 11791 / 14700) loss: 0.631731\n",
      "(Iteration 11792 / 14700) loss: 0.732441\n",
      "(Iteration 11793 / 14700) loss: 0.828725\n",
      "(Iteration 11794 / 14700) loss: 0.870545\n",
      "(Iteration 11795 / 14700) loss: 0.759613\n",
      "(Iteration 11796 / 14700) loss: 0.804576\n",
      "(Iteration 11797 / 14700) loss: 0.886008\n",
      "(Iteration 11798 / 14700) loss: 0.572559\n",
      "(Iteration 11799 / 14700) loss: 0.560493\n",
      "(Iteration 11800 / 14700) loss: 0.750069\n",
      "(Iteration 11801 / 14700) loss: 0.865372\n",
      "(Iteration 11802 / 14700) loss: 0.607678\n",
      "(Iteration 11803 / 14700) loss: 0.712770\n",
      "(Iteration 11804 / 14700) loss: 0.497779\n",
      "(Iteration 11805 / 14700) loss: 0.703305\n",
      "(Iteration 11806 / 14700) loss: 0.874144\n",
      "(Iteration 11807 / 14700) loss: 0.638472\n",
      "(Iteration 11808 / 14700) loss: 0.822410\n",
      "(Iteration 11809 / 14700) loss: 0.679097\n",
      "(Iteration 11810 / 14700) loss: 0.577390\n",
      "(Iteration 11811 / 14700) loss: 0.784478\n",
      "(Iteration 11812 / 14700) loss: 0.748172\n",
      "(Iteration 11813 / 14700) loss: 0.674208\n",
      "(Iteration 11814 / 14700) loss: 0.544829\n",
      "(Iteration 11815 / 14700) loss: 1.012341\n",
      "(Iteration 11816 / 14700) loss: 0.772513\n",
      "(Iteration 11817 / 14700) loss: 0.708487\n",
      "(Iteration 11818 / 14700) loss: 0.762801\n",
      "(Iteration 11819 / 14700) loss: 0.704144\n",
      "(Iteration 11820 / 14700) loss: 0.558537\n",
      "(Iteration 11821 / 14700) loss: 0.746587\n",
      "(Iteration 11822 / 14700) loss: 0.752030\n",
      "(Iteration 11823 / 14700) loss: 0.688478\n",
      "(Iteration 11824 / 14700) loss: 0.450880\n",
      "(Iteration 11825 / 14700) loss: 0.550599\n",
      "(Iteration 11826 / 14700) loss: 0.992200\n",
      "(Iteration 11827 / 14700) loss: 0.822561\n",
      "(Iteration 11828 / 14700) loss: 0.717060\n",
      "(Iteration 11829 / 14700) loss: 0.897015\n",
      "(Iteration 11830 / 14700) loss: 0.780156\n",
      "(Iteration 11831 / 14700) loss: 0.760082\n",
      "(Iteration 11832 / 14700) loss: 0.524998\n",
      "(Iteration 11833 / 14700) loss: 0.697609\n",
      "(Iteration 11834 / 14700) loss: 0.528774\n",
      "(Iteration 11835 / 14700) loss: 0.725056\n",
      "(Iteration 11836 / 14700) loss: 0.652081\n",
      "(Iteration 11837 / 14700) loss: 0.621001\n",
      "(Iteration 11838 / 14700) loss: 0.713294\n",
      "(Iteration 11839 / 14700) loss: 0.745499\n",
      "(Iteration 11840 / 14700) loss: 0.640460\n",
      "(Iteration 11841 / 14700) loss: 0.614455\n",
      "(Iteration 11842 / 14700) loss: 0.771358\n",
      "(Iteration 11843 / 14700) loss: 0.746889\n",
      "(Iteration 11844 / 14700) loss: 0.825053\n",
      "(Iteration 11845 / 14700) loss: 0.981723\n",
      "(Iteration 11846 / 14700) loss: 0.628713\n",
      "(Iteration 11847 / 14700) loss: 0.580061\n",
      "(Iteration 11848 / 14700) loss: 0.912628\n",
      "(Iteration 11849 / 14700) loss: 0.654330\n",
      "(Iteration 11850 / 14700) loss: 0.731246\n",
      "(Iteration 11851 / 14700) loss: 0.644049\n",
      "(Iteration 11852 / 14700) loss: 0.537792\n",
      "(Iteration 11853 / 14700) loss: 0.662733\n",
      "(Iteration 11854 / 14700) loss: 0.834694\n",
      "(Iteration 11855 / 14700) loss: 1.147140\n",
      "(Iteration 11856 / 14700) loss: 1.093082\n",
      "(Iteration 11857 / 14700) loss: 0.514540\n",
      "(Iteration 11858 / 14700) loss: 0.636869\n",
      "(Iteration 11859 / 14700) loss: 0.878355\n",
      "(Iteration 11860 / 14700) loss: 0.670865\n",
      "(Iteration 11861 / 14700) loss: 0.753398\n",
      "(Iteration 11862 / 14700) loss: 1.101945\n",
      "(Iteration 11863 / 14700) loss: 0.799766\n",
      "(Iteration 11864 / 14700) loss: 0.553524\n",
      "(Iteration 11865 / 14700) loss: 0.767543\n",
      "(Iteration 11866 / 14700) loss: 0.752437\n",
      "(Iteration 11867 / 14700) loss: 0.825029\n",
      "(Iteration 11868 / 14700) loss: 0.575960\n",
      "(Iteration 11869 / 14700) loss: 0.499551\n",
      "(Iteration 11870 / 14700) loss: 0.667705\n",
      "(Iteration 11871 / 14700) loss: 0.814213\n",
      "(Iteration 11872 / 14700) loss: 0.650242\n",
      "(Iteration 11873 / 14700) loss: 1.130867\n",
      "(Iteration 11874 / 14700) loss: 0.638371\n",
      "(Iteration 11875 / 14700) loss: 0.658353\n",
      "(Iteration 11876 / 14700) loss: 0.835395\n",
      "(Iteration 11877 / 14700) loss: 0.544734\n",
      "(Iteration 11878 / 14700) loss: 0.766128\n",
      "(Iteration 11879 / 14700) loss: 0.557411\n",
      "(Iteration 11880 / 14700) loss: 0.835068\n",
      "(Iteration 11881 / 14700) loss: 0.664176\n",
      "(Iteration 11882 / 14700) loss: 0.644773\n",
      "(Iteration 11883 / 14700) loss: 0.588142\n",
      "(Iteration 11884 / 14700) loss: 0.630160\n",
      "(Iteration 11885 / 14700) loss: 0.709582\n",
      "(Iteration 11886 / 14700) loss: 0.826977\n",
      "(Iteration 11887 / 14700) loss: 0.709672\n",
      "(Iteration 11888 / 14700) loss: 0.679218\n",
      "(Iteration 11889 / 14700) loss: 0.845853\n",
      "(Iteration 11890 / 14700) loss: 0.814741\n",
      "(Iteration 11891 / 14700) loss: 0.830198\n",
      "(Iteration 11892 / 14700) loss: 0.658640\n",
      "(Iteration 11893 / 14700) loss: 0.682963\n",
      "(Iteration 11894 / 14700) loss: 0.878541\n",
      "(Iteration 11895 / 14700) loss: 0.776994\n",
      "(Iteration 11896 / 14700) loss: 0.698250\n",
      "(Iteration 11897 / 14700) loss: 0.864291\n",
      "(Iteration 11898 / 14700) loss: 0.940925\n",
      "(Iteration 11899 / 14700) loss: 0.767046\n",
      "(Iteration 11900 / 14700) loss: 0.756749\n",
      "(Iteration 11901 / 14700) loss: 0.570051\n",
      "(Iteration 11902 / 14700) loss: 0.917429\n",
      "(Iteration 11903 / 14700) loss: 0.609449\n",
      "(Iteration 11904 / 14700) loss: 0.592158\n",
      "(Iteration 11905 / 14700) loss: 0.669811\n",
      "(Iteration 11906 / 14700) loss: 0.877501\n",
      "(Iteration 11907 / 14700) loss: 0.675440\n",
      "(Iteration 11908 / 14700) loss: 0.557527\n",
      "(Iteration 11909 / 14700) loss: 0.818982\n",
      "(Iteration 11910 / 14700) loss: 0.742379\n",
      "(Iteration 11911 / 14700) loss: 0.925989\n",
      "(Iteration 11912 / 14700) loss: 1.021224\n",
      "(Iteration 11913 / 14700) loss: 0.637368\n",
      "(Iteration 11914 / 14700) loss: 0.951868\n",
      "(Iteration 11915 / 14700) loss: 0.840427\n",
      "(Iteration 11916 / 14700) loss: 0.559137\n",
      "(Iteration 11917 / 14700) loss: 0.898792\n",
      "(Iteration 11918 / 14700) loss: 0.750591\n",
      "(Iteration 11919 / 14700) loss: 0.653404\n",
      "(Iteration 11920 / 14700) loss: 0.900396\n",
      "(Iteration 11921 / 14700) loss: 0.601364\n",
      "(Iteration 11922 / 14700) loss: 0.617571\n",
      "(Iteration 11923 / 14700) loss: 0.674492\n",
      "(Iteration 11924 / 14700) loss: 0.894119\n",
      "(Iteration 11925 / 14700) loss: 0.721767\n",
      "(Iteration 11926 / 14700) loss: 0.512575\n",
      "(Iteration 11927 / 14700) loss: 0.905798\n",
      "(Iteration 11928 / 14700) loss: 0.617478\n",
      "(Iteration 11929 / 14700) loss: 1.296100\n",
      "(Iteration 11930 / 14700) loss: 0.900346\n",
      "(Iteration 11931 / 14700) loss: 0.790185\n",
      "(Iteration 11932 / 14700) loss: 0.596003\n",
      "(Iteration 11933 / 14700) loss: 0.766475\n",
      "(Iteration 11934 / 14700) loss: 0.637162\n",
      "(Iteration 11935 / 14700) loss: 0.860678\n",
      "(Iteration 11936 / 14700) loss: 0.978779\n",
      "(Iteration 11937 / 14700) loss: 0.515480\n",
      "(Iteration 11938 / 14700) loss: 0.503450\n",
      "(Iteration 11939 / 14700) loss: 0.813961\n",
      "(Iteration 11940 / 14700) loss: 0.601926\n",
      "(Iteration 11941 / 14700) loss: 0.599606\n",
      "(Iteration 11942 / 14700) loss: 0.677087\n",
      "(Iteration 11943 / 14700) loss: 0.729664\n",
      "(Iteration 11944 / 14700) loss: 0.820639\n",
      "(Iteration 11945 / 14700) loss: 0.795739\n",
      "(Iteration 11946 / 14700) loss: 0.691619\n",
      "(Iteration 11947 / 14700) loss: 0.520132\n",
      "(Iteration 11948 / 14700) loss: 0.779558\n",
      "(Iteration 11949 / 14700) loss: 0.699018\n",
      "(Iteration 11950 / 14700) loss: 0.521260\n",
      "(Iteration 11951 / 14700) loss: 0.785193\n",
      "(Iteration 11952 / 14700) loss: 0.811663\n",
      "(Iteration 11953 / 14700) loss: 0.883264\n",
      "(Iteration 11954 / 14700) loss: 0.500739\n",
      "(Iteration 11955 / 14700) loss: 0.756401\n",
      "(Iteration 11956 / 14700) loss: 0.787650\n",
      "(Iteration 11957 / 14700) loss: 0.732999\n",
      "(Iteration 11958 / 14700) loss: 0.649609\n",
      "(Iteration 11959 / 14700) loss: 0.829164\n",
      "(Iteration 11960 / 14700) loss: 0.763277\n",
      "(Iteration 11961 / 14700) loss: 0.563622\n",
      "(Iteration 11962 / 14700) loss: 0.590019\n",
      "(Iteration 11963 / 14700) loss: 0.730876\n",
      "(Iteration 11964 / 14700) loss: 0.780286\n",
      "(Iteration 11965 / 14700) loss: 0.731884\n",
      "(Iteration 11966 / 14700) loss: 0.561928\n",
      "(Iteration 11967 / 14700) loss: 0.674627\n",
      "(Iteration 11968 / 14700) loss: 0.516433\n",
      "(Iteration 11969 / 14700) loss: 0.577768\n",
      "(Iteration 11970 / 14700) loss: 0.772017\n",
      "(Iteration 11971 / 14700) loss: 0.500116\n",
      "(Iteration 11972 / 14700) loss: 0.852657\n",
      "(Iteration 11973 / 14700) loss: 0.641663\n",
      "(Iteration 11974 / 14700) loss: 0.685459\n",
      "(Iteration 11975 / 14700) loss: 0.914819\n",
      "(Iteration 11976 / 14700) loss: 0.741450\n",
      "(Iteration 11977 / 14700) loss: 0.789482\n",
      "(Iteration 11978 / 14700) loss: 0.823544\n",
      "(Iteration 11979 / 14700) loss: 0.654037\n",
      "(Iteration 11980 / 14700) loss: 0.861559\n",
      "(Iteration 11981 / 14700) loss: 0.630880\n",
      "(Iteration 11982 / 14700) loss: 0.885895\n",
      "(Iteration 11983 / 14700) loss: 0.620290\n",
      "(Iteration 11984 / 14700) loss: 0.713158\n",
      "(Iteration 11985 / 14700) loss: 0.610145\n",
      "(Iteration 11986 / 14700) loss: 0.815864\n",
      "(Iteration 11987 / 14700) loss: 0.607335\n",
      "(Iteration 11988 / 14700) loss: 0.550035\n",
      "(Iteration 11989 / 14700) loss: 0.774893\n",
      "(Iteration 11990 / 14700) loss: 0.763531\n",
      "(Iteration 11991 / 14700) loss: 0.623452\n",
      "(Iteration 11992 / 14700) loss: 0.594962\n",
      "(Iteration 11993 / 14700) loss: 0.661837\n",
      "(Iteration 11994 / 14700) loss: 0.690160\n",
      "(Iteration 11995 / 14700) loss: 0.688680\n",
      "(Iteration 11996 / 14700) loss: 0.656572\n",
      "(Iteration 11997 / 14700) loss: 0.644119\n",
      "(Iteration 11998 / 14700) loss: 0.688034\n",
      "(Iteration 11999 / 14700) loss: 0.797613\n",
      "(Iteration 12000 / 14700) loss: 0.893641\n",
      "(Iteration 12001 / 14700) loss: 0.945420\n",
      "(Iteration 12002 / 14700) loss: 0.648915\n",
      "(Iteration 12003 / 14700) loss: 0.929522\n",
      "(Iteration 12004 / 14700) loss: 0.723890\n",
      "(Iteration 12005 / 14700) loss: 0.617040\n",
      "(Iteration 12006 / 14700) loss: 0.730755\n",
      "(Iteration 12007 / 14700) loss: 0.548193\n",
      "(Iteration 12008 / 14700) loss: 0.626300\n",
      "(Iteration 12009 / 14700) loss: 0.761533\n",
      "(Iteration 12010 / 14700) loss: 0.554234\n",
      "(Iteration 12011 / 14700) loss: 0.653666\n",
      "(Iteration 12012 / 14700) loss: 0.719891\n",
      "(Iteration 12013 / 14700) loss: 0.780807\n",
      "(Iteration 12014 / 14700) loss: 0.693217\n",
      "(Iteration 12015 / 14700) loss: 0.685075\n",
      "(Iteration 12016 / 14700) loss: 0.775495\n",
      "(Iteration 12017 / 14700) loss: 0.823814\n",
      "(Iteration 12018 / 14700) loss: 0.778391\n",
      "(Iteration 12019 / 14700) loss: 0.545948\n",
      "(Iteration 12020 / 14700) loss: 0.763115\n",
      "(Iteration 12021 / 14700) loss: 0.690654\n",
      "(Iteration 12022 / 14700) loss: 0.629745\n",
      "(Iteration 12023 / 14700) loss: 0.684955\n",
      "(Iteration 12024 / 14700) loss: 0.740993\n",
      "(Iteration 12025 / 14700) loss: 0.781046\n",
      "(Iteration 12026 / 14700) loss: 0.634357\n",
      "(Iteration 12027 / 14700) loss: 0.726310\n",
      "(Iteration 12028 / 14700) loss: 0.944886\n",
      "(Iteration 12029 / 14700) loss: 0.861602\n",
      "(Iteration 12030 / 14700) loss: 0.642252\n",
      "(Iteration 12031 / 14700) loss: 0.806204\n",
      "(Iteration 12032 / 14700) loss: 0.756861\n",
      "(Iteration 12033 / 14700) loss: 0.824032\n",
      "(Iteration 12034 / 14700) loss: 0.572704\n",
      "(Iteration 12035 / 14700) loss: 0.840094\n",
      "(Iteration 12036 / 14700) loss: 0.953135\n",
      "(Iteration 12037 / 14700) loss: 0.650037\n",
      "(Iteration 12038 / 14700) loss: 0.795350\n",
      "(Iteration 12039 / 14700) loss: 0.622719\n",
      "(Iteration 12040 / 14700) loss: 0.860677\n",
      "(Iteration 12041 / 14700) loss: 0.550616\n",
      "(Iteration 12042 / 14700) loss: 0.675086\n",
      "(Iteration 12043 / 14700) loss: 0.724436\n",
      "(Iteration 12044 / 14700) loss: 0.636348\n",
      "(Iteration 12045 / 14700) loss: 0.681165\n",
      "(Iteration 12046 / 14700) loss: 0.583754\n",
      "(Iteration 12047 / 14700) loss: 0.659594\n",
      "(Iteration 12048 / 14700) loss: 0.615843\n",
      "(Iteration 12049 / 14700) loss: 0.783160\n",
      "(Iteration 12050 / 14700) loss: 0.726266\n",
      "(Iteration 12051 / 14700) loss: 0.503002\n",
      "(Iteration 12052 / 14700) loss: 0.580324\n",
      "(Iteration 12053 / 14700) loss: 0.565833\n",
      "(Iteration 12054 / 14700) loss: 0.734643\n",
      "(Iteration 12055 / 14700) loss: 0.640888\n",
      "(Iteration 12056 / 14700) loss: 0.767903\n",
      "(Iteration 12057 / 14700) loss: 0.637082\n",
      "(Iteration 12058 / 14700) loss: 0.783831\n",
      "(Iteration 12059 / 14700) loss: 0.578792\n",
      "(Iteration 12060 / 14700) loss: 0.604465\n",
      "(Iteration 12061 / 14700) loss: 0.449464\n",
      "(Iteration 12062 / 14700) loss: 0.680336\n",
      "(Iteration 12063 / 14700) loss: 0.911422\n",
      "(Iteration 12064 / 14700) loss: 0.873650\n",
      "(Iteration 12065 / 14700) loss: 0.680411\n",
      "(Iteration 12066 / 14700) loss: 1.065642\n",
      "(Iteration 12067 / 14700) loss: 0.704715\n",
      "(Iteration 12068 / 14700) loss: 0.891863\n",
      "(Iteration 12069 / 14700) loss: 0.604451\n",
      "(Iteration 12070 / 14700) loss: 0.553107\n",
      "(Iteration 12071 / 14700) loss: 0.588162\n",
      "(Iteration 12072 / 14700) loss: 0.745576\n",
      "(Iteration 12073 / 14700) loss: 0.692793\n",
      "(Iteration 12074 / 14700) loss: 0.710283\n",
      "(Iteration 12075 / 14700) loss: 0.706756\n",
      "(Iteration 12076 / 14700) loss: 0.830260\n",
      "(Iteration 12077 / 14700) loss: 0.647155\n",
      "(Iteration 12078 / 14700) loss: 0.940636\n",
      "(Iteration 12079 / 14700) loss: 0.683418\n",
      "(Iteration 12080 / 14700) loss: 0.919120\n",
      "(Iteration 12081 / 14700) loss: 0.738538\n",
      "(Iteration 12082 / 14700) loss: 0.782471\n",
      "(Iteration 12083 / 14700) loss: 0.609337\n",
      "(Iteration 12084 / 14700) loss: 0.734478\n",
      "(Iteration 12085 / 14700) loss: 0.658871\n",
      "(Iteration 12086 / 14700) loss: 0.700007\n",
      "(Iteration 12087 / 14700) loss: 0.739104\n",
      "(Iteration 12088 / 14700) loss: 0.799164\n",
      "(Iteration 12089 / 14700) loss: 0.757396\n",
      "(Iteration 12090 / 14700) loss: 0.617869\n",
      "(Iteration 12091 / 14700) loss: 0.563788\n",
      "(Iteration 12092 / 14700) loss: 0.632067\n",
      "(Iteration 12093 / 14700) loss: 0.850389\n",
      "(Iteration 12094 / 14700) loss: 0.683768\n",
      "(Iteration 12095 / 14700) loss: 0.719382\n",
      "(Iteration 12096 / 14700) loss: 0.765447\n",
      "(Iteration 12097 / 14700) loss: 0.880898\n",
      "(Iteration 12098 / 14700) loss: 0.519048\n",
      "(Iteration 12099 / 14700) loss: 0.552246\n",
      "(Iteration 12100 / 14700) loss: 0.604996\n",
      "(Iteration 12101 / 14700) loss: 0.910617\n",
      "(Iteration 12102 / 14700) loss: 0.693737\n",
      "(Iteration 12103 / 14700) loss: 0.521892\n",
      "(Iteration 12104 / 14700) loss: 0.823185\n",
      "(Iteration 12105 / 14700) loss: 0.757113\n",
      "(Iteration 12106 / 14700) loss: 0.674623\n",
      "(Iteration 12107 / 14700) loss: 0.706324\n",
      "(Iteration 12108 / 14700) loss: 0.806454\n",
      "(Iteration 12109 / 14700) loss: 0.556664\n",
      "(Iteration 12110 / 14700) loss: 0.610799\n",
      "(Iteration 12111 / 14700) loss: 0.698220\n",
      "(Iteration 12112 / 14700) loss: 0.709999\n",
      "(Iteration 12113 / 14700) loss: 0.507238\n",
      "(Iteration 12114 / 14700) loss: 0.827787\n",
      "(Iteration 12115 / 14700) loss: 0.680049\n",
      "(Iteration 12116 / 14700) loss: 0.567721\n",
      "(Iteration 12117 / 14700) loss: 0.740126\n",
      "(Iteration 12118 / 14700) loss: 0.854429\n",
      "(Iteration 12119 / 14700) loss: 0.550281\n",
      "(Iteration 12120 / 14700) loss: 0.757580\n",
      "(Iteration 12121 / 14700) loss: 0.947814\n",
      "(Iteration 12122 / 14700) loss: 0.666061\n",
      "(Iteration 12123 / 14700) loss: 0.743294\n",
      "(Iteration 12124 / 14700) loss: 0.629594\n",
      "(Iteration 12125 / 14700) loss: 0.741413\n",
      "(Iteration 12126 / 14700) loss: 0.863680\n",
      "(Iteration 12127 / 14700) loss: 0.655482\n",
      "(Iteration 12128 / 14700) loss: 0.724678\n",
      "(Iteration 12129 / 14700) loss: 0.780120\n",
      "(Iteration 12130 / 14700) loss: 0.873973\n",
      "(Iteration 12131 / 14700) loss: 0.848879\n",
      "(Iteration 12132 / 14700) loss: 1.041426\n",
      "(Iteration 12133 / 14700) loss: 0.886857\n",
      "(Iteration 12134 / 14700) loss: 0.790820\n",
      "(Iteration 12135 / 14700) loss: 0.759304\n",
      "(Iteration 12136 / 14700) loss: 0.852758\n",
      "(Iteration 12137 / 14700) loss: 0.462752\n",
      "(Iteration 12138 / 14700) loss: 0.745594\n",
      "(Iteration 12139 / 14700) loss: 0.501796\n",
      "(Iteration 12140 / 14700) loss: 0.587479\n",
      "(Iteration 12141 / 14700) loss: 0.523564\n",
      "(Iteration 12142 / 14700) loss: 0.645891\n",
      "(Iteration 12143 / 14700) loss: 0.883663\n",
      "(Iteration 12144 / 14700) loss: 0.601038\n",
      "(Iteration 12145 / 14700) loss: 0.708554\n",
      "(Iteration 12146 / 14700) loss: 0.552721\n",
      "(Iteration 12147 / 14700) loss: 0.657258\n",
      "(Iteration 12148 / 14700) loss: 0.746809\n",
      "(Iteration 12149 / 14700) loss: 0.908364\n",
      "(Iteration 12150 / 14700) loss: 0.531232\n",
      "(Iteration 12151 / 14700) loss: 0.934357\n",
      "(Iteration 12152 / 14700) loss: 0.792809\n",
      "(Iteration 12153 / 14700) loss: 0.694290\n",
      "(Iteration 12154 / 14700) loss: 0.980110\n",
      "(Iteration 12155 / 14700) loss: 0.780121\n",
      "(Iteration 12156 / 14700) loss: 0.613788\n",
      "(Iteration 12157 / 14700) loss: 0.568782\n",
      "(Iteration 12158 / 14700) loss: 0.872611\n",
      "(Iteration 12159 / 14700) loss: 0.700218\n",
      "(Iteration 12160 / 14700) loss: 0.928349\n",
      "(Iteration 12161 / 14700) loss: 0.565604\n",
      "(Iteration 12162 / 14700) loss: 0.681204\n",
      "(Iteration 12163 / 14700) loss: 0.703073\n",
      "(Iteration 12164 / 14700) loss: 0.859569\n",
      "(Iteration 12165 / 14700) loss: 0.732703\n",
      "(Iteration 12166 / 14700) loss: 0.893490\n",
      "(Iteration 12167 / 14700) loss: 0.835980\n",
      "(Iteration 12168 / 14700) loss: 0.852742\n",
      "(Iteration 12169 / 14700) loss: 0.573483\n",
      "(Iteration 12170 / 14700) loss: 0.928251\n",
      "(Iteration 12171 / 14700) loss: 0.548629\n",
      "(Iteration 12172 / 14700) loss: 0.520187\n",
      "(Iteration 12173 / 14700) loss: 0.548117\n",
      "(Iteration 12174 / 14700) loss: 0.994020\n",
      "(Iteration 12175 / 14700) loss: 0.903399\n",
      "(Iteration 12176 / 14700) loss: 0.698867\n",
      "(Iteration 12177 / 14700) loss: 0.797441\n",
      "(Iteration 12178 / 14700) loss: 0.692334\n",
      "(Iteration 12179 / 14700) loss: 0.602726\n",
      "(Iteration 12180 / 14700) loss: 0.736957\n",
      "(Iteration 12181 / 14700) loss: 0.623877\n",
      "(Iteration 12182 / 14700) loss: 0.689599\n",
      "(Iteration 12183 / 14700) loss: 0.660095\n",
      "(Iteration 12184 / 14700) loss: 0.834010\n",
      "(Iteration 12185 / 14700) loss: 0.575314\n",
      "(Iteration 12186 / 14700) loss: 0.768114\n",
      "(Iteration 12187 / 14700) loss: 0.974822\n",
      "(Iteration 12188 / 14700) loss: 0.855035\n",
      "(Iteration 12189 / 14700) loss: 0.657385\n",
      "(Iteration 12190 / 14700) loss: 0.693706\n",
      "(Iteration 12191 / 14700) loss: 0.658266\n",
      "(Iteration 12192 / 14700) loss: 0.695343\n",
      "(Iteration 12193 / 14700) loss: 0.795503\n",
      "(Iteration 12194 / 14700) loss: 0.677829\n",
      "(Iteration 12195 / 14700) loss: 0.645986\n",
      "(Iteration 12196 / 14700) loss: 0.821958\n",
      "(Iteration 12197 / 14700) loss: 0.532267\n",
      "(Iteration 12198 / 14700) loss: 0.918367\n",
      "(Iteration 12199 / 14700) loss: 0.606217\n",
      "(Iteration 12200 / 14700) loss: 0.761137\n",
      "(Iteration 12201 / 14700) loss: 0.631468\n",
      "(Iteration 12202 / 14700) loss: 0.559177\n",
      "(Iteration 12203 / 14700) loss: 0.705195\n",
      "(Iteration 12204 / 14700) loss: 0.756012\n",
      "(Iteration 12205 / 14700) loss: 0.661450\n",
      "(Iteration 12206 / 14700) loss: 0.647765\n",
      "(Iteration 12207 / 14700) loss: 0.629969\n",
      "(Iteration 12208 / 14700) loss: 0.669091\n",
      "(Iteration 12209 / 14700) loss: 0.820541\n",
      "(Iteration 12210 / 14700) loss: 0.597976\n",
      "(Iteration 12211 / 14700) loss: 0.688829\n",
      "(Iteration 12212 / 14700) loss: 0.722942\n",
      "(Iteration 12213 / 14700) loss: 1.032986\n",
      "(Iteration 12214 / 14700) loss: 0.722999\n",
      "(Iteration 12215 / 14700) loss: 0.726140\n",
      "(Iteration 12216 / 14700) loss: 0.502427\n",
      "(Iteration 12217 / 14700) loss: 0.792624\n",
      "(Iteration 12218 / 14700) loss: 0.872030\n",
      "(Iteration 12219 / 14700) loss: 0.801709\n",
      "(Iteration 12220 / 14700) loss: 0.479874\n",
      "(Iteration 12221 / 14700) loss: 0.881484\n",
      "(Iteration 12222 / 14700) loss: 0.627848\n",
      "(Iteration 12223 / 14700) loss: 0.541944\n",
      "(Iteration 12224 / 14700) loss: 1.057137\n",
      "(Iteration 12225 / 14700) loss: 1.009841\n",
      "(Iteration 12226 / 14700) loss: 1.097265\n",
      "(Iteration 12227 / 14700) loss: 0.563555\n",
      "(Iteration 12228 / 14700) loss: 0.762053\n",
      "(Iteration 12229 / 14700) loss: 0.538907\n",
      "(Iteration 12230 / 14700) loss: 0.636124\n",
      "(Iteration 12231 / 14700) loss: 0.784998\n",
      "(Iteration 12232 / 14700) loss: 0.487775\n",
      "(Iteration 12233 / 14700) loss: 0.836192\n",
      "(Iteration 12234 / 14700) loss: 0.724514\n",
      "(Iteration 12235 / 14700) loss: 0.723589\n",
      "(Iteration 12236 / 14700) loss: 0.782945\n",
      "(Iteration 12237 / 14700) loss: 0.539973\n",
      "(Iteration 12238 / 14700) loss: 0.820467\n",
      "(Iteration 12239 / 14700) loss: 0.666712\n",
      "(Iteration 12240 / 14700) loss: 0.554721\n",
      "(Iteration 12241 / 14700) loss: 0.682666\n",
      "(Iteration 12242 / 14700) loss: 0.537394\n",
      "(Iteration 12243 / 14700) loss: 1.280542\n",
      "(Iteration 12244 / 14700) loss: 0.775119\n",
      "(Iteration 12245 / 14700) loss: 0.520717\n",
      "(Iteration 12246 / 14700) loss: 0.637570\n",
      "(Iteration 12247 / 14700) loss: 0.836207\n",
      "(Iteration 12248 / 14700) loss: 0.637398\n",
      "(Iteration 12249 / 14700) loss: 0.873185\n",
      "(Iteration 12250 / 14700) loss: 0.610534\n",
      "(Iteration 12251 / 14700) loss: 0.811405\n",
      "(Iteration 12252 / 14700) loss: 0.907064\n",
      "(Iteration 12253 / 14700) loss: 0.653410\n",
      "(Iteration 12254 / 14700) loss: 0.757407\n",
      "(Iteration 12255 / 14700) loss: 0.593549\n",
      "(Iteration 12256 / 14700) loss: 0.556238\n",
      "(Iteration 12257 / 14700) loss: 0.689154\n",
      "(Iteration 12258 / 14700) loss: 0.670413\n",
      "(Iteration 12259 / 14700) loss: 0.675200\n",
      "(Iteration 12260 / 14700) loss: 0.854346\n",
      "(Iteration 12261 / 14700) loss: 0.812609\n",
      "(Iteration 12262 / 14700) loss: 0.589220\n",
      "(Iteration 12263 / 14700) loss: 0.797462\n",
      "(Iteration 12264 / 14700) loss: 0.729746\n",
      "(Iteration 12265 / 14700) loss: 0.628002\n",
      "(Iteration 12266 / 14700) loss: 0.754722\n",
      "(Iteration 12267 / 14700) loss: 0.867352\n",
      "(Iteration 12268 / 14700) loss: 0.689369\n",
      "(Iteration 12269 / 14700) loss: 0.698693\n",
      "(Iteration 12270 / 14700) loss: 0.711172\n",
      "(Iteration 12271 / 14700) loss: 0.557738\n",
      "(Iteration 12272 / 14700) loss: 0.702801\n",
      "(Iteration 12273 / 14700) loss: 0.777068\n",
      "(Iteration 12274 / 14700) loss: 0.785661\n",
      "(Iteration 12275 / 14700) loss: 0.986278\n",
      "(Iteration 12276 / 14700) loss: 0.698977\n",
      "(Iteration 12277 / 14700) loss: 0.665520\n",
      "(Iteration 12278 / 14700) loss: 0.569725\n",
      "(Iteration 12279 / 14700) loss: 0.652458\n",
      "(Iteration 12280 / 14700) loss: 0.726591\n",
      "(Iteration 12281 / 14700) loss: 0.673410\n",
      "(Iteration 12282 / 14700) loss: 0.757665\n",
      "(Iteration 12283 / 14700) loss: 0.750372\n",
      "(Iteration 12284 / 14700) loss: 0.643461\n",
      "(Iteration 12285 / 14700) loss: 0.610117\n",
      "(Iteration 12286 / 14700) loss: 0.682660\n",
      "(Iteration 12287 / 14700) loss: 0.747696\n",
      "(Iteration 12288 / 14700) loss: 0.905549\n",
      "(Iteration 12289 / 14700) loss: 0.530229\n",
      "(Iteration 12290 / 14700) loss: 0.687479\n",
      "(Iteration 12291 / 14700) loss: 0.736558\n",
      "(Iteration 12292 / 14700) loss: 0.848446\n",
      "(Iteration 12293 / 14700) loss: 0.506006\n",
      "(Iteration 12294 / 14700) loss: 0.701814\n",
      "(Iteration 12295 / 14700) loss: 0.864856\n",
      "(Iteration 12296 / 14700) loss: 0.506332\n",
      "(Iteration 12297 / 14700) loss: 0.749925\n",
      "(Iteration 12298 / 14700) loss: 0.632889\n",
      "(Iteration 12299 / 14700) loss: 0.770319\n",
      "(Iteration 12300 / 14700) loss: 0.739370\n",
      "(Iteration 12301 / 14700) loss: 0.684300\n",
      "(Iteration 12302 / 14700) loss: 0.805558\n",
      "(Iteration 12303 / 14700) loss: 0.868259\n",
      "(Iteration 12304 / 14700) loss: 0.679812\n",
      "(Iteration 12305 / 14700) loss: 0.564342\n",
      "(Iteration 12306 / 14700) loss: 0.996277\n",
      "(Iteration 12307 / 14700) loss: 0.800822\n",
      "(Iteration 12308 / 14700) loss: 0.595848\n",
      "(Iteration 12309 / 14700) loss: 0.599294\n",
      "(Iteration 12310 / 14700) loss: 0.760699\n",
      "(Iteration 12311 / 14700) loss: 0.618768\n",
      "(Iteration 12312 / 14700) loss: 0.924661\n",
      "(Iteration 12313 / 14700) loss: 0.689678\n",
      "(Iteration 12314 / 14700) loss: 0.691002\n",
      "(Iteration 12315 / 14700) loss: 0.783606\n",
      "(Iteration 12316 / 14700) loss: 0.949532\n",
      "(Iteration 12317 / 14700) loss: 0.615121\n",
      "(Iteration 12318 / 14700) loss: 0.539686\n",
      "(Iteration 12319 / 14700) loss: 0.547075\n",
      "(Iteration 12320 / 14700) loss: 0.583333\n",
      "(Iteration 12321 / 14700) loss: 0.799702\n",
      "(Iteration 12322 / 14700) loss: 0.741744\n",
      "(Iteration 12323 / 14700) loss: 0.863064\n",
      "(Iteration 12324 / 14700) loss: 0.888711\n",
      "(Iteration 12325 / 14700) loss: 0.540703\n",
      "(Iteration 12326 / 14700) loss: 0.851474\n",
      "(Iteration 12327 / 14700) loss: 0.773682\n",
      "(Iteration 12328 / 14700) loss: 0.801590\n",
      "(Iteration 12329 / 14700) loss: 0.711900\n",
      "(Iteration 12330 / 14700) loss: 0.726970\n",
      "(Iteration 12331 / 14700) loss: 0.686770\n",
      "(Iteration 12332 / 14700) loss: 0.533652\n",
      "(Iteration 12333 / 14700) loss: 0.783305\n",
      "(Iteration 12334 / 14700) loss: 0.568605\n",
      "(Iteration 12335 / 14700) loss: 0.748715\n",
      "(Iteration 12336 / 14700) loss: 0.694110\n",
      "(Iteration 12337 / 14700) loss: 0.565246\n",
      "(Iteration 12338 / 14700) loss: 0.607351\n",
      "(Iteration 12339 / 14700) loss: 0.981954\n",
      "(Iteration 12340 / 14700) loss: 0.969762\n",
      "(Iteration 12341 / 14700) loss: 0.751517\n",
      "(Iteration 12342 / 14700) loss: 0.677299\n",
      "(Iteration 12343 / 14700) loss: 0.845947\n",
      "(Iteration 12344 / 14700) loss: 0.819776\n",
      "(Iteration 12345 / 14700) loss: 0.579191\n",
      "(Iteration 12346 / 14700) loss: 0.745798\n",
      "(Iteration 12347 / 14700) loss: 0.651898\n",
      "(Iteration 12348 / 14700) loss: 0.575224\n",
      "(Iteration 12349 / 14700) loss: 0.783369\n",
      "(Iteration 12350 / 14700) loss: 0.619260\n",
      "(Iteration 12351 / 14700) loss: 0.882796\n",
      "(Iteration 12352 / 14700) loss: 0.558937\n",
      "(Iteration 12353 / 14700) loss: 0.644111\n",
      "(Iteration 12354 / 14700) loss: 0.819389\n",
      "(Iteration 12355 / 14700) loss: 0.662948\n",
      "(Iteration 12356 / 14700) loss: 0.821702\n",
      "(Iteration 12357 / 14700) loss: 0.688728\n",
      "(Iteration 12358 / 14700) loss: 0.966027\n",
      "(Iteration 12359 / 14700) loss: 0.724709\n",
      "(Iteration 12360 / 14700) loss: 0.933345\n",
      "(Iteration 12361 / 14700) loss: 0.754259\n",
      "(Iteration 12362 / 14700) loss: 0.754828\n",
      "(Iteration 12363 / 14700) loss: 0.817814\n",
      "(Iteration 12364 / 14700) loss: 0.678881\n",
      "(Iteration 12365 / 14700) loss: 0.823564\n",
      "(Iteration 12366 / 14700) loss: 0.737337\n",
      "(Iteration 12367 / 14700) loss: 0.658870\n",
      "(Iteration 12368 / 14700) loss: 0.874630\n",
      "(Iteration 12369 / 14700) loss: 0.765627\n",
      "(Iteration 12370 / 14700) loss: 0.738436\n",
      "(Iteration 12371 / 14700) loss: 0.746813\n",
      "(Iteration 12372 / 14700) loss: 0.604803\n",
      "(Iteration 12373 / 14700) loss: 0.660430\n",
      "(Iteration 12374 / 14700) loss: 0.668984\n",
      "(Iteration 12375 / 14700) loss: 0.694942\n",
      "(Iteration 12376 / 14700) loss: 0.803080\n",
      "(Iteration 12377 / 14700) loss: 0.704136\n",
      "(Iteration 12378 / 14700) loss: 0.819246\n",
      "(Iteration 12379 / 14700) loss: 0.626984\n",
      "(Iteration 12380 / 14700) loss: 0.713980\n",
      "(Iteration 12381 / 14700) loss: 0.641931\n",
      "(Iteration 12382 / 14700) loss: 0.528988\n",
      "(Iteration 12383 / 14700) loss: 0.738903\n",
      "(Iteration 12384 / 14700) loss: 0.725181\n",
      "(Iteration 12385 / 14700) loss: 0.553884\n",
      "(Iteration 12386 / 14700) loss: 0.893194\n",
      "(Iteration 12387 / 14700) loss: 0.658477\n",
      "(Iteration 12388 / 14700) loss: 0.739014\n",
      "(Iteration 12389 / 14700) loss: 0.680298\n",
      "(Iteration 12390 / 14700) loss: 0.673386\n",
      "(Iteration 12391 / 14700) loss: 0.741912\n",
      "(Iteration 12392 / 14700) loss: 0.866345\n",
      "(Iteration 12393 / 14700) loss: 0.732299\n",
      "(Iteration 12394 / 14700) loss: 0.938363\n",
      "(Iteration 12395 / 14700) loss: 0.585568\n",
      "(Iteration 12396 / 14700) loss: 0.847161\n",
      "(Iteration 12397 / 14700) loss: 0.727963\n",
      "(Iteration 12398 / 14700) loss: 0.794006\n",
      "(Iteration 12399 / 14700) loss: 0.655472\n",
      "(Iteration 12400 / 14700) loss: 0.654894\n",
      "(Iteration 12401 / 14700) loss: 0.621714\n",
      "(Iteration 12402 / 14700) loss: 0.683682\n",
      "(Iteration 12403 / 14700) loss: 0.810485\n",
      "(Iteration 12404 / 14700) loss: 0.779733\n",
      "(Iteration 12405 / 14700) loss: 0.814603\n",
      "(Iteration 12406 / 14700) loss: 0.804551\n",
      "(Iteration 12407 / 14700) loss: 0.726503\n",
      "(Iteration 12408 / 14700) loss: 0.575280\n",
      "(Iteration 12409 / 14700) loss: 0.828235\n",
      "(Iteration 12410 / 14700) loss: 0.477460\n",
      "(Iteration 12411 / 14700) loss: 1.186508\n",
      "(Iteration 12412 / 14700) loss: 0.775043\n",
      "(Iteration 12413 / 14700) loss: 0.733107\n",
      "(Iteration 12414 / 14700) loss: 0.613424\n",
      "(Iteration 12415 / 14700) loss: 0.668335\n",
      "(Iteration 12416 / 14700) loss: 0.760008\n",
      "(Iteration 12417 / 14700) loss: 0.665522\n",
      "(Iteration 12418 / 14700) loss: 0.648655\n",
      "(Iteration 12419 / 14700) loss: 0.621560\n",
      "(Iteration 12420 / 14700) loss: 0.514683\n",
      "(Iteration 12421 / 14700) loss: 0.802596\n",
      "(Iteration 12422 / 14700) loss: 0.502626\n",
      "(Iteration 12423 / 14700) loss: 0.847357\n",
      "(Iteration 12424 / 14700) loss: 0.684686\n",
      "(Iteration 12425 / 14700) loss: 0.711609\n",
      "(Iteration 12426 / 14700) loss: 0.584867\n",
      "(Iteration 12427 / 14700) loss: 0.720206\n",
      "(Iteration 12428 / 14700) loss: 0.874435\n",
      "(Iteration 12429 / 14700) loss: 0.742183\n",
      "(Iteration 12430 / 14700) loss: 0.799735\n",
      "(Iteration 12431 / 14700) loss: 0.552464\n",
      "(Iteration 12432 / 14700) loss: 0.550344\n",
      "(Iteration 12433 / 14700) loss: 0.529822\n",
      "(Iteration 12434 / 14700) loss: 0.515046\n",
      "(Iteration 12435 / 14700) loss: 0.694519\n",
      "(Iteration 12436 / 14700) loss: 0.551772\n",
      "(Iteration 12437 / 14700) loss: 0.876827\n",
      "(Iteration 12438 / 14700) loss: 0.989897\n",
      "(Iteration 12439 / 14700) loss: 0.730089\n",
      "(Iteration 12440 / 14700) loss: 0.732685\n",
      "(Iteration 12441 / 14700) loss: 0.849050\n",
      "(Iteration 12442 / 14700) loss: 0.754109\n",
      "(Iteration 12443 / 14700) loss: 0.879790\n",
      "(Iteration 12444 / 14700) loss: 0.868035\n",
      "(Iteration 12445 / 14700) loss: 0.696710\n",
      "(Iteration 12446 / 14700) loss: 0.606409\n",
      "(Iteration 12447 / 14700) loss: 0.488321\n",
      "(Iteration 12448 / 14700) loss: 0.711989\n",
      "(Iteration 12449 / 14700) loss: 0.866902\n",
      "(Iteration 12450 / 14700) loss: 0.674335\n",
      "(Iteration 12451 / 14700) loss: 0.648793\n",
      "(Iteration 12452 / 14700) loss: 0.686713\n",
      "(Iteration 12453 / 14700) loss: 0.853196\n",
      "(Iteration 12454 / 14700) loss: 1.103381\n",
      "(Iteration 12455 / 14700) loss: 0.772226\n",
      "(Iteration 12456 / 14700) loss: 0.953951\n",
      "(Iteration 12457 / 14700) loss: 0.663830\n",
      "(Iteration 12458 / 14700) loss: 0.683652\n",
      "(Iteration 12459 / 14700) loss: 0.573421\n",
      "(Iteration 12460 / 14700) loss: 0.532212\n",
      "(Iteration 12461 / 14700) loss: 0.813700\n",
      "(Iteration 12462 / 14700) loss: 0.620217\n",
      "(Iteration 12463 / 14700) loss: 0.579466\n",
      "(Iteration 12464 / 14700) loss: 0.547071\n",
      "(Iteration 12465 / 14700) loss: 0.730674\n",
      "(Iteration 12466 / 14700) loss: 0.951267\n",
      "(Iteration 12467 / 14700) loss: 0.714201\n",
      "(Iteration 12468 / 14700) loss: 0.715396\n",
      "(Iteration 12469 / 14700) loss: 0.679122\n",
      "(Iteration 12470 / 14700) loss: 0.713503\n",
      "(Iteration 12471 / 14700) loss: 0.711900\n",
      "(Iteration 12472 / 14700) loss: 0.718728\n",
      "(Iteration 12473 / 14700) loss: 0.504987\n",
      "(Iteration 12474 / 14700) loss: 0.676624\n",
      "(Iteration 12475 / 14700) loss: 0.797616\n",
      "(Iteration 12476 / 14700) loss: 0.903142\n",
      "(Iteration 12477 / 14700) loss: 0.550676\n",
      "(Iteration 12478 / 14700) loss: 0.758822\n",
      "(Iteration 12479 / 14700) loss: 0.621332\n",
      "(Iteration 12480 / 14700) loss: 0.592506\n",
      "(Iteration 12481 / 14700) loss: 0.849648\n",
      "(Iteration 12482 / 14700) loss: 0.682112\n",
      "(Iteration 12483 / 14700) loss: 0.904267\n",
      "(Iteration 12484 / 14700) loss: 0.933973\n",
      "(Iteration 12485 / 14700) loss: 0.805910\n",
      "(Iteration 12486 / 14700) loss: 0.836912\n",
      "(Iteration 12487 / 14700) loss: 1.104635\n",
      "(Iteration 12488 / 14700) loss: 0.551152\n",
      "(Iteration 12489 / 14700) loss: 0.555379\n",
      "(Iteration 12490 / 14700) loss: 0.551019\n",
      "(Iteration 12491 / 14700) loss: 0.904057\n",
      "(Iteration 12492 / 14700) loss: 0.812323\n",
      "(Iteration 12493 / 14700) loss: 0.704370\n",
      "(Iteration 12494 / 14700) loss: 0.730494\n",
      "(Iteration 12495 / 14700) loss: 0.669369\n",
      "(Iteration 12496 / 14700) loss: 0.476486\n",
      "(Iteration 12497 / 14700) loss: 0.766098\n",
      "(Iteration 12498 / 14700) loss: 0.735825\n",
      "(Iteration 12499 / 14700) loss: 0.578723\n",
      "(Iteration 12500 / 14700) loss: 0.796473\n",
      "(Iteration 12501 / 14700) loss: 0.776261\n",
      "(Iteration 12502 / 14700) loss: 0.882879\n",
      "(Iteration 12503 / 14700) loss: 0.802829\n",
      "(Iteration 12504 / 14700) loss: 0.906885\n",
      "(Iteration 12505 / 14700) loss: 0.596341\n",
      "(Iteration 12506 / 14700) loss: 0.741525\n",
      "(Iteration 12507 / 14700) loss: 0.791795\n",
      "(Iteration 12508 / 14700) loss: 0.744844\n",
      "(Iteration 12509 / 14700) loss: 0.560355\n",
      "(Iteration 12510 / 14700) loss: 0.613977\n",
      "(Iteration 12511 / 14700) loss: 0.796868\n",
      "(Iteration 12512 / 14700) loss: 0.804839\n",
      "(Iteration 12513 / 14700) loss: 0.653344\n",
      "(Iteration 12514 / 14700) loss: 0.925807\n",
      "(Iteration 12515 / 14700) loss: 0.628415\n",
      "(Iteration 12516 / 14700) loss: 0.592438\n",
      "(Iteration 12517 / 14700) loss: 0.698099\n",
      "(Iteration 12518 / 14700) loss: 0.732858\n",
      "(Iteration 12519 / 14700) loss: 0.891230\n",
      "(Iteration 12520 / 14700) loss: 0.724561\n",
      "(Iteration 12521 / 14700) loss: 0.758655\n",
      "(Iteration 12522 / 14700) loss: 0.727810\n",
      "(Iteration 12523 / 14700) loss: 0.872528\n",
      "(Iteration 12524 / 14700) loss: 0.619616\n",
      "(Iteration 12525 / 14700) loss: 0.684404\n",
      "(Iteration 12526 / 14700) loss: 0.731387\n",
      "(Iteration 12527 / 14700) loss: 0.635501\n",
      "(Iteration 12528 / 14700) loss: 0.765617\n",
      "(Iteration 12529 / 14700) loss: 0.688568\n",
      "(Iteration 12530 / 14700) loss: 0.685692\n",
      "(Iteration 12531 / 14700) loss: 0.772488\n",
      "(Iteration 12532 / 14700) loss: 0.720137\n",
      "(Iteration 12533 / 14700) loss: 0.681472\n",
      "(Iteration 12534 / 14700) loss: 0.598191\n",
      "(Iteration 12535 / 14700) loss: 0.528181\n",
      "(Iteration 12536 / 14700) loss: 0.766423\n",
      "(Iteration 12537 / 14700) loss: 0.757836\n",
      "(Iteration 12538 / 14700) loss: 0.585293\n",
      "(Iteration 12539 / 14700) loss: 0.723772\n",
      "(Iteration 12540 / 14700) loss: 0.839665\n",
      "(Iteration 12541 / 14700) loss: 0.785180\n",
      "(Iteration 12542 / 14700) loss: 0.387151\n",
      "(Iteration 12543 / 14700) loss: 0.647890\n",
      "(Iteration 12544 / 14700) loss: 0.620110\n",
      "(Iteration 12545 / 14700) loss: 0.579075\n",
      "(Iteration 12546 / 14700) loss: 0.850729\n",
      "(Iteration 12547 / 14700) loss: 0.611182\n",
      "(Iteration 12548 / 14700) loss: 0.907460\n",
      "(Iteration 12549 / 14700) loss: 0.804440\n",
      "(Iteration 12550 / 14700) loss: 0.882308\n",
      "(Iteration 12551 / 14700) loss: 0.601606\n",
      "(Iteration 12552 / 14700) loss: 0.761863\n",
      "(Iteration 12553 / 14700) loss: 0.912062\n",
      "(Iteration 12554 / 14700) loss: 0.953470\n",
      "(Iteration 12555 / 14700) loss: 0.756078\n",
      "(Iteration 12556 / 14700) loss: 0.617114\n",
      "(Iteration 12557 / 14700) loss: 0.716966\n",
      "(Iteration 12558 / 14700) loss: 0.947967\n",
      "(Iteration 12559 / 14700) loss: 0.750594\n",
      "(Iteration 12560 / 14700) loss: 0.707292\n",
      "(Iteration 12561 / 14700) loss: 0.780822\n",
      "(Iteration 12562 / 14700) loss: 0.748627\n",
      "(Iteration 12563 / 14700) loss: 0.653832\n",
      "(Iteration 12564 / 14700) loss: 0.776638\n",
      "(Iteration 12565 / 14700) loss: 0.501200\n",
      "(Iteration 12566 / 14700) loss: 0.985721\n",
      "(Iteration 12567 / 14700) loss: 0.635988\n",
      "(Iteration 12568 / 14700) loss: 0.761357\n",
      "(Iteration 12569 / 14700) loss: 0.687447\n",
      "(Iteration 12570 / 14700) loss: 0.717187\n",
      "(Iteration 12571 / 14700) loss: 0.556352\n",
      "(Iteration 12572 / 14700) loss: 0.564006\n",
      "(Iteration 12573 / 14700) loss: 0.567574\n",
      "(Iteration 12574 / 14700) loss: 0.487557\n",
      "(Iteration 12575 / 14700) loss: 0.967291\n",
      "(Iteration 12576 / 14700) loss: 0.526092\n",
      "(Iteration 12577 / 14700) loss: 0.691288\n",
      "(Iteration 12578 / 14700) loss: 0.804471\n",
      "(Iteration 12579 / 14700) loss: 0.919438\n",
      "(Iteration 12580 / 14700) loss: 0.746738\n",
      "(Iteration 12581 / 14700) loss: 0.550030\n",
      "(Iteration 12582 / 14700) loss: 0.660027\n",
      "(Iteration 12583 / 14700) loss: 0.628332\n",
      "(Iteration 12584 / 14700) loss: 0.612538\n",
      "(Iteration 12585 / 14700) loss: 0.677319\n",
      "(Iteration 12586 / 14700) loss: 0.697121\n",
      "(Iteration 12587 / 14700) loss: 1.062445\n",
      "(Iteration 12588 / 14700) loss: 0.848368\n",
      "(Iteration 12589 / 14700) loss: 0.671708\n",
      "(Iteration 12590 / 14700) loss: 0.731721\n",
      "(Iteration 12591 / 14700) loss: 0.553548\n",
      "(Iteration 12592 / 14700) loss: 0.657671\n",
      "(Iteration 12593 / 14700) loss: 0.771600\n",
      "(Iteration 12594 / 14700) loss: 0.805510\n",
      "(Iteration 12595 / 14700) loss: 0.574775\n",
      "(Iteration 12596 / 14700) loss: 0.779379\n",
      "(Iteration 12597 / 14700) loss: 0.649576\n",
      "(Iteration 12598 / 14700) loss: 0.805206\n",
      "(Iteration 12599 / 14700) loss: 0.756569\n",
      "(Iteration 12600 / 14700) loss: 0.735525\n",
      "(Iteration 12601 / 14700) loss: 0.504800\n",
      "(Iteration 12602 / 14700) loss: 0.833709\n",
      "(Iteration 12603 / 14700) loss: 0.627318\n",
      "(Iteration 12604 / 14700) loss: 0.618472\n",
      "(Iteration 12605 / 14700) loss: 0.665076\n",
      "(Iteration 12606 / 14700) loss: 0.504353\n",
      "(Iteration 12607 / 14700) loss: 0.662755\n",
      "(Iteration 12608 / 14700) loss: 0.642195\n",
      "(Iteration 12609 / 14700) loss: 0.897704\n",
      "(Iteration 12610 / 14700) loss: 0.861351\n",
      "(Iteration 12611 / 14700) loss: 0.536416\n",
      "(Iteration 12612 / 14700) loss: 0.670777\n",
      "(Iteration 12613 / 14700) loss: 0.729845\n",
      "(Iteration 12614 / 14700) loss: 0.963427\n",
      "(Iteration 12615 / 14700) loss: 0.734749\n",
      "(Iteration 12616 / 14700) loss: 0.666874\n",
      "(Iteration 12617 / 14700) loss: 0.687507\n",
      "(Iteration 12618 / 14700) loss: 1.002637\n",
      "(Iteration 12619 / 14700) loss: 0.512874\n",
      "(Iteration 12620 / 14700) loss: 0.739426\n",
      "(Iteration 12621 / 14700) loss: 0.558561\n",
      "(Iteration 12622 / 14700) loss: 0.490539\n",
      "(Iteration 12623 / 14700) loss: 0.823576\n",
      "(Iteration 12624 / 14700) loss: 0.705835\n",
      "(Iteration 12625 / 14700) loss: 0.772933\n",
      "(Iteration 12626 / 14700) loss: 0.644555\n",
      "(Iteration 12627 / 14700) loss: 0.633797\n",
      "(Iteration 12628 / 14700) loss: 0.617100\n",
      "(Iteration 12629 / 14700) loss: 0.620621\n",
      "(Iteration 12630 / 14700) loss: 0.890179\n",
      "(Iteration 12631 / 14700) loss: 0.922271\n",
      "(Iteration 12632 / 14700) loss: 0.740410\n",
      "(Iteration 12633 / 14700) loss: 0.730042\n",
      "(Iteration 12634 / 14700) loss: 0.638837\n",
      "(Iteration 12635 / 14700) loss: 0.512349\n",
      "(Iteration 12636 / 14700) loss: 0.752941\n",
      "(Iteration 12637 / 14700) loss: 0.577043\n",
      "(Iteration 12638 / 14700) loss: 0.642580\n",
      "(Iteration 12639 / 14700) loss: 0.480771\n",
      "(Iteration 12640 / 14700) loss: 0.609537\n",
      "(Iteration 12641 / 14700) loss: 0.771537\n",
      "(Iteration 12642 / 14700) loss: 0.758032\n",
      "(Iteration 12643 / 14700) loss: 0.876255\n",
      "(Iteration 12644 / 14700) loss: 0.605618\n",
      "(Iteration 12645 / 14700) loss: 0.613695\n",
      "(Iteration 12646 / 14700) loss: 0.514476\n",
      "(Iteration 12647 / 14700) loss: 0.640913\n",
      "(Iteration 12648 / 14700) loss: 0.700172\n",
      "(Iteration 12649 / 14700) loss: 0.632905\n",
      "(Iteration 12650 / 14700) loss: 0.773490\n",
      "(Iteration 12651 / 14700) loss: 0.755793\n",
      "(Iteration 12652 / 14700) loss: 0.711741\n",
      "(Iteration 12653 / 14700) loss: 0.716054\n",
      "(Iteration 12654 / 14700) loss: 0.722467\n",
      "(Iteration 12655 / 14700) loss: 0.869020\n",
      "(Iteration 12656 / 14700) loss: 0.920406\n",
      "(Iteration 12657 / 14700) loss: 0.660034\n",
      "(Iteration 12658 / 14700) loss: 0.602917\n",
      "(Iteration 12659 / 14700) loss: 0.636745\n",
      "(Iteration 12660 / 14700) loss: 0.863564\n",
      "(Iteration 12661 / 14700) loss: 0.813253\n",
      "(Iteration 12662 / 14700) loss: 0.905109\n",
      "(Iteration 12663 / 14700) loss: 0.683732\n",
      "(Iteration 12664 / 14700) loss: 0.847422\n",
      "(Iteration 12665 / 14700) loss: 0.621023\n",
      "(Iteration 12666 / 14700) loss: 0.820236\n",
      "(Iteration 12667 / 14700) loss: 0.736166\n",
      "(Iteration 12668 / 14700) loss: 0.709184\n",
      "(Iteration 12669 / 14700) loss: 0.774205\n",
      "(Iteration 12670 / 14700) loss: 0.884987\n",
      "(Iteration 12671 / 14700) loss: 0.813323\n",
      "(Iteration 12672 / 14700) loss: 1.013020\n",
      "(Iteration 12673 / 14700) loss: 0.732329\n",
      "(Iteration 12674 / 14700) loss: 0.595406\n",
      "(Iteration 12675 / 14700) loss: 0.713380\n",
      "(Iteration 12676 / 14700) loss: 0.682378\n",
      "(Iteration 12677 / 14700) loss: 0.743312\n",
      "(Iteration 12678 / 14700) loss: 0.672627\n",
      "(Iteration 12679 / 14700) loss: 0.785240\n",
      "(Iteration 12680 / 14700) loss: 0.848037\n",
      "(Iteration 12681 / 14700) loss: 0.645993\n",
      "(Iteration 12682 / 14700) loss: 0.970247\n",
      "(Iteration 12683 / 14700) loss: 0.799362\n",
      "(Iteration 12684 / 14700) loss: 0.732677\n",
      "(Iteration 12685 / 14700) loss: 0.885464\n",
      "(Iteration 12686 / 14700) loss: 0.528581\n",
      "(Iteration 12687 / 14700) loss: 0.905412\n",
      "(Iteration 12688 / 14700) loss: 0.706807\n",
      "(Iteration 12689 / 14700) loss: 0.569751\n",
      "(Iteration 12690 / 14700) loss: 0.607990\n",
      "(Iteration 12691 / 14700) loss: 0.635247\n",
      "(Iteration 12692 / 14700) loss: 0.602945\n",
      "(Iteration 12693 / 14700) loss: 0.570159\n",
      "(Iteration 12694 / 14700) loss: 0.685064\n",
      "(Iteration 12695 / 14700) loss: 0.867958\n",
      "(Iteration 12696 / 14700) loss: 0.866399\n",
      "(Iteration 12697 / 14700) loss: 0.768073\n",
      "(Iteration 12698 / 14700) loss: 0.996800\n",
      "(Iteration 12699 / 14700) loss: 1.044465\n",
      "(Iteration 12700 / 14700) loss: 0.866064\n",
      "(Iteration 12701 / 14700) loss: 0.874649\n",
      "(Iteration 12702 / 14700) loss: 0.624638\n",
      "(Iteration 12703 / 14700) loss: 0.924173\n",
      "(Iteration 12704 / 14700) loss: 0.796763\n",
      "(Iteration 12705 / 14700) loss: 0.794567\n",
      "(Iteration 12706 / 14700) loss: 0.791114\n",
      "(Iteration 12707 / 14700) loss: 0.717236\n",
      "(Iteration 12708 / 14700) loss: 0.706061\n",
      "(Iteration 12709 / 14700) loss: 0.762252\n",
      "(Iteration 12710 / 14700) loss: 0.785218\n",
      "(Iteration 12711 / 14700) loss: 0.830819\n",
      "(Iteration 12712 / 14700) loss: 0.720783\n",
      "(Iteration 12713 / 14700) loss: 0.704987\n",
      "(Iteration 12714 / 14700) loss: 0.806146\n",
      "(Iteration 12715 / 14700) loss: 0.511534\n",
      "(Iteration 12716 / 14700) loss: 0.749991\n",
      "(Iteration 12717 / 14700) loss: 0.767202\n",
      "(Iteration 12718 / 14700) loss: 0.579587\n",
      "(Iteration 12719 / 14700) loss: 0.738264\n",
      "(Iteration 12720 / 14700) loss: 0.584929\n",
      "(Iteration 12721 / 14700) loss: 0.771283\n",
      "(Iteration 12722 / 14700) loss: 0.572546\n",
      "(Iteration 12723 / 14700) loss: 0.744699\n",
      "(Iteration 12724 / 14700) loss: 0.692161\n",
      "(Iteration 12725 / 14700) loss: 0.699583\n",
      "(Iteration 12726 / 14700) loss: 0.589197\n",
      "(Iteration 12727 / 14700) loss: 0.547505\n",
      "(Iteration 12728 / 14700) loss: 0.689145\n",
      "(Iteration 12729 / 14700) loss: 0.813085\n",
      "(Iteration 12730 / 14700) loss: 0.600996\n",
      "(Iteration 12731 / 14700) loss: 0.676510\n",
      "(Iteration 12732 / 14700) loss: 0.626009\n",
      "(Iteration 12733 / 14700) loss: 0.632649\n",
      "(Iteration 12734 / 14700) loss: 0.832424\n",
      "(Iteration 12735 / 14700) loss: 0.808332\n",
      "(Iteration 12736 / 14700) loss: 0.730790\n",
      "(Iteration 12737 / 14700) loss: 0.964920\n",
      "(Iteration 12738 / 14700) loss: 0.687708\n",
      "(Iteration 12739 / 14700) loss: 0.858313\n",
      "(Iteration 12740 / 14700) loss: 0.742013\n",
      "(Epoch 13 / 15) train acc: 0.817000; val_acc: 0.769000\n",
      "(Iteration 12741 / 14700) loss: 0.559720\n",
      "(Iteration 12742 / 14700) loss: 0.576225\n",
      "(Iteration 12743 / 14700) loss: 0.559383\n",
      "(Iteration 12744 / 14700) loss: 0.792652\n",
      "(Iteration 12745 / 14700) loss: 0.786165\n",
      "(Iteration 12746 / 14700) loss: 0.676494\n",
      "(Iteration 12747 / 14700) loss: 0.883172\n",
      "(Iteration 12748 / 14700) loss: 0.751756\n",
      "(Iteration 12749 / 14700) loss: 0.935312\n",
      "(Iteration 12750 / 14700) loss: 0.714063\n",
      "(Iteration 12751 / 14700) loss: 0.848604\n",
      "(Iteration 12752 / 14700) loss: 0.684489\n",
      "(Iteration 12753 / 14700) loss: 0.836784\n",
      "(Iteration 12754 / 14700) loss: 0.826970\n",
      "(Iteration 12755 / 14700) loss: 0.615481\n",
      "(Iteration 12756 / 14700) loss: 0.652554\n",
      "(Iteration 12757 / 14700) loss: 0.562415\n",
      "(Iteration 12758 / 14700) loss: 0.790025\n",
      "(Iteration 12759 / 14700) loss: 1.147690\n",
      "(Iteration 12760 / 14700) loss: 0.730903\n",
      "(Iteration 12761 / 14700) loss: 0.772082\n",
      "(Iteration 12762 / 14700) loss: 0.640989\n",
      "(Iteration 12763 / 14700) loss: 0.921524\n",
      "(Iteration 12764 / 14700) loss: 0.760190\n",
      "(Iteration 12765 / 14700) loss: 0.779392\n",
      "(Iteration 12766 / 14700) loss: 0.548348\n",
      "(Iteration 12767 / 14700) loss: 0.785581\n",
      "(Iteration 12768 / 14700) loss: 0.809445\n",
      "(Iteration 12769 / 14700) loss: 0.643779\n",
      "(Iteration 12770 / 14700) loss: 0.585467\n",
      "(Iteration 12771 / 14700) loss: 0.505367\n",
      "(Iteration 12772 / 14700) loss: 0.905037\n",
      "(Iteration 12773 / 14700) loss: 0.828022\n",
      "(Iteration 12774 / 14700) loss: 0.711015\n",
      "(Iteration 12775 / 14700) loss: 0.733275\n",
      "(Iteration 12776 / 14700) loss: 0.656663\n",
      "(Iteration 12777 / 14700) loss: 0.734627\n",
      "(Iteration 12778 / 14700) loss: 0.587533\n",
      "(Iteration 12779 / 14700) loss: 0.666828\n",
      "(Iteration 12780 / 14700) loss: 0.710227\n",
      "(Iteration 12781 / 14700) loss: 0.729035\n",
      "(Iteration 12782 / 14700) loss: 0.671114\n",
      "(Iteration 12783 / 14700) loss: 0.609059\n",
      "(Iteration 12784 / 14700) loss: 0.758894\n",
      "(Iteration 12785 / 14700) loss: 0.693879\n",
      "(Iteration 12786 / 14700) loss: 0.781125\n",
      "(Iteration 12787 / 14700) loss: 0.683612\n",
      "(Iteration 12788 / 14700) loss: 0.577071\n",
      "(Iteration 12789 / 14700) loss: 0.529680\n",
      "(Iteration 12790 / 14700) loss: 0.821075\n",
      "(Iteration 12791 / 14700) loss: 0.625193\n",
      "(Iteration 12792 / 14700) loss: 0.646730\n",
      "(Iteration 12793 / 14700) loss: 0.580827\n",
      "(Iteration 12794 / 14700) loss: 0.508371\n",
      "(Iteration 12795 / 14700) loss: 0.749292\n",
      "(Iteration 12796 / 14700) loss: 0.704056\n",
      "(Iteration 12797 / 14700) loss: 0.891716\n",
      "(Iteration 12798 / 14700) loss: 0.672449\n",
      "(Iteration 12799 / 14700) loss: 0.660014\n",
      "(Iteration 12800 / 14700) loss: 0.787747\n",
      "(Iteration 12801 / 14700) loss: 0.585357\n",
      "(Iteration 12802 / 14700) loss: 0.950195\n",
      "(Iteration 12803 / 14700) loss: 0.908998\n",
      "(Iteration 12804 / 14700) loss: 0.676003\n",
      "(Iteration 12805 / 14700) loss: 0.680742\n",
      "(Iteration 12806 / 14700) loss: 0.683277\n",
      "(Iteration 12807 / 14700) loss: 0.589889\n",
      "(Iteration 12808 / 14700) loss: 0.731141\n",
      "(Iteration 12809 / 14700) loss: 0.578262\n",
      "(Iteration 12810 / 14700) loss: 0.711729\n",
      "(Iteration 12811 / 14700) loss: 0.647101\n",
      "(Iteration 12812 / 14700) loss: 0.586339\n",
      "(Iteration 12813 / 14700) loss: 0.714443\n",
      "(Iteration 12814 / 14700) loss: 0.695876\n",
      "(Iteration 12815 / 14700) loss: 0.668191\n",
      "(Iteration 12816 / 14700) loss: 0.549215\n",
      "(Iteration 12817 / 14700) loss: 0.716019\n",
      "(Iteration 12818 / 14700) loss: 0.598200\n",
      "(Iteration 12819 / 14700) loss: 0.742938\n",
      "(Iteration 12820 / 14700) loss: 0.626947\n",
      "(Iteration 12821 / 14700) loss: 0.797852\n",
      "(Iteration 12822 / 14700) loss: 0.926199\n",
      "(Iteration 12823 / 14700) loss: 0.941791\n",
      "(Iteration 12824 / 14700) loss: 0.685083\n",
      "(Iteration 12825 / 14700) loss: 0.835515\n",
      "(Iteration 12826 / 14700) loss: 0.543458\n",
      "(Iteration 12827 / 14700) loss: 0.900112\n",
      "(Iteration 12828 / 14700) loss: 0.871236\n",
      "(Iteration 12829 / 14700) loss: 0.808692\n",
      "(Iteration 12830 / 14700) loss: 0.697179\n",
      "(Iteration 12831 / 14700) loss: 0.689967\n",
      "(Iteration 12832 / 14700) loss: 0.584869\n",
      "(Iteration 12833 / 14700) loss: 0.854447\n",
      "(Iteration 12834 / 14700) loss: 0.796131\n",
      "(Iteration 12835 / 14700) loss: 0.436383\n",
      "(Iteration 12836 / 14700) loss: 0.649082\n",
      "(Iteration 12837 / 14700) loss: 0.507733\n",
      "(Iteration 12838 / 14700) loss: 0.748915\n",
      "(Iteration 12839 / 14700) loss: 0.723206\n",
      "(Iteration 12840 / 14700) loss: 0.727778\n",
      "(Iteration 12841 / 14700) loss: 0.912851\n",
      "(Iteration 12842 / 14700) loss: 1.084278\n",
      "(Iteration 12843 / 14700) loss: 0.605123\n",
      "(Iteration 12844 / 14700) loss: 0.778790\n",
      "(Iteration 12845 / 14700) loss: 0.562179\n",
      "(Iteration 12846 / 14700) loss: 1.010759\n",
      "(Iteration 12847 / 14700) loss: 0.705258\n",
      "(Iteration 12848 / 14700) loss: 0.562191\n",
      "(Iteration 12849 / 14700) loss: 0.544362\n",
      "(Iteration 12850 / 14700) loss: 0.732351\n",
      "(Iteration 12851 / 14700) loss: 0.710392\n",
      "(Iteration 12852 / 14700) loss: 0.804836\n",
      "(Iteration 12853 / 14700) loss: 0.616541\n",
      "(Iteration 12854 / 14700) loss: 0.993766\n",
      "(Iteration 12855 / 14700) loss: 0.839479\n",
      "(Iteration 12856 / 14700) loss: 0.771441\n",
      "(Iteration 12857 / 14700) loss: 1.047670\n",
      "(Iteration 12858 / 14700) loss: 0.563770\n",
      "(Iteration 12859 / 14700) loss: 0.670587\n",
      "(Iteration 12860 / 14700) loss: 0.561142\n",
      "(Iteration 12861 / 14700) loss: 0.790130\n",
      "(Iteration 12862 / 14700) loss: 0.758847\n",
      "(Iteration 12863 / 14700) loss: 0.769809\n",
      "(Iteration 12864 / 14700) loss: 0.546118\n",
      "(Iteration 12865 / 14700) loss: 0.489878\n",
      "(Iteration 12866 / 14700) loss: 0.762788\n",
      "(Iteration 12867 / 14700) loss: 0.759293\n",
      "(Iteration 12868 / 14700) loss: 0.808314\n",
      "(Iteration 12869 / 14700) loss: 0.575493\n",
      "(Iteration 12870 / 14700) loss: 0.786231\n",
      "(Iteration 12871 / 14700) loss: 1.110589\n",
      "(Iteration 12872 / 14700) loss: 0.808057\n",
      "(Iteration 12873 / 14700) loss: 0.699938\n",
      "(Iteration 12874 / 14700) loss: 0.653088\n",
      "(Iteration 12875 / 14700) loss: 0.602977\n",
      "(Iteration 12876 / 14700) loss: 0.802158\n",
      "(Iteration 12877 / 14700) loss: 0.661812\n",
      "(Iteration 12878 / 14700) loss: 0.705805\n",
      "(Iteration 12879 / 14700) loss: 0.550327\n",
      "(Iteration 12880 / 14700) loss: 0.874256\n",
      "(Iteration 12881 / 14700) loss: 0.720793\n",
      "(Iteration 12882 / 14700) loss: 0.616737\n",
      "(Iteration 12883 / 14700) loss: 0.621843\n",
      "(Iteration 12884 / 14700) loss: 0.751826\n",
      "(Iteration 12885 / 14700) loss: 0.606483\n",
      "(Iteration 12886 / 14700) loss: 0.674732\n",
      "(Iteration 12887 / 14700) loss: 0.601030\n",
      "(Iteration 12888 / 14700) loss: 0.681083\n",
      "(Iteration 12889 / 14700) loss: 0.808931\n",
      "(Iteration 12890 / 14700) loss: 0.604467\n",
      "(Iteration 12891 / 14700) loss: 0.570389\n",
      "(Iteration 12892 / 14700) loss: 0.786569\n",
      "(Iteration 12893 / 14700) loss: 0.828832\n",
      "(Iteration 12894 / 14700) loss: 0.721172\n",
      "(Iteration 12895 / 14700) loss: 0.480802\n",
      "(Iteration 12896 / 14700) loss: 1.003759\n",
      "(Iteration 12897 / 14700) loss: 0.655053\n",
      "(Iteration 12898 / 14700) loss: 0.900376\n",
      "(Iteration 12899 / 14700) loss: 1.020776\n",
      "(Iteration 12900 / 14700) loss: 0.838059\n",
      "(Iteration 12901 / 14700) loss: 0.597370\n",
      "(Iteration 12902 / 14700) loss: 0.592676\n",
      "(Iteration 12903 / 14700) loss: 0.640964\n",
      "(Iteration 12904 / 14700) loss: 0.628782\n",
      "(Iteration 12905 / 14700) loss: 0.659943\n",
      "(Iteration 12906 / 14700) loss: 0.550795\n",
      "(Iteration 12907 / 14700) loss: 0.547682\n",
      "(Iteration 12908 / 14700) loss: 0.629836\n",
      "(Iteration 12909 / 14700) loss: 0.654168\n",
      "(Iteration 12910 / 14700) loss: 0.893066\n",
      "(Iteration 12911 / 14700) loss: 0.702270\n",
      "(Iteration 12912 / 14700) loss: 0.603405\n",
      "(Iteration 12913 / 14700) loss: 0.680131\n",
      "(Iteration 12914 / 14700) loss: 0.696025\n",
      "(Iteration 12915 / 14700) loss: 0.693451\n",
      "(Iteration 12916 / 14700) loss: 0.799369\n",
      "(Iteration 12917 / 14700) loss: 0.648671\n",
      "(Iteration 12918 / 14700) loss: 0.731978\n",
      "(Iteration 12919 / 14700) loss: 0.822924\n",
      "(Iteration 12920 / 14700) loss: 0.582543\n",
      "(Iteration 12921 / 14700) loss: 0.758586\n",
      "(Iteration 12922 / 14700) loss: 0.810330\n",
      "(Iteration 12923 / 14700) loss: 0.616195\n",
      "(Iteration 12924 / 14700) loss: 0.701434\n",
      "(Iteration 12925 / 14700) loss: 0.680462\n",
      "(Iteration 12926 / 14700) loss: 0.564989\n",
      "(Iteration 12927 / 14700) loss: 0.610349\n",
      "(Iteration 12928 / 14700) loss: 0.690263\n",
      "(Iteration 12929 / 14700) loss: 0.432854\n",
      "(Iteration 12930 / 14700) loss: 0.761747\n",
      "(Iteration 12931 / 14700) loss: 0.901196\n",
      "(Iteration 12932 / 14700) loss: 0.824223\n",
      "(Iteration 12933 / 14700) loss: 0.676320\n",
      "(Iteration 12934 / 14700) loss: 0.849857\n",
      "(Iteration 12935 / 14700) loss: 0.865421\n",
      "(Iteration 12936 / 14700) loss: 0.697031\n",
      "(Iteration 12937 / 14700) loss: 0.716759\n",
      "(Iteration 12938 / 14700) loss: 0.601879\n",
      "(Iteration 12939 / 14700) loss: 0.600183\n",
      "(Iteration 12940 / 14700) loss: 0.516945\n",
      "(Iteration 12941 / 14700) loss: 0.492918\n",
      "(Iteration 12942 / 14700) loss: 0.711304\n",
      "(Iteration 12943 / 14700) loss: 0.674405\n",
      "(Iteration 12944 / 14700) loss: 0.699515\n",
      "(Iteration 12945 / 14700) loss: 0.684390\n",
      "(Iteration 12946 / 14700) loss: 0.645874\n",
      "(Iteration 12947 / 14700) loss: 0.900403\n",
      "(Iteration 12948 / 14700) loss: 0.864354\n",
      "(Iteration 12949 / 14700) loss: 0.662825\n",
      "(Iteration 12950 / 14700) loss: 0.788575\n",
      "(Iteration 12951 / 14700) loss: 0.548095\n",
      "(Iteration 12952 / 14700) loss: 0.701983\n",
      "(Iteration 12953 / 14700) loss: 0.763793\n",
      "(Iteration 12954 / 14700) loss: 0.912481\n",
      "(Iteration 12955 / 14700) loss: 0.630653\n",
      "(Iteration 12956 / 14700) loss: 0.947362\n",
      "(Iteration 12957 / 14700) loss: 0.754901\n",
      "(Iteration 12958 / 14700) loss: 0.624205\n",
      "(Iteration 12959 / 14700) loss: 0.925175\n",
      "(Iteration 12960 / 14700) loss: 0.686525\n",
      "(Iteration 12961 / 14700) loss: 0.572721\n",
      "(Iteration 12962 / 14700) loss: 0.752564\n",
      "(Iteration 12963 / 14700) loss: 0.677456\n",
      "(Iteration 12964 / 14700) loss: 0.883727\n",
      "(Iteration 12965 / 14700) loss: 0.614769\n",
      "(Iteration 12966 / 14700) loss: 0.675672\n",
      "(Iteration 12967 / 14700) loss: 0.534778\n",
      "(Iteration 12968 / 14700) loss: 0.708640\n",
      "(Iteration 12969 / 14700) loss: 0.689275\n",
      "(Iteration 12970 / 14700) loss: 0.680539\n",
      "(Iteration 12971 / 14700) loss: 0.477936\n",
      "(Iteration 12972 / 14700) loss: 0.498681\n",
      "(Iteration 12973 / 14700) loss: 0.638352\n",
      "(Iteration 12974 / 14700) loss: 0.715273\n",
      "(Iteration 12975 / 14700) loss: 0.904031\n",
      "(Iteration 12976 / 14700) loss: 0.677421\n",
      "(Iteration 12977 / 14700) loss: 0.748608\n",
      "(Iteration 12978 / 14700) loss: 0.635861\n",
      "(Iteration 12979 / 14700) loss: 0.656132\n",
      "(Iteration 12980 / 14700) loss: 0.792894\n",
      "(Iteration 12981 / 14700) loss: 0.718381\n",
      "(Iteration 12982 / 14700) loss: 0.600831\n",
      "(Iteration 12983 / 14700) loss: 0.720679\n",
      "(Iteration 12984 / 14700) loss: 0.662762\n",
      "(Iteration 12985 / 14700) loss: 0.967375\n",
      "(Iteration 12986 / 14700) loss: 0.837053\n",
      "(Iteration 12987 / 14700) loss: 0.736931\n",
      "(Iteration 12988 / 14700) loss: 0.924679\n",
      "(Iteration 12989 / 14700) loss: 0.658243\n",
      "(Iteration 12990 / 14700) loss: 0.704487\n",
      "(Iteration 12991 / 14700) loss: 0.599584\n",
      "(Iteration 12992 / 14700) loss: 0.661725\n",
      "(Iteration 12993 / 14700) loss: 0.741716\n",
      "(Iteration 12994 / 14700) loss: 0.710700\n",
      "(Iteration 12995 / 14700) loss: 0.811111\n",
      "(Iteration 12996 / 14700) loss: 0.876901\n",
      "(Iteration 12997 / 14700) loss: 0.688272\n",
      "(Iteration 12998 / 14700) loss: 0.972457\n",
      "(Iteration 12999 / 14700) loss: 0.716977\n",
      "(Iteration 13000 / 14700) loss: 0.813880\n",
      "(Iteration 13001 / 14700) loss: 0.659039\n",
      "(Iteration 13002 / 14700) loss: 0.612315\n",
      "(Iteration 13003 / 14700) loss: 0.824655\n",
      "(Iteration 13004 / 14700) loss: 0.894702\n",
      "(Iteration 13005 / 14700) loss: 0.778350\n",
      "(Iteration 13006 / 14700) loss: 0.850118\n",
      "(Iteration 13007 / 14700) loss: 0.683910\n",
      "(Iteration 13008 / 14700) loss: 0.787878\n",
      "(Iteration 13009 / 14700) loss: 0.868824\n",
      "(Iteration 13010 / 14700) loss: 0.825599\n",
      "(Iteration 13011 / 14700) loss: 0.523670\n",
      "(Iteration 13012 / 14700) loss: 0.818642\n",
      "(Iteration 13013 / 14700) loss: 0.723215\n",
      "(Iteration 13014 / 14700) loss: 0.590765\n",
      "(Iteration 13015 / 14700) loss: 0.842057\n",
      "(Iteration 13016 / 14700) loss: 0.756867\n",
      "(Iteration 13017 / 14700) loss: 0.839596\n",
      "(Iteration 13018 / 14700) loss: 0.697917\n",
      "(Iteration 13019 / 14700) loss: 0.762528\n",
      "(Iteration 13020 / 14700) loss: 0.683235\n",
      "(Iteration 13021 / 14700) loss: 0.677285\n",
      "(Iteration 13022 / 14700) loss: 0.760413\n",
      "(Iteration 13023 / 14700) loss: 0.531382\n",
      "(Iteration 13024 / 14700) loss: 0.634367\n",
      "(Iteration 13025 / 14700) loss: 0.670638\n",
      "(Iteration 13026 / 14700) loss: 0.668843\n",
      "(Iteration 13027 / 14700) loss: 0.739619\n",
      "(Iteration 13028 / 14700) loss: 0.698495\n",
      "(Iteration 13029 / 14700) loss: 0.872765\n",
      "(Iteration 13030 / 14700) loss: 0.757187\n",
      "(Iteration 13031 / 14700) loss: 0.935287\n",
      "(Iteration 13032 / 14700) loss: 0.968233\n",
      "(Iteration 13033 / 14700) loss: 0.607037\n",
      "(Iteration 13034 / 14700) loss: 0.654725\n",
      "(Iteration 13035 / 14700) loss: 0.788726\n",
      "(Iteration 13036 / 14700) loss: 0.682646\n",
      "(Iteration 13037 / 14700) loss: 0.587101\n",
      "(Iteration 13038 / 14700) loss: 0.862518\n",
      "(Iteration 13039 / 14700) loss: 0.694384\n",
      "(Iteration 13040 / 14700) loss: 0.576652\n",
      "(Iteration 13041 / 14700) loss: 0.929834\n",
      "(Iteration 13042 / 14700) loss: 0.660009\n",
      "(Iteration 13043 / 14700) loss: 0.811744\n",
      "(Iteration 13044 / 14700) loss: 0.748152\n",
      "(Iteration 13045 / 14700) loss: 0.903748\n",
      "(Iteration 13046 / 14700) loss: 0.746343\n",
      "(Iteration 13047 / 14700) loss: 0.827234\n",
      "(Iteration 13048 / 14700) loss: 0.633971\n",
      "(Iteration 13049 / 14700) loss: 0.663879\n",
      "(Iteration 13050 / 14700) loss: 0.632939\n",
      "(Iteration 13051 / 14700) loss: 0.920354\n",
      "(Iteration 13052 / 14700) loss: 0.839968\n",
      "(Iteration 13053 / 14700) loss: 0.688038\n",
      "(Iteration 13054 / 14700) loss: 0.997325\n",
      "(Iteration 13055 / 14700) loss: 0.729378\n",
      "(Iteration 13056 / 14700) loss: 0.734266\n",
      "(Iteration 13057 / 14700) loss: 0.705380\n",
      "(Iteration 13058 / 14700) loss: 0.777542\n",
      "(Iteration 13059 / 14700) loss: 0.658523\n",
      "(Iteration 13060 / 14700) loss: 0.589706\n",
      "(Iteration 13061 / 14700) loss: 0.830704\n",
      "(Iteration 13062 / 14700) loss: 0.728578\n",
      "(Iteration 13063 / 14700) loss: 0.753818\n",
      "(Iteration 13064 / 14700) loss: 0.845092\n",
      "(Iteration 13065 / 14700) loss: 0.896542\n",
      "(Iteration 13066 / 14700) loss: 0.765040\n",
      "(Iteration 13067 / 14700) loss: 0.512934\n",
      "(Iteration 13068 / 14700) loss: 0.788468\n",
      "(Iteration 13069 / 14700) loss: 0.580053\n",
      "(Iteration 13070 / 14700) loss: 0.731232\n",
      "(Iteration 13071 / 14700) loss: 0.684607\n",
      "(Iteration 13072 / 14700) loss: 0.611133\n",
      "(Iteration 13073 / 14700) loss: 0.720635\n",
      "(Iteration 13074 / 14700) loss: 0.695787\n",
      "(Iteration 13075 / 14700) loss: 0.799828\n",
      "(Iteration 13076 / 14700) loss: 0.888506\n",
      "(Iteration 13077 / 14700) loss: 0.925026\n",
      "(Iteration 13078 / 14700) loss: 0.783827\n",
      "(Iteration 13079 / 14700) loss: 1.023173\n",
      "(Iteration 13080 / 14700) loss: 0.585772\n",
      "(Iteration 13081 / 14700) loss: 0.838359\n",
      "(Iteration 13082 / 14700) loss: 0.590621\n",
      "(Iteration 13083 / 14700) loss: 0.754788\n",
      "(Iteration 13084 / 14700) loss: 0.685371\n",
      "(Iteration 13085 / 14700) loss: 0.595564\n",
      "(Iteration 13086 / 14700) loss: 0.818509\n",
      "(Iteration 13087 / 14700) loss: 0.630409\n",
      "(Iteration 13088 / 14700) loss: 0.714688\n",
      "(Iteration 13089 / 14700) loss: 0.718540\n",
      "(Iteration 13090 / 14700) loss: 0.768980\n",
      "(Iteration 13091 / 14700) loss: 0.600895\n",
      "(Iteration 13092 / 14700) loss: 0.584050\n",
      "(Iteration 13093 / 14700) loss: 0.781970\n",
      "(Iteration 13094 / 14700) loss: 0.446056\n",
      "(Iteration 13095 / 14700) loss: 0.815276\n",
      "(Iteration 13096 / 14700) loss: 0.585368\n",
      "(Iteration 13097 / 14700) loss: 0.858662\n",
      "(Iteration 13098 / 14700) loss: 0.630145\n",
      "(Iteration 13099 / 14700) loss: 0.795410\n",
      "(Iteration 13100 / 14700) loss: 0.582819\n",
      "(Iteration 13101 / 14700) loss: 0.548484\n",
      "(Iteration 13102 / 14700) loss: 0.734239\n",
      "(Iteration 13103 / 14700) loss: 0.757038\n",
      "(Iteration 13104 / 14700) loss: 0.651284\n",
      "(Iteration 13105 / 14700) loss: 0.559346\n",
      "(Iteration 13106 / 14700) loss: 0.753443\n",
      "(Iteration 13107 / 14700) loss: 0.493564\n",
      "(Iteration 13108 / 14700) loss: 0.620207\n",
      "(Iteration 13109 / 14700) loss: 0.723492\n",
      "(Iteration 13110 / 14700) loss: 0.757387\n",
      "(Iteration 13111 / 14700) loss: 0.618489\n",
      "(Iteration 13112 / 14700) loss: 0.779266\n",
      "(Iteration 13113 / 14700) loss: 0.731816\n",
      "(Iteration 13114 / 14700) loss: 0.477376\n",
      "(Iteration 13115 / 14700) loss: 0.578656\n",
      "(Iteration 13116 / 14700) loss: 0.622745\n",
      "(Iteration 13117 / 14700) loss: 0.656044\n",
      "(Iteration 13118 / 14700) loss: 0.968071\n",
      "(Iteration 13119 / 14700) loss: 0.694610\n",
      "(Iteration 13120 / 14700) loss: 0.451849\n",
      "(Iteration 13121 / 14700) loss: 0.702456\n",
      "(Iteration 13122 / 14700) loss: 0.545787\n",
      "(Iteration 13123 / 14700) loss: 0.855892\n",
      "(Iteration 13124 / 14700) loss: 0.498675\n",
      "(Iteration 13125 / 14700) loss: 0.506770\n",
      "(Iteration 13126 / 14700) loss: 0.608070\n",
      "(Iteration 13127 / 14700) loss: 0.500242\n",
      "(Iteration 13128 / 14700) loss: 0.570286\n",
      "(Iteration 13129 / 14700) loss: 0.466238\n",
      "(Iteration 13130 / 14700) loss: 0.727437\n",
      "(Iteration 13131 / 14700) loss: 0.673264\n",
      "(Iteration 13132 / 14700) loss: 0.550252\n",
      "(Iteration 13133 / 14700) loss: 0.666651\n",
      "(Iteration 13134 / 14700) loss: 0.693823\n",
      "(Iteration 13135 / 14700) loss: 0.756451\n",
      "(Iteration 13136 / 14700) loss: 0.661332\n",
      "(Iteration 13137 / 14700) loss: 0.620424\n",
      "(Iteration 13138 / 14700) loss: 0.648607\n",
      "(Iteration 13139 / 14700) loss: 0.585226\n",
      "(Iteration 13140 / 14700) loss: 0.700802\n",
      "(Iteration 13141 / 14700) loss: 0.617223\n",
      "(Iteration 13142 / 14700) loss: 0.718961\n",
      "(Iteration 13143 / 14700) loss: 0.606578\n",
      "(Iteration 13144 / 14700) loss: 0.598852\n",
      "(Iteration 13145 / 14700) loss: 0.630087\n",
      "(Iteration 13146 / 14700) loss: 0.684180\n",
      "(Iteration 13147 / 14700) loss: 0.746681\n",
      "(Iteration 13148 / 14700) loss: 0.913289\n",
      "(Iteration 13149 / 14700) loss: 0.638053\n",
      "(Iteration 13150 / 14700) loss: 0.569348\n",
      "(Iteration 13151 / 14700) loss: 0.554161\n",
      "(Iteration 13152 / 14700) loss: 0.771110\n",
      "(Iteration 13153 / 14700) loss: 0.588930\n",
      "(Iteration 13154 / 14700) loss: 0.719492\n",
      "(Iteration 13155 / 14700) loss: 0.607417\n",
      "(Iteration 13156 / 14700) loss: 0.610153\n",
      "(Iteration 13157 / 14700) loss: 0.857395\n",
      "(Iteration 13158 / 14700) loss: 0.580452\n",
      "(Iteration 13159 / 14700) loss: 0.631728\n",
      "(Iteration 13160 / 14700) loss: 0.797894\n",
      "(Iteration 13161 / 14700) loss: 0.707597\n",
      "(Iteration 13162 / 14700) loss: 0.668250\n",
      "(Iteration 13163 / 14700) loss: 0.896607\n",
      "(Iteration 13164 / 14700) loss: 0.706339\n",
      "(Iteration 13165 / 14700) loss: 0.637077\n",
      "(Iteration 13166 / 14700) loss: 0.809299\n",
      "(Iteration 13167 / 14700) loss: 0.859772\n",
      "(Iteration 13168 / 14700) loss: 0.650533\n",
      "(Iteration 13169 / 14700) loss: 0.696454\n",
      "(Iteration 13170 / 14700) loss: 0.535081\n",
      "(Iteration 13171 / 14700) loss: 0.881324\n",
      "(Iteration 13172 / 14700) loss: 0.808112\n",
      "(Iteration 13173 / 14700) loss: 0.716475\n",
      "(Iteration 13174 / 14700) loss: 0.581184\n",
      "(Iteration 13175 / 14700) loss: 0.727993\n",
      "(Iteration 13176 / 14700) loss: 0.721493\n",
      "(Iteration 13177 / 14700) loss: 0.594764\n",
      "(Iteration 13178 / 14700) loss: 0.746987\n",
      "(Iteration 13179 / 14700) loss: 0.544643\n",
      "(Iteration 13180 / 14700) loss: 0.578672\n",
      "(Iteration 13181 / 14700) loss: 0.681414\n",
      "(Iteration 13182 / 14700) loss: 0.573644\n",
      "(Iteration 13183 / 14700) loss: 0.892870\n",
      "(Iteration 13184 / 14700) loss: 0.826574\n",
      "(Iteration 13185 / 14700) loss: 0.796175\n",
      "(Iteration 13186 / 14700) loss: 0.557427\n",
      "(Iteration 13187 / 14700) loss: 0.604011\n",
      "(Iteration 13188 / 14700) loss: 0.464068\n",
      "(Iteration 13189 / 14700) loss: 0.609482\n",
      "(Iteration 13190 / 14700) loss: 0.607475\n",
      "(Iteration 13191 / 14700) loss: 0.775195\n",
      "(Iteration 13192 / 14700) loss: 0.642326\n",
      "(Iteration 13193 / 14700) loss: 0.900279\n",
      "(Iteration 13194 / 14700) loss: 0.747160\n",
      "(Iteration 13195 / 14700) loss: 0.676474\n",
      "(Iteration 13196 / 14700) loss: 0.749417\n",
      "(Iteration 13197 / 14700) loss: 0.625310\n",
      "(Iteration 13198 / 14700) loss: 0.798349\n",
      "(Iteration 13199 / 14700) loss: 0.445950\n",
      "(Iteration 13200 / 14700) loss: 0.762848\n",
      "(Iteration 13201 / 14700) loss: 0.660707\n",
      "(Iteration 13202 / 14700) loss: 0.602985\n",
      "(Iteration 13203 / 14700) loss: 0.601605\n",
      "(Iteration 13204 / 14700) loss: 0.800557\n",
      "(Iteration 13205 / 14700) loss: 0.673579\n",
      "(Iteration 13206 / 14700) loss: 0.770921\n",
      "(Iteration 13207 / 14700) loss: 0.671798\n",
      "(Iteration 13208 / 14700) loss: 0.686398\n",
      "(Iteration 13209 / 14700) loss: 0.711085\n",
      "(Iteration 13210 / 14700) loss: 0.585203\n",
      "(Iteration 13211 / 14700) loss: 0.748187\n",
      "(Iteration 13212 / 14700) loss: 0.849624\n",
      "(Iteration 13213 / 14700) loss: 0.622994\n",
      "(Iteration 13214 / 14700) loss: 0.766034\n",
      "(Iteration 13215 / 14700) loss: 0.706610\n",
      "(Iteration 13216 / 14700) loss: 0.629723\n",
      "(Iteration 13217 / 14700) loss: 0.905701\n",
      "(Iteration 13218 / 14700) loss: 0.528843\n",
      "(Iteration 13219 / 14700) loss: 0.784748\n",
      "(Iteration 13220 / 14700) loss: 0.772456\n",
      "(Iteration 13221 / 14700) loss: 0.586917\n",
      "(Iteration 13222 / 14700) loss: 0.531952\n",
      "(Iteration 13223 / 14700) loss: 0.564251\n",
      "(Iteration 13224 / 14700) loss: 1.024888\n",
      "(Iteration 13225 / 14700) loss: 0.510504\n",
      "(Iteration 13226 / 14700) loss: 0.775928\n",
      "(Iteration 13227 / 14700) loss: 0.732022\n",
      "(Iteration 13228 / 14700) loss: 0.693776\n",
      "(Iteration 13229 / 14700) loss: 0.675440\n",
      "(Iteration 13230 / 14700) loss: 0.681538\n",
      "(Iteration 13231 / 14700) loss: 0.612652\n",
      "(Iteration 13232 / 14700) loss: 0.584600\n",
      "(Iteration 13233 / 14700) loss: 0.825259\n",
      "(Iteration 13234 / 14700) loss: 0.894321\n",
      "(Iteration 13235 / 14700) loss: 0.734852\n",
      "(Iteration 13236 / 14700) loss: 0.765479\n",
      "(Iteration 13237 / 14700) loss: 0.630490\n",
      "(Iteration 13238 / 14700) loss: 0.551970\n",
      "(Iteration 13239 / 14700) loss: 0.664396\n",
      "(Iteration 13240 / 14700) loss: 0.542453\n",
      "(Iteration 13241 / 14700) loss: 0.477473\n",
      "(Iteration 13242 / 14700) loss: 0.855953\n",
      "(Iteration 13243 / 14700) loss: 0.580018\n",
      "(Iteration 13244 / 14700) loss: 0.620028\n",
      "(Iteration 13245 / 14700) loss: 0.785165\n",
      "(Iteration 13246 / 14700) loss: 0.752494\n",
      "(Iteration 13247 / 14700) loss: 0.745326\n",
      "(Iteration 13248 / 14700) loss: 0.582340\n",
      "(Iteration 13249 / 14700) loss: 0.727625\n",
      "(Iteration 13250 / 14700) loss: 0.718856\n",
      "(Iteration 13251 / 14700) loss: 0.786945\n",
      "(Iteration 13252 / 14700) loss: 0.834002\n",
      "(Iteration 13253 / 14700) loss: 0.695033\n",
      "(Iteration 13254 / 14700) loss: 1.046206\n",
      "(Iteration 13255 / 14700) loss: 0.859708\n",
      "(Iteration 13256 / 14700) loss: 0.671174\n",
      "(Iteration 13257 / 14700) loss: 0.602842\n",
      "(Iteration 13258 / 14700) loss: 0.736137\n",
      "(Iteration 13259 / 14700) loss: 0.600065\n",
      "(Iteration 13260 / 14700) loss: 0.763280\n",
      "(Iteration 13261 / 14700) loss: 0.711149\n",
      "(Iteration 13262 / 14700) loss: 0.568551\n",
      "(Iteration 13263 / 14700) loss: 0.624458\n",
      "(Iteration 13264 / 14700) loss: 0.699108\n",
      "(Iteration 13265 / 14700) loss: 0.645988\n",
      "(Iteration 13266 / 14700) loss: 0.655969\n",
      "(Iteration 13267 / 14700) loss: 0.657734\n",
      "(Iteration 13268 / 14700) loss: 0.605956\n",
      "(Iteration 13269 / 14700) loss: 0.569327\n",
      "(Iteration 13270 / 14700) loss: 0.661872\n",
      "(Iteration 13271 / 14700) loss: 0.731027\n",
      "(Iteration 13272 / 14700) loss: 0.634325\n",
      "(Iteration 13273 / 14700) loss: 0.729312\n",
      "(Iteration 13274 / 14700) loss: 0.720111\n",
      "(Iteration 13275 / 14700) loss: 0.610190\n",
      "(Iteration 13276 / 14700) loss: 0.763339\n",
      "(Iteration 13277 / 14700) loss: 0.483608\n",
      "(Iteration 13278 / 14700) loss: 0.640777\n",
      "(Iteration 13279 / 14700) loss: 0.541496\n",
      "(Iteration 13280 / 14700) loss: 0.702178\n",
      "(Iteration 13281 / 14700) loss: 0.501729\n",
      "(Iteration 13282 / 14700) loss: 0.701055\n",
      "(Iteration 13283 / 14700) loss: 0.673053\n",
      "(Iteration 13284 / 14700) loss: 0.657511\n",
      "(Iteration 13285 / 14700) loss: 0.864170\n",
      "(Iteration 13286 / 14700) loss: 0.507018\n",
      "(Iteration 13287 / 14700) loss: 0.553759\n",
      "(Iteration 13288 / 14700) loss: 0.799150\n",
      "(Iteration 13289 / 14700) loss: 0.796167\n",
      "(Iteration 13290 / 14700) loss: 0.681727\n",
      "(Iteration 13291 / 14700) loss: 0.578769\n",
      "(Iteration 13292 / 14700) loss: 0.862825\n",
      "(Iteration 13293 / 14700) loss: 0.710671\n",
      "(Iteration 13294 / 14700) loss: 0.797262\n",
      "(Iteration 13295 / 14700) loss: 0.741551\n",
      "(Iteration 13296 / 14700) loss: 0.864198\n",
      "(Iteration 13297 / 14700) loss: 0.701818\n",
      "(Iteration 13298 / 14700) loss: 0.480839\n",
      "(Iteration 13299 / 14700) loss: 0.448943\n",
      "(Iteration 13300 / 14700) loss: 0.771947\n",
      "(Iteration 13301 / 14700) loss: 0.664053\n",
      "(Iteration 13302 / 14700) loss: 0.609238\n",
      "(Iteration 13303 / 14700) loss: 0.611843\n",
      "(Iteration 13304 / 14700) loss: 0.724770\n",
      "(Iteration 13305 / 14700) loss: 0.526987\n",
      "(Iteration 13306 / 14700) loss: 0.952555\n",
      "(Iteration 13307 / 14700) loss: 0.670423\n",
      "(Iteration 13308 / 14700) loss: 0.759150\n",
      "(Iteration 13309 / 14700) loss: 0.871564\n",
      "(Iteration 13310 / 14700) loss: 0.642739\n",
      "(Iteration 13311 / 14700) loss: 0.712111\n",
      "(Iteration 13312 / 14700) loss: 0.729960\n",
      "(Iteration 13313 / 14700) loss: 0.802909\n",
      "(Iteration 13314 / 14700) loss: 0.654503\n",
      "(Iteration 13315 / 14700) loss: 0.573245\n",
      "(Iteration 13316 / 14700) loss: 0.654972\n",
      "(Iteration 13317 / 14700) loss: 0.978529\n",
      "(Iteration 13318 / 14700) loss: 0.664903\n",
      "(Iteration 13319 / 14700) loss: 0.901818\n",
      "(Iteration 13320 / 14700) loss: 0.643527\n",
      "(Iteration 13321 / 14700) loss: 0.807084\n",
      "(Iteration 13322 / 14700) loss: 0.986232\n",
      "(Iteration 13323 / 14700) loss: 0.817874\n",
      "(Iteration 13324 / 14700) loss: 0.776918\n",
      "(Iteration 13325 / 14700) loss: 0.607528\n",
      "(Iteration 13326 / 14700) loss: 0.789126\n",
      "(Iteration 13327 / 14700) loss: 0.786522\n",
      "(Iteration 13328 / 14700) loss: 0.586707\n",
      "(Iteration 13329 / 14700) loss: 0.622338\n",
      "(Iteration 13330 / 14700) loss: 0.758751\n",
      "(Iteration 13331 / 14700) loss: 0.897335\n",
      "(Iteration 13332 / 14700) loss: 1.067303\n",
      "(Iteration 13333 / 14700) loss: 0.813929\n",
      "(Iteration 13334 / 14700) loss: 0.797465\n",
      "(Iteration 13335 / 14700) loss: 0.889724\n",
      "(Iteration 13336 / 14700) loss: 0.800850\n",
      "(Iteration 13337 / 14700) loss: 0.929271\n",
      "(Iteration 13338 / 14700) loss: 0.781134\n",
      "(Iteration 13339 / 14700) loss: 0.613690\n",
      "(Iteration 13340 / 14700) loss: 0.788652\n",
      "(Iteration 13341 / 14700) loss: 0.659687\n",
      "(Iteration 13342 / 14700) loss: 0.756610\n",
      "(Iteration 13343 / 14700) loss: 0.647003\n",
      "(Iteration 13344 / 14700) loss: 0.582847\n",
      "(Iteration 13345 / 14700) loss: 0.491056\n",
      "(Iteration 13346 / 14700) loss: 0.983824\n",
      "(Iteration 13347 / 14700) loss: 0.541610\n",
      "(Iteration 13348 / 14700) loss: 0.689322\n",
      "(Iteration 13349 / 14700) loss: 0.608515\n",
      "(Iteration 13350 / 14700) loss: 0.659800\n",
      "(Iteration 13351 / 14700) loss: 0.868116\n",
      "(Iteration 13352 / 14700) loss: 0.764489\n",
      "(Iteration 13353 / 14700) loss: 0.684874\n",
      "(Iteration 13354 / 14700) loss: 0.747876\n",
      "(Iteration 13355 / 14700) loss: 0.642211\n",
      "(Iteration 13356 / 14700) loss: 0.745175\n",
      "(Iteration 13357 / 14700) loss: 0.740878\n",
      "(Iteration 13358 / 14700) loss: 0.762908\n",
      "(Iteration 13359 / 14700) loss: 0.784313\n",
      "(Iteration 13360 / 14700) loss: 0.722414\n",
      "(Iteration 13361 / 14700) loss: 0.774940\n",
      "(Iteration 13362 / 14700) loss: 0.615901\n",
      "(Iteration 13363 / 14700) loss: 0.558403\n",
      "(Iteration 13364 / 14700) loss: 0.652630\n",
      "(Iteration 13365 / 14700) loss: 0.615535\n",
      "(Iteration 13366 / 14700) loss: 0.776371\n",
      "(Iteration 13367 / 14700) loss: 0.664437\n",
      "(Iteration 13368 / 14700) loss: 0.687147\n",
      "(Iteration 13369 / 14700) loss: 1.175714\n",
      "(Iteration 13370 / 14700) loss: 0.613528\n",
      "(Iteration 13371 / 14700) loss: 0.865708\n",
      "(Iteration 13372 / 14700) loss: 0.666126\n",
      "(Iteration 13373 / 14700) loss: 0.678038\n",
      "(Iteration 13374 / 14700) loss: 0.799267\n",
      "(Iteration 13375 / 14700) loss: 0.719445\n",
      "(Iteration 13376 / 14700) loss: 0.527970\n",
      "(Iteration 13377 / 14700) loss: 0.680165\n",
      "(Iteration 13378 / 14700) loss: 0.812424\n",
      "(Iteration 13379 / 14700) loss: 0.703507\n",
      "(Iteration 13380 / 14700) loss: 0.790931\n",
      "(Iteration 13381 / 14700) loss: 0.516040\n",
      "(Iteration 13382 / 14700) loss: 0.770318\n",
      "(Iteration 13383 / 14700) loss: 0.896325\n",
      "(Iteration 13384 / 14700) loss: 0.761002\n",
      "(Iteration 13385 / 14700) loss: 0.702744\n",
      "(Iteration 13386 / 14700) loss: 0.626491\n",
      "(Iteration 13387 / 14700) loss: 0.750140\n",
      "(Iteration 13388 / 14700) loss: 0.826517\n",
      "(Iteration 13389 / 14700) loss: 0.611177\n",
      "(Iteration 13390 / 14700) loss: 0.785734\n",
      "(Iteration 13391 / 14700) loss: 0.585769\n",
      "(Iteration 13392 / 14700) loss: 0.585291\n",
      "(Iteration 13393 / 14700) loss: 0.870300\n",
      "(Iteration 13394 / 14700) loss: 0.688821\n",
      "(Iteration 13395 / 14700) loss: 0.638183\n",
      "(Iteration 13396 / 14700) loss: 0.855474\n",
      "(Iteration 13397 / 14700) loss: 0.764316\n",
      "(Iteration 13398 / 14700) loss: 0.710451\n",
      "(Iteration 13399 / 14700) loss: 0.823539\n",
      "(Iteration 13400 / 14700) loss: 0.610145\n",
      "(Iteration 13401 / 14700) loss: 0.841524\n",
      "(Iteration 13402 / 14700) loss: 0.699050\n",
      "(Iteration 13403 / 14700) loss: 0.574526\n",
      "(Iteration 13404 / 14700) loss: 0.582893\n",
      "(Iteration 13405 / 14700) loss: 0.573104\n",
      "(Iteration 13406 / 14700) loss: 0.769955\n",
      "(Iteration 13407 / 14700) loss: 0.579579\n",
      "(Iteration 13408 / 14700) loss: 0.844208\n",
      "(Iteration 13409 / 14700) loss: 0.622251\n",
      "(Iteration 13410 / 14700) loss: 1.032319\n",
      "(Iteration 13411 / 14700) loss: 0.685823\n",
      "(Iteration 13412 / 14700) loss: 0.538811\n",
      "(Iteration 13413 / 14700) loss: 0.869533\n",
      "(Iteration 13414 / 14700) loss: 0.819710\n",
      "(Iteration 13415 / 14700) loss: 0.500258\n",
      "(Iteration 13416 / 14700) loss: 0.800505\n",
      "(Iteration 13417 / 14700) loss: 0.703569\n",
      "(Iteration 13418 / 14700) loss: 0.714555\n",
      "(Iteration 13419 / 14700) loss: 0.683968\n",
      "(Iteration 13420 / 14700) loss: 0.675440\n",
      "(Iteration 13421 / 14700) loss: 0.718938\n",
      "(Iteration 13422 / 14700) loss: 0.829031\n",
      "(Iteration 13423 / 14700) loss: 0.550594\n",
      "(Iteration 13424 / 14700) loss: 0.845365\n",
      "(Iteration 13425 / 14700) loss: 0.698469\n",
      "(Iteration 13426 / 14700) loss: 1.072446\n",
      "(Iteration 13427 / 14700) loss: 0.738091\n",
      "(Iteration 13428 / 14700) loss: 0.763176\n",
      "(Iteration 13429 / 14700) loss: 0.755944\n",
      "(Iteration 13430 / 14700) loss: 0.722372\n",
      "(Iteration 13431 / 14700) loss: 0.572345\n",
      "(Iteration 13432 / 14700) loss: 0.784713\n",
      "(Iteration 13433 / 14700) loss: 0.975016\n",
      "(Iteration 13434 / 14700) loss: 0.755026\n",
      "(Iteration 13435 / 14700) loss: 0.514181\n",
      "(Iteration 13436 / 14700) loss: 0.659121\n",
      "(Iteration 13437 / 14700) loss: 0.785945\n",
      "(Iteration 13438 / 14700) loss: 0.466701\n",
      "(Iteration 13439 / 14700) loss: 0.397811\n",
      "(Iteration 13440 / 14700) loss: 0.685307\n",
      "(Iteration 13441 / 14700) loss: 0.719932\n",
      "(Iteration 13442 / 14700) loss: 0.836991\n",
      "(Iteration 13443 / 14700) loss: 0.633323\n",
      "(Iteration 13444 / 14700) loss: 0.700021\n",
      "(Iteration 13445 / 14700) loss: 0.666922\n",
      "(Iteration 13446 / 14700) loss: 0.632938\n",
      "(Iteration 13447 / 14700) loss: 0.502940\n",
      "(Iteration 13448 / 14700) loss: 0.472870\n",
      "(Iteration 13449 / 14700) loss: 0.793591\n",
      "(Iteration 13450 / 14700) loss: 0.811255\n",
      "(Iteration 13451 / 14700) loss: 0.630909\n",
      "(Iteration 13452 / 14700) loss: 0.740170\n",
      "(Iteration 13453 / 14700) loss: 0.815918\n",
      "(Iteration 13454 / 14700) loss: 1.002848\n",
      "(Iteration 13455 / 14700) loss: 0.599923\n",
      "(Iteration 13456 / 14700) loss: 0.661603\n",
      "(Iteration 13457 / 14700) loss: 0.708615\n",
      "(Iteration 13458 / 14700) loss: 0.729080\n",
      "(Iteration 13459 / 14700) loss: 0.716850\n",
      "(Iteration 13460 / 14700) loss: 0.798008\n",
      "(Iteration 13461 / 14700) loss: 0.678109\n",
      "(Iteration 13462 / 14700) loss: 0.590857\n",
      "(Iteration 13463 / 14700) loss: 0.714908\n",
      "(Iteration 13464 / 14700) loss: 0.805794\n",
      "(Iteration 13465 / 14700) loss: 0.774969\n",
      "(Iteration 13466 / 14700) loss: 0.785486\n",
      "(Iteration 13467 / 14700) loss: 0.595488\n",
      "(Iteration 13468 / 14700) loss: 0.550121\n",
      "(Iteration 13469 / 14700) loss: 0.765695\n",
      "(Iteration 13470 / 14700) loss: 0.682673\n",
      "(Iteration 13471 / 14700) loss: 0.696621\n",
      "(Iteration 13472 / 14700) loss: 0.639408\n",
      "(Iteration 13473 / 14700) loss: 0.543426\n",
      "(Iteration 13474 / 14700) loss: 0.685010\n",
      "(Iteration 13475 / 14700) loss: 0.586405\n",
      "(Iteration 13476 / 14700) loss: 0.697361\n",
      "(Iteration 13477 / 14700) loss: 0.929043\n",
      "(Iteration 13478 / 14700) loss: 0.793018\n",
      "(Iteration 13479 / 14700) loss: 0.714091\n",
      "(Iteration 13480 / 14700) loss: 0.565100\n",
      "(Iteration 13481 / 14700) loss: 0.733917\n",
      "(Iteration 13482 / 14700) loss: 0.722767\n",
      "(Iteration 13483 / 14700) loss: 0.790234\n",
      "(Iteration 13484 / 14700) loss: 0.889522\n",
      "(Iteration 13485 / 14700) loss: 0.412543\n",
      "(Iteration 13486 / 14700) loss: 0.695038\n",
      "(Iteration 13487 / 14700) loss: 0.494826\n",
      "(Iteration 13488 / 14700) loss: 0.800692\n",
      "(Iteration 13489 / 14700) loss: 0.830326\n",
      "(Iteration 13490 / 14700) loss: 0.623765\n",
      "(Iteration 13491 / 14700) loss: 0.658722\n",
      "(Iteration 13492 / 14700) loss: 0.772511\n",
      "(Iteration 13493 / 14700) loss: 1.012559\n",
      "(Iteration 13494 / 14700) loss: 0.979993\n",
      "(Iteration 13495 / 14700) loss: 0.650241\n",
      "(Iteration 13496 / 14700) loss: 0.666413\n",
      "(Iteration 13497 / 14700) loss: 0.749604\n",
      "(Iteration 13498 / 14700) loss: 1.025554\n",
      "(Iteration 13499 / 14700) loss: 0.537245\n",
      "(Iteration 13500 / 14700) loss: 0.951644\n",
      "(Iteration 13501 / 14700) loss: 0.629046\n",
      "(Iteration 13502 / 14700) loss: 0.411901\n",
      "(Iteration 13503 / 14700) loss: 0.935873\n",
      "(Iteration 13504 / 14700) loss: 0.912401\n",
      "(Iteration 13505 / 14700) loss: 0.926051\n",
      "(Iteration 13506 / 14700) loss: 0.738740\n",
      "(Iteration 13507 / 14700) loss: 0.666158\n",
      "(Iteration 13508 / 14700) loss: 0.710770\n",
      "(Iteration 13509 / 14700) loss: 0.559408\n",
      "(Iteration 13510 / 14700) loss: 0.555779\n",
      "(Iteration 13511 / 14700) loss: 0.571606\n",
      "(Iteration 13512 / 14700) loss: 0.575151\n",
      "(Iteration 13513 / 14700) loss: 0.770854\n",
      "(Iteration 13514 / 14700) loss: 0.584130\n",
      "(Iteration 13515 / 14700) loss: 0.622130\n",
      "(Iteration 13516 / 14700) loss: 0.729137\n",
      "(Iteration 13517 / 14700) loss: 0.651297\n",
      "(Iteration 13518 / 14700) loss: 0.542652\n",
      "(Iteration 13519 / 14700) loss: 0.695914\n",
      "(Iteration 13520 / 14700) loss: 0.685512\n",
      "(Iteration 13521 / 14700) loss: 0.855186\n",
      "(Iteration 13522 / 14700) loss: 0.569235\n",
      "(Iteration 13523 / 14700) loss: 0.892710\n",
      "(Iteration 13524 / 14700) loss: 1.095923\n",
      "(Iteration 13525 / 14700) loss: 0.681143\n",
      "(Iteration 13526 / 14700) loss: 0.742203\n",
      "(Iteration 13527 / 14700) loss: 0.631496\n",
      "(Iteration 13528 / 14700) loss: 0.630131\n",
      "(Iteration 13529 / 14700) loss: 0.669388\n",
      "(Iteration 13530 / 14700) loss: 0.879602\n",
      "(Iteration 13531 / 14700) loss: 0.564019\n",
      "(Iteration 13532 / 14700) loss: 0.697657\n",
      "(Iteration 13533 / 14700) loss: 0.684146\n",
      "(Iteration 13534 / 14700) loss: 0.795071\n",
      "(Iteration 13535 / 14700) loss: 0.939641\n",
      "(Iteration 13536 / 14700) loss: 0.658702\n",
      "(Iteration 13537 / 14700) loss: 0.670153\n",
      "(Iteration 13538 / 14700) loss: 0.587315\n",
      "(Iteration 13539 / 14700) loss: 0.779547\n",
      "(Iteration 13540 / 14700) loss: 0.873068\n",
      "(Iteration 13541 / 14700) loss: 0.866691\n",
      "(Iteration 13542 / 14700) loss: 0.608145\n",
      "(Iteration 13543 / 14700) loss: 0.639631\n",
      "(Iteration 13544 / 14700) loss: 0.825807\n",
      "(Iteration 13545 / 14700) loss: 0.682297\n",
      "(Iteration 13546 / 14700) loss: 0.595571\n",
      "(Iteration 13547 / 14700) loss: 0.599645\n",
      "(Iteration 13548 / 14700) loss: 0.489762\n",
      "(Iteration 13549 / 14700) loss: 0.653917\n",
      "(Iteration 13550 / 14700) loss: 0.518418\n",
      "(Iteration 13551 / 14700) loss: 0.602333\n",
      "(Iteration 13552 / 14700) loss: 0.669026\n",
      "(Iteration 13553 / 14700) loss: 0.567569\n",
      "(Iteration 13554 / 14700) loss: 0.958452\n",
      "(Iteration 13555 / 14700) loss: 0.586697\n",
      "(Iteration 13556 / 14700) loss: 0.656435\n",
      "(Iteration 13557 / 14700) loss: 0.566071\n",
      "(Iteration 13558 / 14700) loss: 0.745410\n",
      "(Iteration 13559 / 14700) loss: 0.799420\n",
      "(Iteration 13560 / 14700) loss: 0.746694\n",
      "(Iteration 13561 / 14700) loss: 0.643490\n",
      "(Iteration 13562 / 14700) loss: 0.790999\n",
      "(Iteration 13563 / 14700) loss: 0.613450\n",
      "(Iteration 13564 / 14700) loss: 0.610914\n",
      "(Iteration 13565 / 14700) loss: 0.714196\n",
      "(Iteration 13566 / 14700) loss: 0.780012\n",
      "(Iteration 13567 / 14700) loss: 0.530798\n",
      "(Iteration 13568 / 14700) loss: 0.799783\n",
      "(Iteration 13569 / 14700) loss: 0.682490\n",
      "(Iteration 13570 / 14700) loss: 0.944845\n",
      "(Iteration 13571 / 14700) loss: 0.783339\n",
      "(Iteration 13572 / 14700) loss: 0.953453\n",
      "(Iteration 13573 / 14700) loss: 0.509889\n",
      "(Iteration 13574 / 14700) loss: 0.652339\n",
      "(Iteration 13575 / 14700) loss: 0.798390\n",
      "(Iteration 13576 / 14700) loss: 0.804951\n",
      "(Iteration 13577 / 14700) loss: 0.789164\n",
      "(Iteration 13578 / 14700) loss: 0.896171\n",
      "(Iteration 13579 / 14700) loss: 0.456555\n",
      "(Iteration 13580 / 14700) loss: 0.791588\n",
      "(Iteration 13581 / 14700) loss: 0.463329\n",
      "(Iteration 13582 / 14700) loss: 1.004189\n",
      "(Iteration 13583 / 14700) loss: 0.801665\n",
      "(Iteration 13584 / 14700) loss: 0.712162\n",
      "(Iteration 13585 / 14700) loss: 0.548138\n",
      "(Iteration 13586 / 14700) loss: 0.709132\n",
      "(Iteration 13587 / 14700) loss: 0.813957\n",
      "(Iteration 13588 / 14700) loss: 0.941153\n",
      "(Iteration 13589 / 14700) loss: 0.925015\n",
      "(Iteration 13590 / 14700) loss: 0.506513\n",
      "(Iteration 13591 / 14700) loss: 0.791096\n",
      "(Iteration 13592 / 14700) loss: 0.761514\n",
      "(Iteration 13593 / 14700) loss: 0.767420\n",
      "(Iteration 13594 / 14700) loss: 0.760979\n",
      "(Iteration 13595 / 14700) loss: 0.766807\n",
      "(Iteration 13596 / 14700) loss: 0.566208\n",
      "(Iteration 13597 / 14700) loss: 0.566822\n",
      "(Iteration 13598 / 14700) loss: 0.628592\n",
      "(Iteration 13599 / 14700) loss: 0.616581\n",
      "(Iteration 13600 / 14700) loss: 0.868326\n",
      "(Iteration 13601 / 14700) loss: 0.847003\n",
      "(Iteration 13602 / 14700) loss: 0.561191\n",
      "(Iteration 13603 / 14700) loss: 0.692604\n",
      "(Iteration 13604 / 14700) loss: 0.825281\n",
      "(Iteration 13605 / 14700) loss: 0.582843\n",
      "(Iteration 13606 / 14700) loss: 0.882110\n",
      "(Iteration 13607 / 14700) loss: 0.644862\n",
      "(Iteration 13608 / 14700) loss: 0.654469\n",
      "(Iteration 13609 / 14700) loss: 0.627857\n",
      "(Iteration 13610 / 14700) loss: 0.488387\n",
      "(Iteration 13611 / 14700) loss: 0.743119\n",
      "(Iteration 13612 / 14700) loss: 0.831971\n",
      "(Iteration 13613 / 14700) loss: 0.583457\n",
      "(Iteration 13614 / 14700) loss: 0.629673\n",
      "(Iteration 13615 / 14700) loss: 0.637281\n",
      "(Iteration 13616 / 14700) loss: 0.613086\n",
      "(Iteration 13617 / 14700) loss: 0.796047\n",
      "(Iteration 13618 / 14700) loss: 0.516777\n",
      "(Iteration 13619 / 14700) loss: 0.693613\n",
      "(Iteration 13620 / 14700) loss: 0.719338\n",
      "(Iteration 13621 / 14700) loss: 0.597760\n",
      "(Iteration 13622 / 14700) loss: 0.939040\n",
      "(Iteration 13623 / 14700) loss: 0.755204\n",
      "(Iteration 13624 / 14700) loss: 0.671366\n",
      "(Iteration 13625 / 14700) loss: 0.637778\n",
      "(Iteration 13626 / 14700) loss: 0.735860\n",
      "(Iteration 13627 / 14700) loss: 0.872534\n",
      "(Iteration 13628 / 14700) loss: 0.593639\n",
      "(Iteration 13629 / 14700) loss: 0.630093\n",
      "(Iteration 13630 / 14700) loss: 1.065574\n",
      "(Iteration 13631 / 14700) loss: 0.599154\n",
      "(Iteration 13632 / 14700) loss: 0.608347\n",
      "(Iteration 13633 / 14700) loss: 0.608608\n",
      "(Iteration 13634 / 14700) loss: 0.626730\n",
      "(Iteration 13635 / 14700) loss: 0.572691\n",
      "(Iteration 13636 / 14700) loss: 0.690507\n",
      "(Iteration 13637 / 14700) loss: 0.429727\n",
      "(Iteration 13638 / 14700) loss: 0.728157\n",
      "(Iteration 13639 / 14700) loss: 0.858194\n",
      "(Iteration 13640 / 14700) loss: 0.708135\n",
      "(Iteration 13641 / 14700) loss: 0.619471\n",
      "(Iteration 13642 / 14700) loss: 0.680609\n",
      "(Iteration 13643 / 14700) loss: 0.656057\n",
      "(Iteration 13644 / 14700) loss: 0.874868\n",
      "(Iteration 13645 / 14700) loss: 0.473665\n",
      "(Iteration 13646 / 14700) loss: 0.656801\n",
      "(Iteration 13647 / 14700) loss: 0.677909\n",
      "(Iteration 13648 / 14700) loss: 0.725879\n",
      "(Iteration 13649 / 14700) loss: 0.552533\n",
      "(Iteration 13650 / 14700) loss: 0.632089\n",
      "(Iteration 13651 / 14700) loss: 0.771150\n",
      "(Iteration 13652 / 14700) loss: 0.629168\n",
      "(Iteration 13653 / 14700) loss: 0.818358\n",
      "(Iteration 13654 / 14700) loss: 0.906658\n",
      "(Iteration 13655 / 14700) loss: 0.907043\n",
      "(Iteration 13656 / 14700) loss: 0.954393\n",
      "(Iteration 13657 / 14700) loss: 0.659171\n",
      "(Iteration 13658 / 14700) loss: 0.485888\n",
      "(Iteration 13659 / 14700) loss: 1.035670\n",
      "(Iteration 13660 / 14700) loss: 0.879846\n",
      "(Iteration 13661 / 14700) loss: 0.821350\n",
      "(Iteration 13662 / 14700) loss: 0.766370\n",
      "(Iteration 13663 / 14700) loss: 0.680433\n",
      "(Iteration 13664 / 14700) loss: 0.687884\n",
      "(Iteration 13665 / 14700) loss: 0.636436\n",
      "(Iteration 13666 / 14700) loss: 0.722486\n",
      "(Iteration 13667 / 14700) loss: 0.887820\n",
      "(Iteration 13668 / 14700) loss: 0.464217\n",
      "(Iteration 13669 / 14700) loss: 0.544529\n",
      "(Iteration 13670 / 14700) loss: 0.614833\n",
      "(Iteration 13671 / 14700) loss: 0.683536\n",
      "(Iteration 13672 / 14700) loss: 0.575985\n",
      "(Iteration 13673 / 14700) loss: 0.783154\n",
      "(Iteration 13674 / 14700) loss: 0.895425\n",
      "(Iteration 13675 / 14700) loss: 0.771165\n",
      "(Iteration 13676 / 14700) loss: 0.803533\n",
      "(Iteration 13677 / 14700) loss: 0.560029\n",
      "(Iteration 13678 / 14700) loss: 0.674900\n",
      "(Iteration 13679 / 14700) loss: 0.870330\n",
      "(Iteration 13680 / 14700) loss: 0.536962\n",
      "(Iteration 13681 / 14700) loss: 0.579985\n",
      "(Iteration 13682 / 14700) loss: 0.818763\n",
      "(Iteration 13683 / 14700) loss: 0.937641\n",
      "(Iteration 13684 / 14700) loss: 0.486152\n",
      "(Iteration 13685 / 14700) loss: 0.681862\n",
      "(Iteration 13686 / 14700) loss: 0.733172\n",
      "(Iteration 13687 / 14700) loss: 0.699538\n",
      "(Iteration 13688 / 14700) loss: 0.730422\n",
      "(Iteration 13689 / 14700) loss: 0.742570\n",
      "(Iteration 13690 / 14700) loss: 0.795289\n",
      "(Iteration 13691 / 14700) loss: 0.639022\n",
      "(Iteration 13692 / 14700) loss: 0.648652\n",
      "(Iteration 13693 / 14700) loss: 0.665848\n",
      "(Iteration 13694 / 14700) loss: 0.723250\n",
      "(Iteration 13695 / 14700) loss: 0.700118\n",
      "(Iteration 13696 / 14700) loss: 0.702214\n",
      "(Iteration 13697 / 14700) loss: 0.663924\n",
      "(Iteration 13698 / 14700) loss: 0.666022\n",
      "(Iteration 13699 / 14700) loss: 0.748748\n",
      "(Iteration 13700 / 14700) loss: 0.782309\n",
      "(Iteration 13701 / 14700) loss: 0.697420\n",
      "(Iteration 13702 / 14700) loss: 0.739939\n",
      "(Iteration 13703 / 14700) loss: 0.865987\n",
      "(Iteration 13704 / 14700) loss: 0.849117\n",
      "(Iteration 13705 / 14700) loss: 1.011401\n",
      "(Iteration 13706 / 14700) loss: 0.552775\n",
      "(Iteration 13707 / 14700) loss: 0.619150\n",
      "(Iteration 13708 / 14700) loss: 0.613837\n",
      "(Iteration 13709 / 14700) loss: 0.778986\n",
      "(Iteration 13710 / 14700) loss: 0.666562\n",
      "(Iteration 13711 / 14700) loss: 0.702555\n",
      "(Iteration 13712 / 14700) loss: 0.705482\n",
      "(Iteration 13713 / 14700) loss: 0.623304\n",
      "(Iteration 13714 / 14700) loss: 0.667051\n",
      "(Iteration 13715 / 14700) loss: 0.656099\n",
      "(Iteration 13716 / 14700) loss: 0.596421\n",
      "(Iteration 13717 / 14700) loss: 0.540455\n",
      "(Iteration 13718 / 14700) loss: 0.805412\n",
      "(Iteration 13719 / 14700) loss: 0.776879\n",
      "(Iteration 13720 / 14700) loss: 0.720033\n",
      "(Epoch 14 / 15) train acc: 0.829000; val_acc: 0.773000\n",
      "(Iteration 13721 / 14700) loss: 0.797912\n",
      "(Iteration 13722 / 14700) loss: 0.500214\n",
      "(Iteration 13723 / 14700) loss: 0.962654\n",
      "(Iteration 13724 / 14700) loss: 0.936709\n",
      "(Iteration 13725 / 14700) loss: 0.583280\n",
      "(Iteration 13726 / 14700) loss: 0.698761\n",
      "(Iteration 13727 / 14700) loss: 0.792408\n",
      "(Iteration 13728 / 14700) loss: 0.796337\n",
      "(Iteration 13729 / 14700) loss: 0.625080\n",
      "(Iteration 13730 / 14700) loss: 0.822236\n",
      "(Iteration 13731 / 14700) loss: 0.642965\n",
      "(Iteration 13732 / 14700) loss: 0.752642\n",
      "(Iteration 13733 / 14700) loss: 0.746145\n",
      "(Iteration 13734 / 14700) loss: 0.707357\n",
      "(Iteration 13735 / 14700) loss: 0.736993\n",
      "(Iteration 13736 / 14700) loss: 0.667163\n",
      "(Iteration 13737 / 14700) loss: 0.482504\n",
      "(Iteration 13738 / 14700) loss: 0.873724\n",
      "(Iteration 13739 / 14700) loss: 0.591973\n",
      "(Iteration 13740 / 14700) loss: 0.671898\n",
      "(Iteration 13741 / 14700) loss: 0.743133\n",
      "(Iteration 13742 / 14700) loss: 0.747881\n",
      "(Iteration 13743 / 14700) loss: 0.859487\n",
      "(Iteration 13744 / 14700) loss: 0.560539\n",
      "(Iteration 13745 / 14700) loss: 0.744287\n",
      "(Iteration 13746 / 14700) loss: 0.670478\n",
      "(Iteration 13747 / 14700) loss: 0.773827\n",
      "(Iteration 13748 / 14700) loss: 0.693536\n",
      "(Iteration 13749 / 14700) loss: 0.588478\n",
      "(Iteration 13750 / 14700) loss: 0.605037\n",
      "(Iteration 13751 / 14700) loss: 0.648832\n",
      "(Iteration 13752 / 14700) loss: 0.730232\n",
      "(Iteration 13753 / 14700) loss: 0.915504\n",
      "(Iteration 13754 / 14700) loss: 0.658486\n",
      "(Iteration 13755 / 14700) loss: 0.551900\n",
      "(Iteration 13756 / 14700) loss: 0.795827\n",
      "(Iteration 13757 / 14700) loss: 0.735238\n",
      "(Iteration 13758 / 14700) loss: 0.737615\n",
      "(Iteration 13759 / 14700) loss: 0.777280\n",
      "(Iteration 13760 / 14700) loss: 0.733521\n",
      "(Iteration 13761 / 14700) loss: 0.679159\n",
      "(Iteration 13762 / 14700) loss: 0.706410\n",
      "(Iteration 13763 / 14700) loss: 0.758690\n",
      "(Iteration 13764 / 14700) loss: 0.593630\n",
      "(Iteration 13765 / 14700) loss: 0.569070\n",
      "(Iteration 13766 / 14700) loss: 0.647818\n",
      "(Iteration 13767 / 14700) loss: 0.698814\n",
      "(Iteration 13768 / 14700) loss: 0.621493\n",
      "(Iteration 13769 / 14700) loss: 0.705427\n",
      "(Iteration 13770 / 14700) loss: 0.627538\n",
      "(Iteration 13771 / 14700) loss: 0.758008\n",
      "(Iteration 13772 / 14700) loss: 0.740897\n",
      "(Iteration 13773 / 14700) loss: 0.760771\n",
      "(Iteration 13774 / 14700) loss: 0.762354\n",
      "(Iteration 13775 / 14700) loss: 0.760286\n",
      "(Iteration 13776 / 14700) loss: 0.692360\n",
      "(Iteration 13777 / 14700) loss: 0.984352\n",
      "(Iteration 13778 / 14700) loss: 0.747768\n",
      "(Iteration 13779 / 14700) loss: 0.606333\n",
      "(Iteration 13780 / 14700) loss: 0.503490\n",
      "(Iteration 13781 / 14700) loss: 0.747073\n",
      "(Iteration 13782 / 14700) loss: 0.773473\n",
      "(Iteration 13783 / 14700) loss: 0.494134\n",
      "(Iteration 13784 / 14700) loss: 0.679362\n",
      "(Iteration 13785 / 14700) loss: 0.626172\n",
      "(Iteration 13786 / 14700) loss: 0.586995\n",
      "(Iteration 13787 / 14700) loss: 0.831027\n",
      "(Iteration 13788 / 14700) loss: 0.519196\n",
      "(Iteration 13789 / 14700) loss: 0.667919\n",
      "(Iteration 13790 / 14700) loss: 0.664572\n",
      "(Iteration 13791 / 14700) loss: 0.496758\n",
      "(Iteration 13792 / 14700) loss: 0.590925\n",
      "(Iteration 13793 / 14700) loss: 0.950920\n",
      "(Iteration 13794 / 14700) loss: 0.686513\n",
      "(Iteration 13795 / 14700) loss: 0.779763\n",
      "(Iteration 13796 / 14700) loss: 0.924214\n",
      "(Iteration 13797 / 14700) loss: 0.826873\n",
      "(Iteration 13798 / 14700) loss: 0.797524\n",
      "(Iteration 13799 / 14700) loss: 0.762477\n",
      "(Iteration 13800 / 14700) loss: 0.591945\n",
      "(Iteration 13801 / 14700) loss: 0.859791\n",
      "(Iteration 13802 / 14700) loss: 0.817291\n",
      "(Iteration 13803 / 14700) loss: 0.844718\n",
      "(Iteration 13804 / 14700) loss: 0.833292\n",
      "(Iteration 13805 / 14700) loss: 0.816802\n",
      "(Iteration 13806 / 14700) loss: 0.636820\n",
      "(Iteration 13807 / 14700) loss: 0.644802\n",
      "(Iteration 13808 / 14700) loss: 0.565070\n",
      "(Iteration 13809 / 14700) loss: 0.572328\n",
      "(Iteration 13810 / 14700) loss: 0.809582\n",
      "(Iteration 13811 / 14700) loss: 0.676608\n",
      "(Iteration 13812 / 14700) loss: 0.652515\n",
      "(Iteration 13813 / 14700) loss: 0.616618\n",
      "(Iteration 13814 / 14700) loss: 0.708115\n",
      "(Iteration 13815 / 14700) loss: 0.795934\n",
      "(Iteration 13816 / 14700) loss: 0.533523\n",
      "(Iteration 13817 / 14700) loss: 0.671270\n",
      "(Iteration 13818 / 14700) loss: 0.531662\n",
      "(Iteration 13819 / 14700) loss: 0.614744\n",
      "(Iteration 13820 / 14700) loss: 0.766765\n",
      "(Iteration 13821 / 14700) loss: 0.621624\n",
      "(Iteration 13822 / 14700) loss: 0.876521\n",
      "(Iteration 13823 / 14700) loss: 0.992346\n",
      "(Iteration 13824 / 14700) loss: 0.631784\n",
      "(Iteration 13825 / 14700) loss: 0.696890\n",
      "(Iteration 13826 / 14700) loss: 0.868562\n",
      "(Iteration 13827 / 14700) loss: 0.576598\n",
      "(Iteration 13828 / 14700) loss: 0.560010\n",
      "(Iteration 13829 / 14700) loss: 0.812358\n",
      "(Iteration 13830 / 14700) loss: 0.459913\n",
      "(Iteration 13831 / 14700) loss: 0.630236\n",
      "(Iteration 13832 / 14700) loss: 0.672969\n",
      "(Iteration 13833 / 14700) loss: 0.629434\n",
      "(Iteration 13834 / 14700) loss: 0.756633\n",
      "(Iteration 13835 / 14700) loss: 0.597040\n",
      "(Iteration 13836 / 14700) loss: 0.812084\n",
      "(Iteration 13837 / 14700) loss: 0.612964\n",
      "(Iteration 13838 / 14700) loss: 0.538500\n",
      "(Iteration 13839 / 14700) loss: 0.769605\n",
      "(Iteration 13840 / 14700) loss: 0.458575\n",
      "(Iteration 13841 / 14700) loss: 0.823725\n",
      "(Iteration 13842 / 14700) loss: 0.700467\n",
      "(Iteration 13843 / 14700) loss: 0.753629\n",
      "(Iteration 13844 / 14700) loss: 0.878466\n",
      "(Iteration 13845 / 14700) loss: 0.617907\n",
      "(Iteration 13846 / 14700) loss: 0.693530\n",
      "(Iteration 13847 / 14700) loss: 0.626323\n",
      "(Iteration 13848 / 14700) loss: 0.624609\n",
      "(Iteration 13849 / 14700) loss: 1.091854\n",
      "(Iteration 13850 / 14700) loss: 0.679399\n",
      "(Iteration 13851 / 14700) loss: 0.592087\n",
      "(Iteration 13852 / 14700) loss: 0.659692\n",
      "(Iteration 13853 / 14700) loss: 0.683457\n",
      "(Iteration 13854 / 14700) loss: 0.608636\n",
      "(Iteration 13855 / 14700) loss: 0.639592\n",
      "(Iteration 13856 / 14700) loss: 0.685290\n",
      "(Iteration 13857 / 14700) loss: 0.760719\n",
      "(Iteration 13858 / 14700) loss: 0.721043\n",
      "(Iteration 13859 / 14700) loss: 0.894515\n",
      "(Iteration 13860 / 14700) loss: 0.867433\n",
      "(Iteration 13861 / 14700) loss: 0.701134\n",
      "(Iteration 13862 / 14700) loss: 0.640984\n",
      "(Iteration 13863 / 14700) loss: 0.687483\n",
      "(Iteration 13864 / 14700) loss: 0.682855\n",
      "(Iteration 13865 / 14700) loss: 0.430713\n",
      "(Iteration 13866 / 14700) loss: 0.655360\n",
      "(Iteration 13867 / 14700) loss: 1.055374\n",
      "(Iteration 13868 / 14700) loss: 0.633134\n",
      "(Iteration 13869 / 14700) loss: 0.611305\n",
      "(Iteration 13870 / 14700) loss: 0.696927\n",
      "(Iteration 13871 / 14700) loss: 1.002152\n",
      "(Iteration 13872 / 14700) loss: 0.670804\n",
      "(Iteration 13873 / 14700) loss: 0.711742\n",
      "(Iteration 13874 / 14700) loss: 0.652775\n",
      "(Iteration 13875 / 14700) loss: 0.666631\n",
      "(Iteration 13876 / 14700) loss: 0.512328\n",
      "(Iteration 13877 / 14700) loss: 0.691961\n",
      "(Iteration 13878 / 14700) loss: 0.896812\n",
      "(Iteration 13879 / 14700) loss: 0.608101\n",
      "(Iteration 13880 / 14700) loss: 0.606298\n",
      "(Iteration 13881 / 14700) loss: 0.527478\n",
      "(Iteration 13882 / 14700) loss: 0.686651\n",
      "(Iteration 13883 / 14700) loss: 0.724520\n",
      "(Iteration 13884 / 14700) loss: 0.674420\n",
      "(Iteration 13885 / 14700) loss: 0.736610\n",
      "(Iteration 13886 / 14700) loss: 0.665663\n",
      "(Iteration 13887 / 14700) loss: 0.680677\n",
      "(Iteration 13888 / 14700) loss: 0.505503\n",
      "(Iteration 13889 / 14700) loss: 0.663029\n",
      "(Iteration 13890 / 14700) loss: 0.588075\n",
      "(Iteration 13891 / 14700) loss: 0.541239\n",
      "(Iteration 13892 / 14700) loss: 1.032817\n",
      "(Iteration 13893 / 14700) loss: 0.933284\n",
      "(Iteration 13894 / 14700) loss: 0.615362\n",
      "(Iteration 13895 / 14700) loss: 0.530207\n",
      "(Iteration 13896 / 14700) loss: 0.681053\n",
      "(Iteration 13897 / 14700) loss: 0.538874\n",
      "(Iteration 13898 / 14700) loss: 0.459988\n",
      "(Iteration 13899 / 14700) loss: 0.711621\n",
      "(Iteration 13900 / 14700) loss: 0.663280\n",
      "(Iteration 13901 / 14700) loss: 0.662362\n",
      "(Iteration 13902 / 14700) loss: 0.750645\n",
      "(Iteration 13903 / 14700) loss: 0.709758\n",
      "(Iteration 13904 / 14700) loss: 0.779641\n",
      "(Iteration 13905 / 14700) loss: 0.754108\n",
      "(Iteration 13906 / 14700) loss: 0.694258\n",
      "(Iteration 13907 / 14700) loss: 0.737611\n",
      "(Iteration 13908 / 14700) loss: 0.771300\n",
      "(Iteration 13909 / 14700) loss: 0.635476\n",
      "(Iteration 13910 / 14700) loss: 0.611766\n",
      "(Iteration 13911 / 14700) loss: 0.654422\n",
      "(Iteration 13912 / 14700) loss: 0.600537\n",
      "(Iteration 13913 / 14700) loss: 0.620843\n",
      "(Iteration 13914 / 14700) loss: 0.542063\n",
      "(Iteration 13915 / 14700) loss: 0.893195\n",
      "(Iteration 13916 / 14700) loss: 0.892017\n",
      "(Iteration 13917 / 14700) loss: 0.680778\n",
      "(Iteration 13918 / 14700) loss: 0.536079\n",
      "(Iteration 13919 / 14700) loss: 0.748116\n",
      "(Iteration 13920 / 14700) loss: 0.735670\n",
      "(Iteration 13921 / 14700) loss: 0.689508\n",
      "(Iteration 13922 / 14700) loss: 0.806680\n",
      "(Iteration 13923 / 14700) loss: 0.727700\n",
      "(Iteration 13924 / 14700) loss: 0.676689\n",
      "(Iteration 13925 / 14700) loss: 0.697869\n",
      "(Iteration 13926 / 14700) loss: 0.845302\n",
      "(Iteration 13927 / 14700) loss: 1.124296\n",
      "(Iteration 13928 / 14700) loss: 0.973119\n",
      "(Iteration 13929 / 14700) loss: 0.678671\n",
      "(Iteration 13930 / 14700) loss: 0.782054\n",
      "(Iteration 13931 / 14700) loss: 0.689048\n",
      "(Iteration 13932 / 14700) loss: 0.656370\n",
      "(Iteration 13933 / 14700) loss: 0.700162\n",
      "(Iteration 13934 / 14700) loss: 0.596473\n",
      "(Iteration 13935 / 14700) loss: 0.641878\n",
      "(Iteration 13936 / 14700) loss: 0.869031\n",
      "(Iteration 13937 / 14700) loss: 0.614658\n",
      "(Iteration 13938 / 14700) loss: 0.588828\n",
      "(Iteration 13939 / 14700) loss: 0.768647\n",
      "(Iteration 13940 / 14700) loss: 0.927452\n",
      "(Iteration 13941 / 14700) loss: 0.575657\n",
      "(Iteration 13942 / 14700) loss: 0.616600\n",
      "(Iteration 13943 / 14700) loss: 0.699200\n",
      "(Iteration 13944 / 14700) loss: 0.697515\n",
      "(Iteration 13945 / 14700) loss: 0.744481\n",
      "(Iteration 13946 / 14700) loss: 0.765027\n",
      "(Iteration 13947 / 14700) loss: 0.749688\n",
      "(Iteration 13948 / 14700) loss: 0.800078\n",
      "(Iteration 13949 / 14700) loss: 0.825455\n",
      "(Iteration 13950 / 14700) loss: 0.689891\n",
      "(Iteration 13951 / 14700) loss: 0.664584\n",
      "(Iteration 13952 / 14700) loss: 0.681991\n",
      "(Iteration 13953 / 14700) loss: 0.645293\n",
      "(Iteration 13954 / 14700) loss: 0.722650\n",
      "(Iteration 13955 / 14700) loss: 0.706261\n",
      "(Iteration 13956 / 14700) loss: 0.694049\n",
      "(Iteration 13957 / 14700) loss: 0.756480\n",
      "(Iteration 13958 / 14700) loss: 0.667493\n",
      "(Iteration 13959 / 14700) loss: 0.909700\n",
      "(Iteration 13960 / 14700) loss: 0.769525\n",
      "(Iteration 13961 / 14700) loss: 0.711231\n",
      "(Iteration 13962 / 14700) loss: 0.557093\n",
      "(Iteration 13963 / 14700) loss: 0.664773\n",
      "(Iteration 13964 / 14700) loss: 0.690384\n",
      "(Iteration 13965 / 14700) loss: 0.718590\n",
      "(Iteration 13966 / 14700) loss: 0.763665\n",
      "(Iteration 13967 / 14700) loss: 1.208743\n",
      "(Iteration 13968 / 14700) loss: 0.522670\n",
      "(Iteration 13969 / 14700) loss: 0.750011\n",
      "(Iteration 13970 / 14700) loss: 0.785550\n",
      "(Iteration 13971 / 14700) loss: 0.674134\n",
      "(Iteration 13972 / 14700) loss: 0.603181\n",
      "(Iteration 13973 / 14700) loss: 0.765971\n",
      "(Iteration 13974 / 14700) loss: 0.527623\n",
      "(Iteration 13975 / 14700) loss: 0.766636\n",
      "(Iteration 13976 / 14700) loss: 0.577630\n",
      "(Iteration 13977 / 14700) loss: 0.799711\n",
      "(Iteration 13978 / 14700) loss: 0.628933\n",
      "(Iteration 13979 / 14700) loss: 0.976277\n",
      "(Iteration 13980 / 14700) loss: 0.903209\n",
      "(Iteration 13981 / 14700) loss: 0.564934\n",
      "(Iteration 13982 / 14700) loss: 0.558511\n",
      "(Iteration 13983 / 14700) loss: 0.861371\n",
      "(Iteration 13984 / 14700) loss: 0.862781\n",
      "(Iteration 13985 / 14700) loss: 0.798820\n",
      "(Iteration 13986 / 14700) loss: 0.673595\n",
      "(Iteration 13987 / 14700) loss: 0.481362\n",
      "(Iteration 13988 / 14700) loss: 0.623542\n",
      "(Iteration 13989 / 14700) loss: 0.462126\n",
      "(Iteration 13990 / 14700) loss: 1.020148\n",
      "(Iteration 13991 / 14700) loss: 0.704635\n",
      "(Iteration 13992 / 14700) loss: 0.844333\n",
      "(Iteration 13993 / 14700) loss: 0.807178\n",
      "(Iteration 13994 / 14700) loss: 0.751488\n",
      "(Iteration 13995 / 14700) loss: 0.724033\n",
      "(Iteration 13996 / 14700) loss: 0.709043\n",
      "(Iteration 13997 / 14700) loss: 0.700507\n",
      "(Iteration 13998 / 14700) loss: 0.768938\n",
      "(Iteration 13999 / 14700) loss: 0.666708\n",
      "(Iteration 14000 / 14700) loss: 0.798372\n",
      "(Iteration 14001 / 14700) loss: 0.566125\n",
      "(Iteration 14002 / 14700) loss: 0.663792\n",
      "(Iteration 14003 / 14700) loss: 0.601064\n",
      "(Iteration 14004 / 14700) loss: 0.820059\n",
      "(Iteration 14005 / 14700) loss: 0.880365\n",
      "(Iteration 14006 / 14700) loss: 0.590824\n",
      "(Iteration 14007 / 14700) loss: 0.773066\n",
      "(Iteration 14008 / 14700) loss: 0.910414\n",
      "(Iteration 14009 / 14700) loss: 0.588121\n",
      "(Iteration 14010 / 14700) loss: 0.579991\n",
      "(Iteration 14011 / 14700) loss: 0.653133\n",
      "(Iteration 14012 / 14700) loss: 0.990893\n",
      "(Iteration 14013 / 14700) loss: 0.725552\n",
      "(Iteration 14014 / 14700) loss: 0.688563\n",
      "(Iteration 14015 / 14700) loss: 0.653985\n",
      "(Iteration 14016 / 14700) loss: 0.831456\n",
      "(Iteration 14017 / 14700) loss: 0.745344\n",
      "(Iteration 14018 / 14700) loss: 0.406885\n",
      "(Iteration 14019 / 14700) loss: 0.719218\n",
      "(Iteration 14020 / 14700) loss: 1.020931\n",
      "(Iteration 14021 / 14700) loss: 0.595288\n",
      "(Iteration 14022 / 14700) loss: 0.767743\n",
      "(Iteration 14023 / 14700) loss: 0.707481\n",
      "(Iteration 14024 / 14700) loss: 0.727320\n",
      "(Iteration 14025 / 14700) loss: 0.972121\n",
      "(Iteration 14026 / 14700) loss: 1.236225\n",
      "(Iteration 14027 / 14700) loss: 0.712490\n",
      "(Iteration 14028 / 14700) loss: 0.593088\n",
      "(Iteration 14029 / 14700) loss: 0.742638\n",
      "(Iteration 14030 / 14700) loss: 0.872487\n",
      "(Iteration 14031 / 14700) loss: 0.534702\n",
      "(Iteration 14032 / 14700) loss: 0.972638\n",
      "(Iteration 14033 / 14700) loss: 0.801678\n",
      "(Iteration 14034 / 14700) loss: 0.686239\n",
      "(Iteration 14035 / 14700) loss: 0.508706\n",
      "(Iteration 14036 / 14700) loss: 1.087207\n",
      "(Iteration 14037 / 14700) loss: 0.745586\n",
      "(Iteration 14038 / 14700) loss: 0.619439\n",
      "(Iteration 14039 / 14700) loss: 0.718371\n",
      "(Iteration 14040 / 14700) loss: 0.785334\n",
      "(Iteration 14041 / 14700) loss: 0.804631\n",
      "(Iteration 14042 / 14700) loss: 0.822457\n",
      "(Iteration 14043 / 14700) loss: 0.656799\n",
      "(Iteration 14044 / 14700) loss: 0.978929\n",
      "(Iteration 14045 / 14700) loss: 0.628334\n",
      "(Iteration 14046 / 14700) loss: 0.797043\n",
      "(Iteration 14047 / 14700) loss: 0.773199\n",
      "(Iteration 14048 / 14700) loss: 0.611708\n",
      "(Iteration 14049 / 14700) loss: 0.853845\n",
      "(Iteration 14050 / 14700) loss: 0.555474\n",
      "(Iteration 14051 / 14700) loss: 0.625009\n",
      "(Iteration 14052 / 14700) loss: 0.801890\n",
      "(Iteration 14053 / 14700) loss: 0.612784\n",
      "(Iteration 14054 / 14700) loss: 0.679453\n",
      "(Iteration 14055 / 14700) loss: 0.636600\n",
      "(Iteration 14056 / 14700) loss: 0.914711\n",
      "(Iteration 14057 / 14700) loss: 0.853318\n",
      "(Iteration 14058 / 14700) loss: 0.506428\n",
      "(Iteration 14059 / 14700) loss: 0.582583\n",
      "(Iteration 14060 / 14700) loss: 0.564481\n",
      "(Iteration 14061 / 14700) loss: 0.937571\n",
      "(Iteration 14062 / 14700) loss: 0.651582\n",
      "(Iteration 14063 / 14700) loss: 0.681931\n",
      "(Iteration 14064 / 14700) loss: 0.746627\n",
      "(Iteration 14065 / 14700) loss: 0.446367\n",
      "(Iteration 14066 / 14700) loss: 0.685443\n",
      "(Iteration 14067 / 14700) loss: 0.615336\n",
      "(Iteration 14068 / 14700) loss: 0.586851\n",
      "(Iteration 14069 / 14700) loss: 0.745525\n",
      "(Iteration 14070 / 14700) loss: 0.834707\n",
      "(Iteration 14071 / 14700) loss: 0.692691\n",
      "(Iteration 14072 / 14700) loss: 0.681095\n",
      "(Iteration 14073 / 14700) loss: 0.603894\n",
      "(Iteration 14074 / 14700) loss: 0.678123\n",
      "(Iteration 14075 / 14700) loss: 0.908796\n",
      "(Iteration 14076 / 14700) loss: 0.745622\n",
      "(Iteration 14077 / 14700) loss: 0.786607\n",
      "(Iteration 14078 / 14700) loss: 0.763532\n",
      "(Iteration 14079 / 14700) loss: 0.562864\n",
      "(Iteration 14080 / 14700) loss: 0.768664\n",
      "(Iteration 14081 / 14700) loss: 0.673225\n",
      "(Iteration 14082 / 14700) loss: 0.599335\n",
      "(Iteration 14083 / 14700) loss: 0.821990\n",
      "(Iteration 14084 / 14700) loss: 0.830330\n",
      "(Iteration 14085 / 14700) loss: 0.511250\n",
      "(Iteration 14086 / 14700) loss: 0.601286\n",
      "(Iteration 14087 / 14700) loss: 0.638393\n",
      "(Iteration 14088 / 14700) loss: 0.869571\n",
      "(Iteration 14089 / 14700) loss: 0.739186\n",
      "(Iteration 14090 / 14700) loss: 0.668513\n",
      "(Iteration 14091 / 14700) loss: 0.738391\n",
      "(Iteration 14092 / 14700) loss: 0.605790\n",
      "(Iteration 14093 / 14700) loss: 0.623999\n",
      "(Iteration 14094 / 14700) loss: 0.627156\n",
      "(Iteration 14095 / 14700) loss: 0.552283\n",
      "(Iteration 14096 / 14700) loss: 0.772284\n",
      "(Iteration 14097 / 14700) loss: 0.730161\n",
      "(Iteration 14098 / 14700) loss: 0.577054\n",
      "(Iteration 14099 / 14700) loss: 0.694353\n",
      "(Iteration 14100 / 14700) loss: 0.408743\n",
      "(Iteration 14101 / 14700) loss: 0.807769\n",
      "(Iteration 14102 / 14700) loss: 0.738640\n",
      "(Iteration 14103 / 14700) loss: 0.629515\n",
      "(Iteration 14104 / 14700) loss: 0.758138\n",
      "(Iteration 14105 / 14700) loss: 0.479692\n",
      "(Iteration 14106 / 14700) loss: 0.882382\n",
      "(Iteration 14107 / 14700) loss: 0.523432\n",
      "(Iteration 14108 / 14700) loss: 0.885406\n",
      "(Iteration 14109 / 14700) loss: 0.871850\n",
      "(Iteration 14110 / 14700) loss: 0.808297\n",
      "(Iteration 14111 / 14700) loss: 0.902704\n",
      "(Iteration 14112 / 14700) loss: 0.939856\n",
      "(Iteration 14113 / 14700) loss: 0.693552\n",
      "(Iteration 14114 / 14700) loss: 0.783318\n",
      "(Iteration 14115 / 14700) loss: 0.996488\n",
      "(Iteration 14116 / 14700) loss: 0.675149\n",
      "(Iteration 14117 / 14700) loss: 0.902522\n",
      "(Iteration 14118 / 14700) loss: 0.747479\n",
      "(Iteration 14119 / 14700) loss: 0.689587\n",
      "(Iteration 14120 / 14700) loss: 0.644244\n",
      "(Iteration 14121 / 14700) loss: 0.754236\n",
      "(Iteration 14122 / 14700) loss: 0.814247\n",
      "(Iteration 14123 / 14700) loss: 0.830093\n",
      "(Iteration 14124 / 14700) loss: 0.476306\n",
      "(Iteration 14125 / 14700) loss: 0.633494\n",
      "(Iteration 14126 / 14700) loss: 0.653369\n",
      "(Iteration 14127 / 14700) loss: 0.726896\n",
      "(Iteration 14128 / 14700) loss: 0.561673\n",
      "(Iteration 14129 / 14700) loss: 0.442248\n",
      "(Iteration 14130 / 14700) loss: 0.733760\n",
      "(Iteration 14131 / 14700) loss: 0.581558\n",
      "(Iteration 14132 / 14700) loss: 0.710881\n",
      "(Iteration 14133 / 14700) loss: 0.595380\n",
      "(Iteration 14134 / 14700) loss: 0.599628\n",
      "(Iteration 14135 / 14700) loss: 0.667651\n",
      "(Iteration 14136 / 14700) loss: 0.616636\n",
      "(Iteration 14137 / 14700) loss: 0.767077\n",
      "(Iteration 14138 / 14700) loss: 0.764318\n",
      "(Iteration 14139 / 14700) loss: 1.009258\n",
      "(Iteration 14140 / 14700) loss: 0.635524\n",
      "(Iteration 14141 / 14700) loss: 1.000317\n",
      "(Iteration 14142 / 14700) loss: 0.645766\n",
      "(Iteration 14143 / 14700) loss: 0.713690\n",
      "(Iteration 14144 / 14700) loss: 0.669910\n",
      "(Iteration 14145 / 14700) loss: 0.713412\n",
      "(Iteration 14146 / 14700) loss: 0.566844\n",
      "(Iteration 14147 / 14700) loss: 0.619887\n",
      "(Iteration 14148 / 14700) loss: 0.834551\n",
      "(Iteration 14149 / 14700) loss: 0.662351\n",
      "(Iteration 14150 / 14700) loss: 0.743197\n",
      "(Iteration 14151 / 14700) loss: 0.765143\n",
      "(Iteration 14152 / 14700) loss: 0.692423\n",
      "(Iteration 14153 / 14700) loss: 0.980276\n",
      "(Iteration 14154 / 14700) loss: 0.777551\n",
      "(Iteration 14155 / 14700) loss: 0.560314\n",
      "(Iteration 14156 / 14700) loss: 0.671135\n",
      "(Iteration 14157 / 14700) loss: 0.710280\n",
      "(Iteration 14158 / 14700) loss: 0.670738\n",
      "(Iteration 14159 / 14700) loss: 0.715786\n",
      "(Iteration 14160 / 14700) loss: 0.644130\n",
      "(Iteration 14161 / 14700) loss: 0.646887\n",
      "(Iteration 14162 / 14700) loss: 0.621322\n",
      "(Iteration 14163 / 14700) loss: 0.725324\n",
      "(Iteration 14164 / 14700) loss: 0.715744\n",
      "(Iteration 14165 / 14700) loss: 0.871519\n",
      "(Iteration 14166 / 14700) loss: 0.847271\n",
      "(Iteration 14167 / 14700) loss: 0.969127\n",
      "(Iteration 14168 / 14700) loss: 0.816245\n",
      "(Iteration 14169 / 14700) loss: 0.735909\n",
      "(Iteration 14170 / 14700) loss: 0.749527\n",
      "(Iteration 14171 / 14700) loss: 0.803092\n",
      "(Iteration 14172 / 14700) loss: 0.726929\n",
      "(Iteration 14173 / 14700) loss: 0.747253\n",
      "(Iteration 14174 / 14700) loss: 0.740934\n",
      "(Iteration 14175 / 14700) loss: 0.553839\n",
      "(Iteration 14176 / 14700) loss: 0.717363\n",
      "(Iteration 14177 / 14700) loss: 0.808598\n",
      "(Iteration 14178 / 14700) loss: 0.725540\n",
      "(Iteration 14179 / 14700) loss: 0.685199\n",
      "(Iteration 14180 / 14700) loss: 0.824397\n",
      "(Iteration 14181 / 14700) loss: 0.987300\n",
      "(Iteration 14182 / 14700) loss: 0.969855\n",
      "(Iteration 14183 / 14700) loss: 0.575038\n",
      "(Iteration 14184 / 14700) loss: 0.563109\n",
      "(Iteration 14185 / 14700) loss: 0.572031\n",
      "(Iteration 14186 / 14700) loss: 0.783544\n",
      "(Iteration 14187 / 14700) loss: 0.717405\n",
      "(Iteration 14188 / 14700) loss: 0.604630\n",
      "(Iteration 14189 / 14700) loss: 0.778734\n",
      "(Iteration 14190 / 14700) loss: 0.976302\n",
      "(Iteration 14191 / 14700) loss: 0.799337\n",
      "(Iteration 14192 / 14700) loss: 0.528940\n",
      "(Iteration 14193 / 14700) loss: 0.917859\n",
      "(Iteration 14194 / 14700) loss: 0.592570\n",
      "(Iteration 14195 / 14700) loss: 0.726800\n",
      "(Iteration 14196 / 14700) loss: 0.580266\n",
      "(Iteration 14197 / 14700) loss: 0.586888\n",
      "(Iteration 14198 / 14700) loss: 0.882712\n",
      "(Iteration 14199 / 14700) loss: 0.824596\n",
      "(Iteration 14200 / 14700) loss: 0.725207\n",
      "(Iteration 14201 / 14700) loss: 0.666240\n",
      "(Iteration 14202 / 14700) loss: 0.734075\n",
      "(Iteration 14203 / 14700) loss: 0.593578\n",
      "(Iteration 14204 / 14700) loss: 0.615181\n",
      "(Iteration 14205 / 14700) loss: 0.473868\n",
      "(Iteration 14206 / 14700) loss: 0.696669\n",
      "(Iteration 14207 / 14700) loss: 0.834307\n",
      "(Iteration 14208 / 14700) loss: 0.971236\n",
      "(Iteration 14209 / 14700) loss: 0.719713\n",
      "(Iteration 14210 / 14700) loss: 0.540999\n",
      "(Iteration 14211 / 14700) loss: 0.691296\n",
      "(Iteration 14212 / 14700) loss: 0.652850\n",
      "(Iteration 14213 / 14700) loss: 0.689562\n",
      "(Iteration 14214 / 14700) loss: 0.649945\n",
      "(Iteration 14215 / 14700) loss: 0.568662\n",
      "(Iteration 14216 / 14700) loss: 0.760071\n",
      "(Iteration 14217 / 14700) loss: 0.534692\n",
      "(Iteration 14218 / 14700) loss: 0.685367\n",
      "(Iteration 14219 / 14700) loss: 0.673071\n",
      "(Iteration 14220 / 14700) loss: 0.661566\n",
      "(Iteration 14221 / 14700) loss: 0.664762\n",
      "(Iteration 14222 / 14700) loss: 0.707522\n",
      "(Iteration 14223 / 14700) loss: 0.670882\n",
      "(Iteration 14224 / 14700) loss: 0.746346\n",
      "(Iteration 14225 / 14700) loss: 0.680376\n",
      "(Iteration 14226 / 14700) loss: 0.728445\n",
      "(Iteration 14227 / 14700) loss: 0.735847\n",
      "(Iteration 14228 / 14700) loss: 0.698310\n",
      "(Iteration 14229 / 14700) loss: 1.132639\n",
      "(Iteration 14230 / 14700) loss: 1.151625\n",
      "(Iteration 14231 / 14700) loss: 0.874501\n",
      "(Iteration 14232 / 14700) loss: 0.990940\n",
      "(Iteration 14233 / 14700) loss: 0.728711\n",
      "(Iteration 14234 / 14700) loss: 0.768285\n",
      "(Iteration 14235 / 14700) loss: 0.798666\n",
      "(Iteration 14236 / 14700) loss: 0.747204\n",
      "(Iteration 14237 / 14700) loss: 1.033714\n",
      "(Iteration 14238 / 14700) loss: 0.713839\n",
      "(Iteration 14239 / 14700) loss: 0.974041\n",
      "(Iteration 14240 / 14700) loss: 0.602704\n",
      "(Iteration 14241 / 14700) loss: 0.661017\n",
      "(Iteration 14242 / 14700) loss: 0.553626\n",
      "(Iteration 14243 / 14700) loss: 0.778702\n",
      "(Iteration 14244 / 14700) loss: 0.700513\n",
      "(Iteration 14245 / 14700) loss: 0.643764\n",
      "(Iteration 14246 / 14700) loss: 0.710477\n",
      "(Iteration 14247 / 14700) loss: 0.791816\n",
      "(Iteration 14248 / 14700) loss: 0.874711\n",
      "(Iteration 14249 / 14700) loss: 0.655542\n",
      "(Iteration 14250 / 14700) loss: 0.776410\n",
      "(Iteration 14251 / 14700) loss: 0.639180\n",
      "(Iteration 14252 / 14700) loss: 0.812574\n",
      "(Iteration 14253 / 14700) loss: 0.665476\n",
      "(Iteration 14254 / 14700) loss: 0.574894\n",
      "(Iteration 14255 / 14700) loss: 0.706445\n",
      "(Iteration 14256 / 14700) loss: 0.551523\n",
      "(Iteration 14257 / 14700) loss: 0.768013\n",
      "(Iteration 14258 / 14700) loss: 0.680110\n",
      "(Iteration 14259 / 14700) loss: 0.500322\n",
      "(Iteration 14260 / 14700) loss: 0.702245\n",
      "(Iteration 14261 / 14700) loss: 0.692608\n",
      "(Iteration 14262 / 14700) loss: 0.560510\n",
      "(Iteration 14263 / 14700) loss: 0.710568\n",
      "(Iteration 14264 / 14700) loss: 0.940060\n",
      "(Iteration 14265 / 14700) loss: 0.941239\n",
      "(Iteration 14266 / 14700) loss: 0.627674\n",
      "(Iteration 14267 / 14700) loss: 0.777898\n",
      "(Iteration 14268 / 14700) loss: 0.556179\n",
      "(Iteration 14269 / 14700) loss: 0.716245\n",
      "(Iteration 14270 / 14700) loss: 0.663947\n",
      "(Iteration 14271 / 14700) loss: 0.877864\n",
      "(Iteration 14272 / 14700) loss: 0.731269\n",
      "(Iteration 14273 / 14700) loss: 1.023300\n",
      "(Iteration 14274 / 14700) loss: 0.765730\n",
      "(Iteration 14275 / 14700) loss: 0.638043\n",
      "(Iteration 14276 / 14700) loss: 0.567996\n",
      "(Iteration 14277 / 14700) loss: 0.690264\n",
      "(Iteration 14278 / 14700) loss: 0.595934\n",
      "(Iteration 14279 / 14700) loss: 0.421392\n",
      "(Iteration 14280 / 14700) loss: 0.721546\n",
      "(Iteration 14281 / 14700) loss: 0.650668\n",
      "(Iteration 14282 / 14700) loss: 0.659631\n",
      "(Iteration 14283 / 14700) loss: 0.546591\n",
      "(Iteration 14284 / 14700) loss: 0.624245\n",
      "(Iteration 14285 / 14700) loss: 0.723598\n",
      "(Iteration 14286 / 14700) loss: 0.526999\n",
      "(Iteration 14287 / 14700) loss: 0.646083\n",
      "(Iteration 14288 / 14700) loss: 0.636910\n",
      "(Iteration 14289 / 14700) loss: 0.591545\n",
      "(Iteration 14290 / 14700) loss: 0.682433\n",
      "(Iteration 14291 / 14700) loss: 0.718309\n",
      "(Iteration 14292 / 14700) loss: 0.751739\n",
      "(Iteration 14293 / 14700) loss: 0.644629\n",
      "(Iteration 14294 / 14700) loss: 0.691413\n",
      "(Iteration 14295 / 14700) loss: 0.649099\n",
      "(Iteration 14296 / 14700) loss: 0.696277\n",
      "(Iteration 14297 / 14700) loss: 0.789143\n",
      "(Iteration 14298 / 14700) loss: 0.597081\n",
      "(Iteration 14299 / 14700) loss: 0.760762\n",
      "(Iteration 14300 / 14700) loss: 0.513568\n",
      "(Iteration 14301 / 14700) loss: 0.973016\n",
      "(Iteration 14302 / 14700) loss: 0.827160\n",
      "(Iteration 14303 / 14700) loss: 0.726478\n",
      "(Iteration 14304 / 14700) loss: 0.806799\n",
      "(Iteration 14305 / 14700) loss: 0.777425\n",
      "(Iteration 14306 / 14700) loss: 0.621856\n",
      "(Iteration 14307 / 14700) loss: 0.723943\n",
      "(Iteration 14308 / 14700) loss: 0.837173\n",
      "(Iteration 14309 / 14700) loss: 0.648468\n",
      "(Iteration 14310 / 14700) loss: 0.577744\n",
      "(Iteration 14311 / 14700) loss: 0.642483\n",
      "(Iteration 14312 / 14700) loss: 0.705839\n",
      "(Iteration 14313 / 14700) loss: 0.621846\n",
      "(Iteration 14314 / 14700) loss: 0.556023\n",
      "(Iteration 14315 / 14700) loss: 0.434160\n",
      "(Iteration 14316 / 14700) loss: 0.556794\n",
      "(Iteration 14317 / 14700) loss: 0.708451\n",
      "(Iteration 14318 / 14700) loss: 0.829670\n",
      "(Iteration 14319 / 14700) loss: 0.839732\n",
      "(Iteration 14320 / 14700) loss: 0.618830\n",
      "(Iteration 14321 / 14700) loss: 0.750425\n",
      "(Iteration 14322 / 14700) loss: 0.664920\n",
      "(Iteration 14323 / 14700) loss: 0.730626\n",
      "(Iteration 14324 / 14700) loss: 0.624913\n",
      "(Iteration 14325 / 14700) loss: 0.678267\n",
      "(Iteration 14326 / 14700) loss: 0.804258\n",
      "(Iteration 14327 / 14700) loss: 0.459311\n",
      "(Iteration 14328 / 14700) loss: 0.653683\n",
      "(Iteration 14329 / 14700) loss: 0.838991\n",
      "(Iteration 14330 / 14700) loss: 0.738363\n",
      "(Iteration 14331 / 14700) loss: 0.720448\n",
      "(Iteration 14332 / 14700) loss: 0.757225\n",
      "(Iteration 14333 / 14700) loss: 0.515453\n",
      "(Iteration 14334 / 14700) loss: 0.779197\n",
      "(Iteration 14335 / 14700) loss: 0.839587\n",
      "(Iteration 14336 / 14700) loss: 0.697352\n",
      "(Iteration 14337 / 14700) loss: 0.635237\n",
      "(Iteration 14338 / 14700) loss: 0.700785\n",
      "(Iteration 14339 / 14700) loss: 0.826102\n",
      "(Iteration 14340 / 14700) loss: 0.816167\n",
      "(Iteration 14341 / 14700) loss: 0.578227\n",
      "(Iteration 14342 / 14700) loss: 0.917520\n",
      "(Iteration 14343 / 14700) loss: 0.703413\n",
      "(Iteration 14344 / 14700) loss: 0.861389\n",
      "(Iteration 14345 / 14700) loss: 0.444482\n",
      "(Iteration 14346 / 14700) loss: 0.881553\n",
      "(Iteration 14347 / 14700) loss: 0.574719\n",
      "(Iteration 14348 / 14700) loss: 0.785827\n",
      "(Iteration 14349 / 14700) loss: 0.769298\n",
      "(Iteration 14350 / 14700) loss: 0.574460\n",
      "(Iteration 14351 / 14700) loss: 0.789866\n",
      "(Iteration 14352 / 14700) loss: 0.721772\n",
      "(Iteration 14353 / 14700) loss: 0.767372\n",
      "(Iteration 14354 / 14700) loss: 0.675328\n",
      "(Iteration 14355 / 14700) loss: 0.517664\n",
      "(Iteration 14356 / 14700) loss: 0.802733\n",
      "(Iteration 14357 / 14700) loss: 0.710959\n",
      "(Iteration 14358 / 14700) loss: 0.929306\n",
      "(Iteration 14359 / 14700) loss: 0.934160\n",
      "(Iteration 14360 / 14700) loss: 0.759654\n",
      "(Iteration 14361 / 14700) loss: 0.782252\n",
      "(Iteration 14362 / 14700) loss: 0.738998\n",
      "(Iteration 14363 / 14700) loss: 0.675461\n",
      "(Iteration 14364 / 14700) loss: 0.572489\n",
      "(Iteration 14365 / 14700) loss: 0.639245\n",
      "(Iteration 14366 / 14700) loss: 0.766351\n",
      "(Iteration 14367 / 14700) loss: 0.597713\n",
      "(Iteration 14368 / 14700) loss: 0.811236\n",
      "(Iteration 14369 / 14700) loss: 0.748310\n",
      "(Iteration 14370 / 14700) loss: 0.872966\n",
      "(Iteration 14371 / 14700) loss: 0.726017\n",
      "(Iteration 14372 / 14700) loss: 0.819688\n",
      "(Iteration 14373 / 14700) loss: 0.705226\n",
      "(Iteration 14374 / 14700) loss: 0.853290\n",
      "(Iteration 14375 / 14700) loss: 0.693431\n",
      "(Iteration 14376 / 14700) loss: 0.678819\n",
      "(Iteration 14377 / 14700) loss: 0.806171\n",
      "(Iteration 14378 / 14700) loss: 0.562030\n",
      "(Iteration 14379 / 14700) loss: 0.684471\n",
      "(Iteration 14380 / 14700) loss: 0.769858\n",
      "(Iteration 14381 / 14700) loss: 0.837152\n",
      "(Iteration 14382 / 14700) loss: 0.634419\n",
      "(Iteration 14383 / 14700) loss: 0.676693\n",
      "(Iteration 14384 / 14700) loss: 0.688524\n",
      "(Iteration 14385 / 14700) loss: 0.689091\n",
      "(Iteration 14386 / 14700) loss: 0.615805\n",
      "(Iteration 14387 / 14700) loss: 0.575668\n",
      "(Iteration 14388 / 14700) loss: 0.791666\n",
      "(Iteration 14389 / 14700) loss: 0.618808\n",
      "(Iteration 14390 / 14700) loss: 0.674973\n",
      "(Iteration 14391 / 14700) loss: 0.750594\n",
      "(Iteration 14392 / 14700) loss: 0.696146\n",
      "(Iteration 14393 / 14700) loss: 0.588710\n",
      "(Iteration 14394 / 14700) loss: 0.728663\n",
      "(Iteration 14395 / 14700) loss: 0.467476\n",
      "(Iteration 14396 / 14700) loss: 0.644907\n",
      "(Iteration 14397 / 14700) loss: 0.840288\n",
      "(Iteration 14398 / 14700) loss: 0.760035\n",
      "(Iteration 14399 / 14700) loss: 0.821298\n",
      "(Iteration 14400 / 14700) loss: 0.830105\n",
      "(Iteration 14401 / 14700) loss: 0.616842\n",
      "(Iteration 14402 / 14700) loss: 0.677512\n",
      "(Iteration 14403 / 14700) loss: 0.923629\n",
      "(Iteration 14404 / 14700) loss: 0.514184\n",
      "(Iteration 14405 / 14700) loss: 0.810219\n",
      "(Iteration 14406 / 14700) loss: 0.681344\n",
      "(Iteration 14407 / 14700) loss: 0.553115\n",
      "(Iteration 14408 / 14700) loss: 0.782250\n",
      "(Iteration 14409 / 14700) loss: 0.521305\n",
      "(Iteration 14410 / 14700) loss: 0.670858\n",
      "(Iteration 14411 / 14700) loss: 0.717880\n",
      "(Iteration 14412 / 14700) loss: 0.756117\n",
      "(Iteration 14413 / 14700) loss: 0.757158\n",
      "(Iteration 14414 / 14700) loss: 0.829080\n",
      "(Iteration 14415 / 14700) loss: 0.706707\n",
      "(Iteration 14416 / 14700) loss: 0.736550\n",
      "(Iteration 14417 / 14700) loss: 0.493354\n",
      "(Iteration 14418 / 14700) loss: 0.684282\n",
      "(Iteration 14419 / 14700) loss: 0.689960\n",
      "(Iteration 14420 / 14700) loss: 0.981506\n",
      "(Iteration 14421 / 14700) loss: 0.650668\n",
      "(Iteration 14422 / 14700) loss: 0.565263\n",
      "(Iteration 14423 / 14700) loss: 0.786827\n",
      "(Iteration 14424 / 14700) loss: 0.639128\n",
      "(Iteration 14425 / 14700) loss: 0.733258\n",
      "(Iteration 14426 / 14700) loss: 0.650358\n",
      "(Iteration 14427 / 14700) loss: 0.785577\n",
      "(Iteration 14428 / 14700) loss: 0.645644\n",
      "(Iteration 14429 / 14700) loss: 0.607892\n",
      "(Iteration 14430 / 14700) loss: 0.688010\n",
      "(Iteration 14431 / 14700) loss: 0.950219\n",
      "(Iteration 14432 / 14700) loss: 0.741691\n",
      "(Iteration 14433 / 14700) loss: 0.627333\n",
      "(Iteration 14434 / 14700) loss: 0.797902\n",
      "(Iteration 14435 / 14700) loss: 0.845737\n",
      "(Iteration 14436 / 14700) loss: 0.750411\n",
      "(Iteration 14437 / 14700) loss: 0.639977\n",
      "(Iteration 14438 / 14700) loss: 0.765895\n",
      "(Iteration 14439 / 14700) loss: 0.847594\n",
      "(Iteration 14440 / 14700) loss: 0.749301\n",
      "(Iteration 14441 / 14700) loss: 0.886272\n",
      "(Iteration 14442 / 14700) loss: 0.853830\n",
      "(Iteration 14443 / 14700) loss: 0.666318\n",
      "(Iteration 14444 / 14700) loss: 0.766410\n",
      "(Iteration 14445 / 14700) loss: 0.781023\n",
      "(Iteration 14446 / 14700) loss: 0.809144\n",
      "(Iteration 14447 / 14700) loss: 1.039428\n",
      "(Iteration 14448 / 14700) loss: 0.646883\n",
      "(Iteration 14449 / 14700) loss: 0.723411\n",
      "(Iteration 14450 / 14700) loss: 0.776115\n",
      "(Iteration 14451 / 14700) loss: 0.888581\n",
      "(Iteration 14452 / 14700) loss: 0.618898\n",
      "(Iteration 14453 / 14700) loss: 0.748254\n",
      "(Iteration 14454 / 14700) loss: 0.666886\n",
      "(Iteration 14455 / 14700) loss: 0.562783\n",
      "(Iteration 14456 / 14700) loss: 0.603326\n",
      "(Iteration 14457 / 14700) loss: 0.573932\n",
      "(Iteration 14458 / 14700) loss: 0.970670\n",
      "(Iteration 14459 / 14700) loss: 0.690436\n",
      "(Iteration 14460 / 14700) loss: 0.764680\n",
      "(Iteration 14461 / 14700) loss: 0.547958\n",
      "(Iteration 14462 / 14700) loss: 0.773637\n",
      "(Iteration 14463 / 14700) loss: 0.796125\n",
      "(Iteration 14464 / 14700) loss: 0.699287\n",
      "(Iteration 14465 / 14700) loss: 0.557086\n",
      "(Iteration 14466 / 14700) loss: 0.751276\n",
      "(Iteration 14467 / 14700) loss: 0.869578\n",
      "(Iteration 14468 / 14700) loss: 0.577145\n",
      "(Iteration 14469 / 14700) loss: 0.735976\n",
      "(Iteration 14470 / 14700) loss: 0.571059\n",
      "(Iteration 14471 / 14700) loss: 0.832863\n",
      "(Iteration 14472 / 14700) loss: 0.641514\n",
      "(Iteration 14473 / 14700) loss: 0.957913\n",
      "(Iteration 14474 / 14700) loss: 0.530618\n",
      "(Iteration 14475 / 14700) loss: 0.526489\n",
      "(Iteration 14476 / 14700) loss: 0.610155\n",
      "(Iteration 14477 / 14700) loss: 0.744758\n",
      "(Iteration 14478 / 14700) loss: 0.735968\n",
      "(Iteration 14479 / 14700) loss: 0.800521\n",
      "(Iteration 14480 / 14700) loss: 0.472588\n",
      "(Iteration 14481 / 14700) loss: 0.703839\n",
      "(Iteration 14482 / 14700) loss: 0.844383\n",
      "(Iteration 14483 / 14700) loss: 0.630465\n",
      "(Iteration 14484 / 14700) loss: 0.503768\n",
      "(Iteration 14485 / 14700) loss: 0.693610\n",
      "(Iteration 14486 / 14700) loss: 0.511847\n",
      "(Iteration 14487 / 14700) loss: 0.731269\n",
      "(Iteration 14488 / 14700) loss: 0.825211\n",
      "(Iteration 14489 / 14700) loss: 0.666658\n",
      "(Iteration 14490 / 14700) loss: 0.569459\n",
      "(Iteration 14491 / 14700) loss: 0.720078\n",
      "(Iteration 14492 / 14700) loss: 0.657825\n",
      "(Iteration 14493 / 14700) loss: 0.647645\n",
      "(Iteration 14494 / 14700) loss: 0.744159\n",
      "(Iteration 14495 / 14700) loss: 0.675949\n",
      "(Iteration 14496 / 14700) loss: 0.753940\n",
      "(Iteration 14497 / 14700) loss: 0.550287\n",
      "(Iteration 14498 / 14700) loss: 0.578301\n",
      "(Iteration 14499 / 14700) loss: 0.524317\n",
      "(Iteration 14500 / 14700) loss: 0.926073\n",
      "(Iteration 14501 / 14700) loss: 0.686238\n",
      "(Iteration 14502 / 14700) loss: 0.727488\n",
      "(Iteration 14503 / 14700) loss: 0.778682\n",
      "(Iteration 14504 / 14700) loss: 0.727419\n",
      "(Iteration 14505 / 14700) loss: 0.706430\n",
      "(Iteration 14506 / 14700) loss: 0.773295\n",
      "(Iteration 14507 / 14700) loss: 0.633157\n",
      "(Iteration 14508 / 14700) loss: 0.583239\n",
      "(Iteration 14509 / 14700) loss: 0.647386\n",
      "(Iteration 14510 / 14700) loss: 0.744850\n",
      "(Iteration 14511 / 14700) loss: 0.657775\n",
      "(Iteration 14512 / 14700) loss: 0.704005\n",
      "(Iteration 14513 / 14700) loss: 0.670027\n",
      "(Iteration 14514 / 14700) loss: 0.625645\n",
      "(Iteration 14515 / 14700) loss: 0.707037\n",
      "(Iteration 14516 / 14700) loss: 0.575211\n",
      "(Iteration 14517 / 14700) loss: 0.715582\n",
      "(Iteration 14518 / 14700) loss: 1.009773\n",
      "(Iteration 14519 / 14700) loss: 0.598017\n",
      "(Iteration 14520 / 14700) loss: 0.578881\n",
      "(Iteration 14521 / 14700) loss: 0.771594\n",
      "(Iteration 14522 / 14700) loss: 0.761399\n",
      "(Iteration 14523 / 14700) loss: 0.548860\n",
      "(Iteration 14524 / 14700) loss: 0.475219\n",
      "(Iteration 14525 / 14700) loss: 0.562996\n",
      "(Iteration 14526 / 14700) loss: 0.761348\n",
      "(Iteration 14527 / 14700) loss: 0.534474\n",
      "(Iteration 14528 / 14700) loss: 0.810458\n",
      "(Iteration 14529 / 14700) loss: 0.677667\n",
      "(Iteration 14530 / 14700) loss: 0.677958\n",
      "(Iteration 14531 / 14700) loss: 0.443239\n",
      "(Iteration 14532 / 14700) loss: 0.499666\n",
      "(Iteration 14533 / 14700) loss: 0.534967\n",
      "(Iteration 14534 / 14700) loss: 0.771937\n",
      "(Iteration 14535 / 14700) loss: 0.513229\n",
      "(Iteration 14536 / 14700) loss: 0.675649\n",
      "(Iteration 14537 / 14700) loss: 0.945630\n",
      "(Iteration 14538 / 14700) loss: 0.527437\n",
      "(Iteration 14539 / 14700) loss: 0.519078\n",
      "(Iteration 14540 / 14700) loss: 0.830868\n",
      "(Iteration 14541 / 14700) loss: 0.743205\n",
      "(Iteration 14542 / 14700) loss: 0.643672\n",
      "(Iteration 14543 / 14700) loss: 0.648684\n",
      "(Iteration 14544 / 14700) loss: 0.619181\n",
      "(Iteration 14545 / 14700) loss: 0.456279\n",
      "(Iteration 14546 / 14700) loss: 0.809091\n",
      "(Iteration 14547 / 14700) loss: 0.689730\n",
      "(Iteration 14548 / 14700) loss: 0.843494\n",
      "(Iteration 14549 / 14700) loss: 0.610623\n",
      "(Iteration 14550 / 14700) loss: 0.706978\n",
      "(Iteration 14551 / 14700) loss: 0.704120\n",
      "(Iteration 14552 / 14700) loss: 0.663307\n",
      "(Iteration 14553 / 14700) loss: 0.630578\n",
      "(Iteration 14554 / 14700) loss: 0.606520\n",
      "(Iteration 14555 / 14700) loss: 0.505948\n",
      "(Iteration 14556 / 14700) loss: 0.662022\n",
      "(Iteration 14557 / 14700) loss: 0.574673\n",
      "(Iteration 14558 / 14700) loss: 0.792619\n",
      "(Iteration 14559 / 14700) loss: 0.622448\n",
      "(Iteration 14560 / 14700) loss: 0.732965\n",
      "(Iteration 14561 / 14700) loss: 0.633326\n",
      "(Iteration 14562 / 14700) loss: 0.534273\n",
      "(Iteration 14563 / 14700) loss: 0.530456\n",
      "(Iteration 14564 / 14700) loss: 0.684678\n",
      "(Iteration 14565 / 14700) loss: 0.701167\n",
      "(Iteration 14566 / 14700) loss: 0.638288\n",
      "(Iteration 14567 / 14700) loss: 0.826647\n",
      "(Iteration 14568 / 14700) loss: 0.643719\n",
      "(Iteration 14569 / 14700) loss: 0.801582\n",
      "(Iteration 14570 / 14700) loss: 0.599631\n",
      "(Iteration 14571 / 14700) loss: 0.731975\n",
      "(Iteration 14572 / 14700) loss: 0.567501\n",
      "(Iteration 14573 / 14700) loss: 0.681190\n",
      "(Iteration 14574 / 14700) loss: 0.703896\n",
      "(Iteration 14575 / 14700) loss: 0.547084\n",
      "(Iteration 14576 / 14700) loss: 0.641590\n",
      "(Iteration 14577 / 14700) loss: 0.813117\n",
      "(Iteration 14578 / 14700) loss: 0.692147\n",
      "(Iteration 14579 / 14700) loss: 0.642589\n",
      "(Iteration 14580 / 14700) loss: 0.677005\n",
      "(Iteration 14581 / 14700) loss: 0.867917\n",
      "(Iteration 14582 / 14700) loss: 0.910516\n",
      "(Iteration 14583 / 14700) loss: 0.661541\n",
      "(Iteration 14584 / 14700) loss: 0.571319\n",
      "(Iteration 14585 / 14700) loss: 0.655540\n",
      "(Iteration 14586 / 14700) loss: 0.522822\n",
      "(Iteration 14587 / 14700) loss: 0.562762\n",
      "(Iteration 14588 / 14700) loss: 0.741869\n",
      "(Iteration 14589 / 14700) loss: 0.609014\n",
      "(Iteration 14590 / 14700) loss: 0.527613\n",
      "(Iteration 14591 / 14700) loss: 0.678160\n",
      "(Iteration 14592 / 14700) loss: 0.585832\n",
      "(Iteration 14593 / 14700) loss: 0.513324\n",
      "(Iteration 14594 / 14700) loss: 0.580526\n",
      "(Iteration 14595 / 14700) loss: 0.692583\n",
      "(Iteration 14596 / 14700) loss: 0.657194\n",
      "(Iteration 14597 / 14700) loss: 0.551284\n",
      "(Iteration 14598 / 14700) loss: 0.600901\n",
      "(Iteration 14599 / 14700) loss: 0.568755\n",
      "(Iteration 14600 / 14700) loss: 0.698208\n",
      "(Iteration 14601 / 14700) loss: 0.744956\n",
      "(Iteration 14602 / 14700) loss: 0.757154\n",
      "(Iteration 14603 / 14700) loss: 0.699728\n",
      "(Iteration 14604 / 14700) loss: 0.671940\n",
      "(Iteration 14605 / 14700) loss: 0.641228\n",
      "(Iteration 14606 / 14700) loss: 0.836245\n",
      "(Iteration 14607 / 14700) loss: 0.747573\n",
      "(Iteration 14608 / 14700) loss: 0.650390\n",
      "(Iteration 14609 / 14700) loss: 0.626375\n",
      "(Iteration 14610 / 14700) loss: 0.946938\n",
      "(Iteration 14611 / 14700) loss: 0.914966\n",
      "(Iteration 14612 / 14700) loss: 0.531895\n",
      "(Iteration 14613 / 14700) loss: 0.708306\n",
      "(Iteration 14614 / 14700) loss: 0.885664\n",
      "(Iteration 14615 / 14700) loss: 0.693717\n",
      "(Iteration 14616 / 14700) loss: 0.717181\n",
      "(Iteration 14617 / 14700) loss: 0.623148\n",
      "(Iteration 14618 / 14700) loss: 0.586823\n",
      "(Iteration 14619 / 14700) loss: 0.931259\n",
      "(Iteration 14620 / 14700) loss: 0.586139\n",
      "(Iteration 14621 / 14700) loss: 0.655236\n",
      "(Iteration 14622 / 14700) loss: 0.568795\n",
      "(Iteration 14623 / 14700) loss: 0.710707\n",
      "(Iteration 14624 / 14700) loss: 0.852726\n",
      "(Iteration 14625 / 14700) loss: 0.822746\n",
      "(Iteration 14626 / 14700) loss: 0.766004\n",
      "(Iteration 14627 / 14700) loss: 0.587996\n",
      "(Iteration 14628 / 14700) loss: 0.665115\n",
      "(Iteration 14629 / 14700) loss: 0.544071\n",
      "(Iteration 14630 / 14700) loss: 0.952441\n",
      "(Iteration 14631 / 14700) loss: 0.577989\n",
      "(Iteration 14632 / 14700) loss: 0.828740\n",
      "(Iteration 14633 / 14700) loss: 0.731702\n",
      "(Iteration 14634 / 14700) loss: 0.738648\n",
      "(Iteration 14635 / 14700) loss: 0.877363\n",
      "(Iteration 14636 / 14700) loss: 0.646227\n",
      "(Iteration 14637 / 14700) loss: 0.596110\n",
      "(Iteration 14638 / 14700) loss: 0.756319\n",
      "(Iteration 14639 / 14700) loss: 0.703978\n",
      "(Iteration 14640 / 14700) loss: 0.758503\n",
      "(Iteration 14641 / 14700) loss: 0.928138\n",
      "(Iteration 14642 / 14700) loss: 0.761798\n",
      "(Iteration 14643 / 14700) loss: 0.895515\n",
      "(Iteration 14644 / 14700) loss: 0.555953\n",
      "(Iteration 14645 / 14700) loss: 0.615156\n",
      "(Iteration 14646 / 14700) loss: 0.846132\n",
      "(Iteration 14647 / 14700) loss: 0.646522\n",
      "(Iteration 14648 / 14700) loss: 0.570900\n",
      "(Iteration 14649 / 14700) loss: 0.573969\n",
      "(Iteration 14650 / 14700) loss: 0.630955\n",
      "(Iteration 14651 / 14700) loss: 0.609187\n",
      "(Iteration 14652 / 14700) loss: 0.708967\n",
      "(Iteration 14653 / 14700) loss: 0.500928\n",
      "(Iteration 14654 / 14700) loss: 0.706105\n",
      "(Iteration 14655 / 14700) loss: 0.768285\n",
      "(Iteration 14656 / 14700) loss: 0.552275\n",
      "(Iteration 14657 / 14700) loss: 0.930608\n",
      "(Iteration 14658 / 14700) loss: 0.621697\n",
      "(Iteration 14659 / 14700) loss: 0.737419\n",
      "(Iteration 14660 / 14700) loss: 0.613647\n",
      "(Iteration 14661 / 14700) loss: 0.602231\n",
      "(Iteration 14662 / 14700) loss: 0.806711\n",
      "(Iteration 14663 / 14700) loss: 0.665887\n",
      "(Iteration 14664 / 14700) loss: 0.795681\n",
      "(Iteration 14665 / 14700) loss: 0.691025\n",
      "(Iteration 14666 / 14700) loss: 0.625964\n",
      "(Iteration 14667 / 14700) loss: 0.703243\n",
      "(Iteration 14668 / 14700) loss: 0.628846\n",
      "(Iteration 14669 / 14700) loss: 0.777122\n",
      "(Iteration 14670 / 14700) loss: 0.708502\n",
      "(Iteration 14671 / 14700) loss: 0.773026\n",
      "(Iteration 14672 / 14700) loss: 0.949920\n",
      "(Iteration 14673 / 14700) loss: 0.569651\n",
      "(Iteration 14674 / 14700) loss: 0.813533\n",
      "(Iteration 14675 / 14700) loss: 0.976875\n",
      "(Iteration 14676 / 14700) loss: 0.832368\n",
      "(Iteration 14677 / 14700) loss: 0.532431\n",
      "(Iteration 14678 / 14700) loss: 0.828587\n",
      "(Iteration 14679 / 14700) loss: 0.587165\n",
      "(Iteration 14680 / 14700) loss: 0.617695\n",
      "(Iteration 14681 / 14700) loss: 0.545068\n",
      "(Iteration 14682 / 14700) loss: 0.931987\n",
      "(Iteration 14683 / 14700) loss: 0.615267\n",
      "(Iteration 14684 / 14700) loss: 0.684201\n",
      "(Iteration 14685 / 14700) loss: 0.548157\n",
      "(Iteration 14686 / 14700) loss: 0.835561\n",
      "(Iteration 14687 / 14700) loss: 0.604751\n",
      "(Iteration 14688 / 14700) loss: 0.567426\n",
      "(Iteration 14689 / 14700) loss: 0.512736\n",
      "(Iteration 14690 / 14700) loss: 0.739214\n",
      "(Iteration 14691 / 14700) loss: 0.791147\n",
      "(Iteration 14692 / 14700) loss: 0.830291\n",
      "(Iteration 14693 / 14700) loss: 0.587038\n",
      "(Iteration 14694 / 14700) loss: 0.886366\n",
      "(Iteration 14695 / 14700) loss: 0.599571\n",
      "(Iteration 14696 / 14700) loss: 0.679597\n",
      "(Iteration 14697 / 14700) loss: 0.562786\n",
      "(Iteration 14698 / 14700) loss: 0.593409\n",
      "(Iteration 14699 / 14700) loss: 0.701059\n",
      "(Iteration 14700 / 14700) loss: 0.558835\n",
      "(Epoch 15 / 15) train acc: 0.824000; val_acc: 0.769000\n",
      "2.967037e-03 9.000000e-04  final l: 0.55883  acc: 0.769\n",
      "Discarded: 0\n",
      "Best params: 2.967037e-03 9.000000e-04 1.000000e+00\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2EAAAK9CAYAAACth2TfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIEsQgjIiIiIiIKEH/DyRnQ4H5mjNuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1171faa10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper parameter search\n",
    "\n",
    "def uniform(minv, maxv):\n",
    "    return np.random.rand() * (maxv - minv) + minv\n",
    "\n",
    "batch_size = 50\n",
    "batch_shape = (batch_size,) + val_data['X_train'].shape[1:]\n",
    "\n",
    "discarded = 0\n",
    "for _ in range(1):\n",
    "    # r 2.967037e-03   lr 4.242814e-03\n",
    "    weight_scale = 1.0  # 10 ** uniform(-4, -2)\n",
    "    learning_rate = 9e-04  # 10 ** uniform(-2.8, -2)\n",
    "    reg = 2.967037e-03  # 10 ** uniform(-3.3, -2.5)\n",
    "    \n",
    "    model = Sequential(batch_shape=batch_shape, weight_scale=weight_scale, reg=reg)\n",
    "    for num_filters in [8, 16, 32]:\n",
    "        model.add(ConvBnRelu(num_filters))\n",
    "        model.add(ConvBnRelu(num_filters))\n",
    "        model.add(Pool())\n",
    "    for num_neurons in [256, 128]:\n",
    "        model.add(Dense(num_neurons, weight_init='sqrt_2_over_n'))\n",
    "        model.add(Relu())\n",
    "    model.add(Dense(num_neurons=10, weight_init='sqrt_2_over_n'))\n",
    "    model.build(loss=Softmax())\n",
    "    \n",
    "    # {'X_train': X, 'y_train': y, 'X_val': X, 'y_val': y}\n",
    "    solver = Solver(model, data,\n",
    "                num_epochs=15, batch_size=batch_size,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                },\n",
    "                verbose=True, print_every=1)\n",
    "    #solver = Solver(model, val_data,\n",
    "    #                print_every=10, num_epochs=5, batch_size=batch_size,\n",
    "    #                update_rule='adam',\n",
    "    #                optim_config={\n",
    "    #                  'learning_rate': learning_rate,\n",
    "    #                },\n",
    "    #                verbose=True)\n",
    "    solver.train()\n",
    "    acc = solver.val_acc_history[-1]\n",
    "    #acc = solver.train_acc_history[-1]\n",
    "    final_loss = solver.loss_history[-1]\n",
    "    format_str = '{:e} {:e}  final l: {:.5f}  acc: {}'\n",
    "    print format_str.format(reg, learning_rate, final_loss, acc)\n",
    "    #if np.max(solver.loss_history) > 5:\n",
    "    #    discarded += 1\n",
    "    #    continue\n",
    "    plt.plot(solver.loss_history, '-')\n",
    "    params = (reg, learning_rate, weight_scale)\n",
    "    results[params] = (acc, final_loss)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_loss = final_loss\n",
    "        best_solver = solver\n",
    "        best_model = model\n",
    "        best_params = params\n",
    "\n",
    "print 'Discarded:', discarded\n",
    "print 'Best params: {:e} {:e} {:e}'.format(*best_params)\n",
    "        \n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzAAAAK9CAYAAAD7UYmpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFfCVQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11770fe90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the cross-validation results\n",
    "import math\n",
    "marker_size = 50\n",
    "x_scatter, x_label = np.array([np.log10(x[0]) for x in results]), 'log reg'\n",
    "#x_scatter, x_label = np.array([np.log10(x[2]) for x in results]), 'log weight scale'\n",
    "y_scatter, y_label = np.array([np.log10(x[1]) for x in results]), 'log learning rate'\n",
    "\n",
    "# plot validation accuracy\n",
    "things = [(0, 'Accuracy', 1),\n",
    "          (1, 'Loss', -1)]\n",
    "for i, th in enumerate(things):\n",
    "    color_data_i, title, sens = th\n",
    "    colors = np.array([results[x][color_data_i] for x in results])\n",
    "    plt.subplot(2, 1, i+1)\n",
    "    order = np.argsort(colors)[::sens]\n",
    "    plt.scatter(x_scatter[order], y_scatter[order], marker_size, c=colors[order])\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2EAAAKvCAYAAADjmcdPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvtbV6GIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1171d5f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(best_solver.loss_history, '-')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(best_solver.train_acc_history, '-')\n",
    "plt.plot(best_solver.val_acc_history, '-')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print best_model.params['ConvBnRelu1_W'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA00AAAIGCAYAAACI62Q3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117ed6c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class.\n",
    "# Depending on your choice of learning rate and regularization strength, these may\n",
    "# or may not be nice to look at.\n",
    "w = best_model.params['ConvBnRelu1_W']\n",
    "w = np.moveaxis(w, 0, -1)  # --> (C, 3, 3, 8)\n",
    "w = np.moveaxis(w, 0, 2)   # --> (3, 3, C, 8)\n",
    "w = w.reshape(3, 3, 3, -1)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "for i in xrange(w.shape[-1]):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'weights-{:.4f}.bin'.format(best_solver.loss_history[-1])\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(best_model.params, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open('weights.bin', 'rb') as f:\n",
    "#    w = pickle.load(f)\n",
    "#for p in sorted(w.keys()):\n",
    "#    print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pitou]",
   "language": "python",
   "name": "conda-env-pitou-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
